[{"content":"I needed to monitor the uptime of a network device, but with the added twist of tracking when dropped packets occurred. While ping doesn\u0026rsquo;t natively support a timestamp option, Powershell lets me quickly craft a command to do so:\nping -t 8.8.8.8|Foreach{\u0026quot;{0} - {1}\u0026quot; -f (Get-Date),$_}\nPS C:\\Users\\Greg\u0026gt; ping -t 8.8.8.8|Foreach{\u0026#34;{0} - {1}\u0026#34; -f (Get-Date),$_} 2025-05-20 11:29:25 AM - 2025-05-20 11:29:25 AM - Pinging 8.8.8.8 with 32 bytes of data: 2025-05-20 11:29:25 AM - Reply from 8.8.8.8: bytes=32 time=19ms TTL=114 2025-05-20 11:29:26 AM - Reply from 8.8.8.8: bytes=32 time=16ms TTL=114 2025-05-20 11:29:27 AM - Reply from 8.8.8.8: bytes=32 time=17ms TTL=114 2025-05-20 11:29:28 AM - Reply from 8.8.8.8: bytes=32 time=18ms TTL=114 2025-05-20 11:29:28 AM - PS C:\\Users\\Greg\u0026gt; The crummy part is that when I trigger an endless ping (-t option), cancelling it via. CRTL-C doesn\u0026rsquo;t display the statistics (total ping sent, total dropped, RTT summary, etc).\nIf I want to log it to a file:\nping -t 8.8.8.8|Foreach{\u0026quot;{0} - {1}\u0026quot; -f (Get-Date),$_} \u0026gt; desktop\\ping_log.txt\n","date":"2025-05-20T11:24:32-04:00","permalink":"https://gbeifuss.github.io/p/ping-with-timestamps/","title":"Ping with timestamps"},{"content":"I had to remove a corporate MDM from an iPhone I\u0026rsquo;d been given and found tazMeah\u0026rsquo;s excellent project MDMPatcher-Universal. It\u0026rsquo;s an OS X-based tool, which means that I had to go through a number of hoops to get it working with my setup. There is a lot of rebooting in this process, but I wanted to make sure that there were no problems due to components that couldn\u0026rsquo;t be properly installed/removed due to pending reboots.\nHere\u0026rsquo;s the environment I was using:\nMac OS X Big Sur 10.16 VMWare Workstation 15.5.7 Dell 7230 laptop with Windows 11 24H2 iPhone 12 with iOS 18 I started by removing Hyper-V on my latop (which I\u0026rsquo;d previously used for PoC), as it was going to cause complications with VMWare. In theory, they can co-exist, but I decided to simplify my life by removing the hypervisor I no longer needed. Turn Windows Features On or Off \u0026gt; Uncheck \u0026ldquo;Hyper-V\u0026rdquo; Reboot bcdedit /set hypervisorlaunchtype off Reboot\nInstall VMWare Workstation. I was able to find a license key online for this older product. Reboot. Patch VMWare Workstation to support the Apple VM type using https://github.com/paolo-projects/unlocker Download Download darwin.iso. The tool didn\u0026rsquo;t download it from me, which I assume is due to the Broadcom/VMWare split. I grabbed it directly from VMWare: https://packages.vmware.com/tools/frozen/darwin/ Run win-install.cmd. This tool also includes the necessary python components, but you could also download/install the latest version of python 3 as a separate application Running win-update-tools.cmd is not necessary. This script simply extracts darwin.iso and stores it in the VMWare Workstation installation directory, which I manually did.\nAt this point, I made my Mac OS X VM by following this guide: https://github.com/dortania/macOS-VMware-Guide/blob/master/making-the-virtual-machine.md\nI had every intention of doing a full writeup, but after going through the whole process - which worked, by the way - it turns out that anytime the iphone is updated (or rebooted?), this process has to be repeated. I don\u0026rsquo;t want to go through that hassle, since I have other options. So, the rest of this guide is very sparse, but I will list the links that I referenced.\ninstall MDMPatcher on the OSx VM Download the right ipsw file from ipsw.me Put the iphone into recovery mode Flash the ipsw file using iTunes (the ipsw.me site has instructions). Use the SHIFT key instead of Option if you\u0026rsquo;re using a Windows device. Connect the iphone to the VM using USB passthrough Follow the initial setup, but do not connect to any network. If you do, you\u0026rsquo;ll have to reflash the iphone Launch the MDMPatcher in OSX. When your device info shows in MDMPatcher, click \u0026ldquo;PATCH\u0026rdquo; to complete the process After reboot, follow the remaining setup instructions on the device References\rSome articles I referenced when figuring this out was:\nhttps://github.com/fled-dev/MDMPatcher-Enhanced https://github.com/DavidsonRafaelK/MacOS-Installation https://ipsw.me/download/iPhone13,1/22E240 https://github.com/tazMeah/MDMPatcher-Universal?tab=readme-ov-file\n","date":"2025-05-15T22:07:25-04:00","permalink":"https://gbeifuss.github.io/p/running-mdmpatcher-on-windows-11/","title":"Running MDMPatcher on Windows 11"},{"content":"I\u0026rsquo;ve been using the internet since the mid-90s, and I can remember a time when people created pages because they had interests and knowledge to share, before an age of advertising. While I really appreciate web crawlers, the indexes that used to pass as \u0026lsquo;search engines\u0026rsquo; (ie. Yahoo) also had their advantages.\nIn any case, I am tired of seeing Google AI results showing up at the top of my search results. I don\u0026rsquo;t want to skim AI content, trying to determine if it\u0026rsquo;s from relevant/legitimate/correct sources. It turns out there\u0026rsquo;s a way to append that preference to Google searches, so I decided to set Firefox (my browser of choice) up to tell Google that I\u0026rsquo;ve not interested in AI results, so don\u0026rsquo;t give them to me.\nThe search string I\u0026rsquo;m using is: https://www.google.com/search?q=%s\u0026amp;udm=14\u0026amp;tbs=li:1\nIn other words, when searching via. Google, pass it the term you are looking for — that\u0026rsquo;s the ?q=%s – and apply two special parameters. The first tells Google to exclude AI-generated overviews from the results it returns, and the second parameter (tbs=li:1) tells it to use verbatim matching.\nIf you want this by default, simply add a new search engine to Firefox\u0026rsquo;s list. First we have to unlock the list of search engines:\nLaunch Firefox In the address bar, enter about:config; if it warns you to proceed with caution, click Continue Search for an entry called browser.urlbar.update2.engineAliasRefresh, and set it to TRUE Restart Firefox Load the Settings, then the Search page Scroll to the Search Settings at the bottom, and add a custom engine with this Engine URL: https://www.google.com/search?q=%s\u0026amp;udm=14\u0026amp;tbs=li:1 Save At the top of the Search page, set the default Search Engine to the one that you just created Now your searches from the Address Bar will exclude Google\u0026rsquo;s AI content while still using their indexing\nReferences\rAn article I referenced when figuring this out was:\nEurope plots escape hatch\u0026hellip;\n","date":"2025-05-15T21:39:37-04:00","permalink":"https://gbeifuss.github.io/p/exclude-google-ai-summary-with-udm14/","title":"Exclude Google AI summary with udm=14"},{"content":"I was trying to use the VMWare Standalone Converter tool today to P2V a very old physical server, running CentOS 5.2. The age of the OS meant that I had to use Converter v.5 as newer converters would not connect to the source server OS. I also had to build a brand new ESXi 5.5 host, as newer ESXi versions require modern security protocols \u0026amp; ciphers which aren\u0026rsquo;t supported by Converter 5.5.\nAfter setting everything up, I ran the Converter wizard. The task failed with the following message:\nFAILED: Unable to obtain the IP address of the helper virtual machine\nIt turns out that the VM receiving the data couldn\u0026rsquo;t obtain an DHCP address from the network. This, in turn, meant there was no IP for the Converter to send the data to.\nWhen running through the Conversion wizard, you can set Helper VM Network Options via. the Options screen. Simply assign a compatible static IP and the Helper VM will boot with those settings.\nAfter manually setting the IP, the migration task proceeded normally.\n","date":"2024-08-15T17:48:28-04:00","permalink":"https://gbeifuss.github.io/p/vmware-converter-error/","title":"VMWare Converter Error"},{"content":"This website is primarily generated through hugo. I recently switched my main computer from a Surface Pro (v4) to a Dell Latitude 7320, which means that I had to reconfigure my website authoring environment from scratch. Along the way, there were lots of little hiccups because of a) changes to hugo since the last install and b) steps and tweaks that I\u0026rsquo;d not documented anywhere. It took a couple of hours, but everything is configured and publishing the way that I like again.\nGenerally speaking, I:\ntook a backup of my existing site installed hugo-extended and git created my hugo site used git to link my site with github pages restored files published I started by creating a .zip backup of my existing files. Then, I deleted all the existing files as they are in OneDrive. I don\u0026rsquo;t want the cloud copies being automatically pushed back down to my laptop while I\u0026rsquo;m configuring everything.\nNext, I installed hugo-extended and git with chocolately package manager and powershell:\nchocolatey install hugo-extended git -y\nAfter that, I created my directories again, and told hugo that this was a new site:\nC:\\Users\\Greg\\OneDrive\\myblog.dev.repo\u0026gt;hugo new site . Congratulations! Your new Hugo site was created in C:\\Users\\Greg\\OneDrive\\myblog.dev.repo. Just a few more steps... 1. Change the current directory to C:\\Users\\Greg\\OneDrive\\myblog.dev.repo. 2. Create or install a theme: - Create a new theme with the command \u0026#34;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026#34; - Or, install a theme from https://themes.gohugo.io/ 3. Edit hugo.toml, setting the \u0026#34;theme\u0026#34; property to the theme name. 4. Create new content with the command \u0026#34;hugo new content \u0026lt;SECTIONNAME\u0026gt;\\\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026#34;. 5. Start the embedded web server with the command \u0026#34;hugo server --buildDrafts\u0026#34;. See documentation at https://gohugo.io/. C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\u0026gt;dir Volume in drive C has no label. Volume Serial Number is D8FC-8442 Directory of C:\\Users\\Greg\\OneDrive\\myblog.dev.repo 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; . 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; .. 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; archetypes 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; assets 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; content 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; data 2024-07-10 11:53 AM 83 hugo.toml 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; i18n 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; layouts 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; static 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; themes 1 File(s) 83 bytes 10 Dir(s) 434,143,707,136 bytes free C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\u0026gt;hugo server Watching for changes in C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\{archetypes,assets,content,data,i18n,layouts,static} Watching for config changes in C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\hugo.toml Start building sites … hugo v0.128.2-de36c1a95d28595d8243fd8b891665b069ed0850+extended windows/amd64 BuildDate=2024-07-04T08:13:25Z VendorInfo=gohugoio WARN found no layout file for \u0026#34;html\u0026#34; for kind \u0026#34;home\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. WARN found no layout file for \u0026#34;html\u0026#34; for kind \u0026#34;taxonomy\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. | EN -------------------+----- Pages | 4 Paginator pages | 0 Non-page files | 0 Static files | 0 Processed images | 0 Aliases | 0 Cleaned | 0 Built in 7 ms Environment: \u0026#34;development\u0026#34; Serving pages from disk Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\u0026gt;dir Volume in drive C has no label. Volume Serial Number is D8FC-8442 Directory of C:\\Users\\Greg\\OneDrive\\myblog.dev.repo 2024-07-10 12:15 PM \u0026lt;DIR\u0026gt; . 2024-07-10 12:15 PM \u0026lt;DIR\u0026gt; .. 2024-07-10 12:15 PM 0 .hugo_build.lock 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; archetypes 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; assets 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; content 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; data 2024-07-10 11:53 AM 83 hugo.toml 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; i18n 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; layouts 2024-07-10 12:15 PM \u0026lt;DIR\u0026gt; public 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; static 2024-07-10 11:53 AM \u0026lt;DIR\u0026gt; themes 2 File(s) 83 bytes 11 Dir(s) 434,142,384,128 bytes free I launched \u0026amp; killed hugo (above) just so that it can create all the files and directories that aren\u0026rsquo;t automatically generated (like /public). I then grabbed a copy of the stack theme and installed it in the themes directory:\nC:\\Users\\Greg\\OneDrive\\myblog.dev.repo\u0026gt;git init Initialized empty Git repository in C:/Users/Greg/OneDrive/myblog.dev.repo/.git/ C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\u0026gt;git submodule add git@github.com:gbeifuss/hugo-theme-stack themes/hugo-theme-stack Cloning into \u0026#39;C:/Users/Greg/OneDrive/myblog.dev.repo/themes/hugo-theme-stack\u0026#39;... remote: Enumerating objects: 3111, done. remote: Total 3111 (delta 0), reused 0 (delta 0), pack-reused 3111 Receiving objects: 100% (3111/3111), 852.37 KiB | 2.10 MiB/s, done. Resolving deltas: 100% (1964/1964), done. warning: in the working copy of \u0026#39;.gitmodules\u0026#39;, LF will be replaced by CRLF the next time Git touches it After this, I restored all all of the necessary configuration files \u0026amp; assets from the backup into their appropriate locations. hugo.toml, archtypes/default.md, /static, /layout, /contents, etc.\nThe next step is to link this back up with my github page. I did grab a copy of my id_ed25519.pub file (has my SSH key for github authentication) from my old laptop and validated a successful connection:\nC:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt;ssh -T git@github.com Hi gbeifuss! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt;git remote add origin https://github.com/gbeifuss/gbeifuss.github.io.git C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt;git remote -v origin https://github.com/gbeifuss/gbeifuss.github.io.git (fetch) origin https://github.com/gbeifuss/gbeifuss.github.io.git (push) C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt;git push origin main error: src refspec main does not match any error: failed to push some refs to \u0026#39;https://github.com/gbeifuss/gbeifuss.github.io.git\u0026#39; C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt;git checkout -b main Switched to a new branch \u0026#39;main\u0026#39; C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt;git push --force origin main Enumerating objects: 372, done. Counting objects: 100% (372/372), done. Delta compression using up to 8 threads Compressing objects: 100% (230/230), done. Writing objects: 100% (372/372), 1.74 MiB | 1.72 MiB/s, done. Total 372 (delta 152), reused 0 (delta 0), pack-reused 0 (from 0) remote: Resolving deltas: 100% (152/152), done. To https://github.com/gbeifuss/gbeifuss.github.io.git + e4d1dbd...cccf12b main -\u0026gt; main (forced update) C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt;git push origin main Everything up-to-date C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt; Finally, create new content:\nC:\\Users\\Greg\\OneDrive\\myblog.dev.repo\u0026gt;hugo new content/post/esxi_vmware_converter_error.md Content \u0026#34;C:\\\\Users\\\\Greg\\\\OneDrive\\\\myblog.dev.repo\\\\content\\\\post\\\\esxi_vmware_converter_error.md\u0026#34; created Use hugo server -D to run the server in Draft mode, showing all published and draft content. Use hugo to create the entire static copy of the site for publishing.\n","date":"2024-07-10T14:34:17-04:00","permalink":"https://gbeifuss.github.io/p/website-publishing-with-hugo-and-github/","title":"Website Publishing with Hugo and GitHub"},{"content":"I needed to spin up an SFTP server to validate that a backup of our NSX appliance was working, so I tried a couple of free/lightweight SFTP applications that ended up not working (likely because they didn\u0026rsquo;t support the correct encryption keys/ciphers/algorithms). Dell Support recommends OpenSSH 8.1, so I decided to install the OpenSSH Server that comes with Windows 11.\nInstall it via. Start \u0026gt; Optional Features \u0026gt; Add \u0026gt; OpenSSH Server\nAfter the installation is complete, validate that the service is running. I believe it is Stopped by default, so start it up.\nServices.msc \u0026gt; OpenSSH SSH Server \u0026gt; Start\nI then validated that it was working with the following command: ssh localhost -v This command tries to connect to the SSH server and authenticate as the current user. Using -v enables verbose mode, which is helpful for seeing the server host key (fingerprint).\nNext, I created a new user account, because I didn\u0026rsquo;t want to enter my domain user credentials in the NSX backup configuration window, especially during a vendor call. Computer Management \u0026gt; Users \u0026gt; Create\nI then tried running an NSX backup, which failed. After checking Windows Firewall (a rule had already been created to allow :22), I decided to edit the SSH Server configuration since it seemed to be listening only via. localhost, not my IP address. Open c:\\programdata\\ssh\\sshd_config and uncomment the #ListenAddress 0.0.0.0 entry, which tells SSH Server to listen on all IPv4 IPs known to the system.\nRestart the OpenSSH SSH Server service for the changes to take effect.\nConnecting to the IP now worked: ssh 10.1.1.15 -v\nI then tried connecting with the new user I\u0026rsquo;d created: ssh tester@10.1.1.15 -v, which also worked.\nRetrying the NSX backup resulted in a successful backup\n","date":"2024-07-09T10:12:04-04:00","permalink":"https://gbeifuss.github.io/p/installing-openssh-server-on-windows-11/","title":"Installing OpenSSH Server on Windows 11"},{"content":"One of my favourite \u0026rsquo;listening experiences\u0026rsquo; is the radio show Higher Ground on CUIT 89.5. It\u0026rsquo;s hosted by Jason Palma and I\u0026rsquo;ve been listening for 20+ years.\ntldr; I\u0026rsquo;ve been downloading the show for many years\nWhen I first discovered this show circa. 2000, the station allowed streaming of that week\u0026rsquo;s show with RealAudio. I discovered that the stream file could be opened, giving me the URL for the underlying .ra file. I faithfully downloaded the file weekly, and listened to the shows while working on my college labs. Later on, the station quit that, and the show would post the MP3s on a third party site. I downloaded all of those too, which were shows from 2008-2010.\nSometime in/around September 2014, I managed to infect my laptop with CryptoWall (1.0). In the process of recovering/reformatting my laptop, I erased all of my historical archives of Higher Ground shows. The only ones I had left were encrypted copies of the 2008-2010 shows. Of course, by 2014 the site hosting the originals had quit offering that service, so there was no way to re-download them. The station at that point was only hosting the .mp3 for the current show - each week a new file was posted for streaming.\nBy then I was into Powershell and scripting, so I set up a scheduled task that ran my script to download the .mp3 file each week. This ran faithfully for a few years, until the station changed things again. Higher Ground was by this time posting the files to Podbean.com.\nI use OMV to host a SMB share plus a small Jellyfin instance. I figured that with its embedded support for Docker, this would be a great opportunity to have OMV handle the downloading for me. I found a great docker container (ichapod) that will parse a podcast playlist and download all the files, including prefixing the filename with the posted date so that I can listen in order. If there\u0026rsquo;s nothing to download, ichapod will terminate.\nOMV \u0026gt; Services \u0026gt; Compose \u0026gt; Files \u0026gt; Add\nI used this configuration file:\nversion: \u0026#39;3\u0026#39; services: ichapod: image: shikasta/ichapod build: . volumes: - \u0026#34;./podcasts.yml:/var/lib/ichapod/podcasts.yml\u0026#34; - \u0026#34;/srv/dev-disk-by-uuid-964d6de1-d26d-4f07-b743-30799f1c511d/backup/archive:/podcasts\u0026#34; ##saves to Higher Ground Radio folder command: - \u0026#34;/var/lib/ichapod/podcasts.yml\u0026#34; - \u0026#34;/podcasts\u0026#34; user: \u0026#34;1000:10000\u0026#34; restart: \u0026#39;always\u0026#39; The volumes entries tell the Docker instance to present the podcasts.yml file as /var/lib/ichapod/podcasts.yml, which is where ichapod expects to find the list of podcasts. I also present my /backup/archive directory as /podcasts, which is where ichapod will create a directory named after the podcast series, and save the playlist files.\nI created a podcasts.yml file with the following content:\n--- - series: Higher Ground Radio url: https://feed.podbean.com/ciuthigherground/feed.xml References\rSome articles I referenced when figuring this out were:\nhttps://hub.docker.com/r/shikasta/ichapod\nhttps://github.com/shikasta-net/Ichapod\nHigher Ground Radio\nHigher Ground Radio Facebook Group\n","date":"2024-04-11T10:11:58-04:00","permalink":"https://gbeifuss.github.io/p/configuring-docker-instance-of-ichapod/","title":"Configuring Docker instance of ichapod"},{"content":"Windows Terminal is one of my most loved features of recent Windows releases. Seriously Microsoft, where was this application in Windows 2000? I can launch Powershell, Command Prompt and ssh sessions from the same window - simply amazing! (Yes, I know that Linux has supported this for decades).\nI wanted to create a profile that would let me administer my omv Linux server, without having to input credentials every. single. time. To do that, a public/private key needs to be created, before copying the public key over to the destination server. Then, when I connect via. SSH and present my private key, the connection is transparently (to me) approved.\nStart by running the command ssh-keygen -t rsa to generate a keypair. I accepted all the defaults, and skipped a passphrase:\nC:\\Users\\Greg\u0026gt;ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (C:\\Users\\Greg/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in C:\\Users\\Greg/.ssh/id_rsa. Your public key has been saved in C:\\Users\\Greg/.ssh/id_rsa.pub. The key fingerprint is: SHA256:IaBSPSCx3y416+iY7qLYbMf9FoxBNhSNbAR/SMRh5yM greg@DESKTOP-SU06DHB The key\u0026#39;s randomart image is: +---[RSA 3072]----+ |o.oooBO=. | | + .o=O+. | |o . =E.= | | o . .+ o | | . + +S | | o o. o | | ..o. . | |o=.+o . . | |X==.. o. | +----[SHA256]-----+ C:\\Users\\Greg\u0026gt; Now we copy the public key over to the omv server:\ncat ~/.ssh/id_rsa.pub | ssh root@192.168.4.107 \u0026quot;mkdir -p ~/.ssh \u0026amp;\u0026amp; chmod 700 ~/.ssh \u0026amp;\u0026amp; cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys\u0026quot;\nFinally, we setup the connection profile in Windows Terminal via. Launch Terminal \u0026gt; Settings \u0026gt; Open JSON Paste this configuration string into the profiles section. You\u0026rsquo;ll need to update the username and IP address to reflect your situation:\n{ \u0026#34;commandline\u0026#34;: \u0026#34;ssh -i \\\u0026#34;~/.ssh/id_rsa\\\u0026#34; root@192.168.4.107\u0026#34;, \u0026#34;font\u0026#34;: {\u0026#34;size\u0026#34;: 10.0}, \u0026#34;guid\u0026#34;: \u0026#34;{0caa0dad-35be-5f56-a8ff-afceaeaa6103}\u0026#34;, \u0026#34;hidden\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;OMV Server\u0026#34; } Save the settings, then test the connection!\n","date":"2024-03-05T10:13:04-04:00","permalink":"https://gbeifuss.github.io/p/configure-ssh-access-via.-windows-terminal-a-private-key/","title":"Configure SSH access via. Windows Terminal \u0026 a private key"},{"content":"Someone on our VDI team informed me that they were getting an error when trying to log into their VSCA: \u0026ldquo;An error occurred during authentication.\u0026rdquo; I started with the usual troubleshooting - did it work with my credentials? (No, it did not). Did it work in a private browser window, to rule out a caching issue? (No, it did not).\nI tried logging in locally, but none of the accounts that should have worked, actually worked. So, before actually troubleshooting the issue, I had to break into the VCSA. Following the advice in Resetting root password in vCenter Server Appliance 6.5 / 6.7 / 7.x (2147144), I:\ntook a snapshot (including memory) of the VCSA as currently configured Reboot the vCenter Server Appliance. After the VCSA Photon OS starts, press the e key to enter the GNU GRUB Edit Menu. Locate the line that begins with the word Linux, then appending these entries to the end of the line: rw init=/bin/bash Press F10 to continue booting. Ran the command mount -o remount,rw / In the Command prompt, enter the command passwd and provide a new root password (twice for confirmation): passwd Unmount the filesystem by running the umount command: umount / Reboot the vCenter Server Appliance via reboot -f Confirm that you can access the vCenter Server Appliance using the new root password. At that point, I was finally in via CLI. I then had to reset the password for administrator@vsphere.local. I followed the steps in How to unlock and reset SSO password in vSphere 6.x/7.x using the vdcadmintool (2146224) to use the /usr/lib/vmware-vmdir/bin/vdcadmintool tool for this.\nA look in /var/log/vmware/vsphere-ui/logs/vsphere_client_virgo.log showed entries similar to this:\nThe SSL certificate does not match when connecting to the vCenter Single Sign-On. com.vmware.vim.vmomi.client.exception.VlsiCertificateException: Server certificate chain is not trusted and thumnprint verification is not configured. ![VSCA Log Errors](/img/esxi-vcsa-cert-log.png) That confirmed that this was likely a certificate error. I started by checking the STS certificate, following the advice in Checking Expiration of STS Certificate on vCenter Servers (79248), but the STS certificate was valid until 2029.\nI then looked at the other certificates by using check-trust-anchors -cml, which showed that the certificates had indeed expired 10 days ago: I followed the advice in How to regenerate vSphere 6.x, 7.x, and 8.0 certificates using self-signed VMCA (2112283) to generate a new certificate:\nConnect via. SSL or console Launch the vSphere 6.x Certificate Manager: /usr/lib/vmware-vmca/bin/certificate-manager Select Option 4 (Regenerate a new VMCA Root Certificate and replace all certificates) Type the administrator@vsphere.local password when prompted. Enter the appropriate values when prompted by the VMCA Confirm proceeding: You are going to regenerate Root Certificate and all other certificates using VMCA. Continue operation : Option[Y/N] ? : Y After 5 minutes, the operation completed and the new certificates were in place. Refreshing the VCSA website allowed me to log in, and the VDI team confirmed that everything was working again. Hurrah!\n","date":"2023-07-19T13:28:02-04:00","permalink":"https://gbeifuss.github.io/p/esxi-vcentre-certificates/","title":"ESXI VCentre Certificates"},{"content":"I worked on an issue at work today that was really interesting (from a technical troubleshooting perspective), so I thought that I\u0026rsquo;d write about it.\nOur organization is in the midst of moving from one datacentre to another. The PKI team reported that their new, just-spun-up servers at the new site were all performing very poorly. Their servers were all running at about 50% CPU usage on an 8 vCPU, 16GB RAM server, but took forever to boot or shutdown, and the GUI had noticible lag when clicking or moving around windows. They suspected a CPU issue - instead of having their 8 vCPU allocated as 8 sockets x 1 core each, they wanted 2 sockets x 4 core, or 1 socket x 8 core because they believed this would improve performance.\nAccording to VMWare, this allocation doesn\u0026rsquo;t make any real difference in terms of performance since ESXi 6.5/6.7 (but may be needed for licensing compliance).\nHere\u0026rsquo;s what the VM metrics looked like: Migrating to another ESXi host in the cluster made no difference. The host servers in the cluster were all running at about 50% load, so it didn\u0026rsquo;t seem to be an oversubscription issue (my first suspicion).\nEven doubling the resources (to 16 vCPU, 32GB RAM) on a VM made no real difference in performance, although I could see that the VM CPU load dropped to about 30% from 50%.\nI found the CPU Ready Time metric and after reading up on what it was and what acceptable values should be, figured out that the problem was that the VM\u0026rsquo;s CPU couldn\u0026rsquo;t get scheduled time on the host server.\nCPU Ready Time in VSphere is a metric that records the amount of time that a VM is ready to use CPU, but cannot because all CPU resources (on an ESXi host) are busy.\nThe VM in question was averaging 1000 second wait time for each CPU call (1.05M ms) over the prior day. VMWare documentation suggests that CPU Ready Time at or over 5% could indicate a performance problem. I quickly crunched some numbers using the vmcalc.com website. An average wait time of 1046000ms works out to a CPU Ready Time of about 656%\u0026hellip; well above the 5% threshold!\nI compared the PKI Server\u0026rsquo;s CPU Ready Time (1.05M ms) to that of a random server on another cluster, which had an average of just 30 ms!\nI worked with a colleague to find the cause of this CPU throttling, which turned out to be a Resource Group limit set on the entire PKI \u0026lsquo;folder\u0026rsquo;, affecting 27 VMs.\nWe changed the CPU Limit from 30,000 MHz to 60,000 Mhz, effectively doubling the amount of CPU that the servers in this PKI group could consume.\nWe also checked each VM to make sure that limits were not set on individual VMs. Each one did have the CPU capped to 19200 MHz, which works out to 2.4GHz (when all 8 vCPU are considered). An effective 2.4GHz vCPU rate should be enough to run PKI, so this was contributing to the performance issue that the PKI team was experiencing.\nAfter applying the Limit change, we immediately saw the CPU Ready Time drop on the PKI VMs to something in the 300-400 ms range.\nIt doesn\u0026rsquo;t show well in this image because the scale is too vast, but you can clearly see it drop off from 40000-80000 ms to a negligable level.\n","date":"2023-04-03T17:00:36-04:00","permalink":"https://gbeifuss.github.io/p/esxi-cpu-ready-time-statistics/","title":"ESXi CPU Ready Time statistics"},{"content":"Last week I found some Shadow IT at work - a single business-managed ESXi server hosting legacy VMs for code testing. We have an arm of our business that supports products for 25 years, so releases need to be tested on Windows XP servers and other OSes of that era. In any case, this server was happily chugging along \u0026hellip; on ESXi 5.0! Needless to say, I need to bring these VMs into the IT-fold and migrate them to a modern ESXi server: VxRail running ESXi 7.0. But with such an obsolete product, migration can\u0026rsquo;t rely on some of the modern tools that I normally rely on.\nThat meant it was time for Open Virtualization Formal Tool (ovftool) to come to my rescue! This is a VMWare tool used to manipulate VMs to OVFs (or OVAs), OVFs to VMs, and VM-to-VM migrations. Exactly what I need! The only caveat is that that no vmotion is possible - the VMs need to be powered down.\nI downloaded ovftool 4.4.3 to my laptop and got to work. The syntax for a VM-VM migration on a VxRail took me quite a bit of time to figure out, because the documentation isn\u0026rsquo;t very helpful, and all of examples I found with Google-fu claimed that I could just use the destination server name or IP. (Hint: I couldn\u0026rsquo;t.)\nThe source server, source VM, destination storage and destination server (and credentials!) are needed, according to the documentation. Also, the datastore is the destination file system, not the source.\nC:\\apps\\ovftool\u0026gt;ovftool.exe vi://root:password@source.server.hostname/AMT-1800.BuildServer vi://userid@domain.com@destination.server.hostname -ds=vxrail-vsan1 Error: Unexpected option: -ds=vxrail-vsan1 Completed with errors I took a quick look at the help file and determined I should be providing the information like this: Usage: ovftool [options] \u0026lt;source\u0026gt; [\u0026lt;target\u0026gt;]\nI found that I could specify only the information above, but then I\u0026rsquo;d be prompted for credentials each time I ran the command, and typing in credentials became old very fast\u0026hellip; so I added them to the command:\nC:\\apps\\ovftool\u0026gt;ovftool.exe -ds=vxrail-vsan1 vi://root:password@source.server.hostname/AMT-1800.BuildServer vi://userid@domain.com@destination.server.hostname Opening VI source: vi://root@ip.address:443/AMT-1800.BuildServer Opening VI source: vi://root@ip.address:443/AMT-1800.BuildServer Enter login information for target vi://destination.server.hostname/ Username: username@hostname Password: **************** Error: Found wrong kind of object (Folder). Possible completions are: site/ Completed with errors Clearly I need to add \u0026lsquo;site/\u0026rsquo; to the destination server path, so that OVFTool knows where to create the file. Note that I need to add \u0026ldquo;host/\u0026rdquo; too:\nC:\\apps\\ovftool\u0026gt;ovftool.exe -ds=vxrail-vsan1 vi://root:password@source.server.hostname/AMT-1800.BuildServer vi://h488735a@global.ds.honeywell.com@destination.server.hostname/site/host Opening VI source: vi://root@ip.address:443/AMT-1800.BuildServer Opening VI source: vi://root@ip.address:443/AMT-1800.BuildServer Enter login information for target vi://destination.server.hostname/ Username: username@hostname Password: **************** Error: Found wrong kind of object (Folder). Possible completions are: site-vxrail/ Completed with errors sigh another folder to add to the destination server path. Your path may differ based on the heirarchy of your ESXi cluster. ovftool will continue generating an error about the Folder if you\u0026rsquo;re not at the final path:\nC:\\apps\\ovftool\u0026gt;ovftool.exe -ds=vxrail-vsan1 vi://root:password@source.server.hostname/AMT-1800.BuildServer vi://h488735a@global.ds.honeywell.com@destination.server.hostname/site/host/site-vxrail/ Opening VI source: vi://root@ip.address:443/AMT-1800.BuildServer Opening VI source: vi://root@ip.address:443/AMT-1800.BuildServer Enter login information for target vi://destination.server.hostname/ Username: username@hostname Password: **************** Opening VI target: vi://username@hostname@destination.server.hostname:443/site/host/site-vxrail/ Error: No network mapping specified. OVF networks: VM Network. Target networks: Management Network-92c1a646-e30f-436e-bfd6-b05f32371c4d Virtual SAN-92c1a646-e30f-436e-bfd6-b05f32371c4d VxRail Management-92c1a646-e30f-436e-bfd6-b05f32371c4d site-VLAN101 site-VLAN200 site-VLAN210 vCenter Server Network-92c1a646-e30f-436e-bfd6-b05f32371c4d vSphere vMotion-92c1a646-e30f-436e-bfd6-b05f32371c4d Completed with errors Oh, this is finally promising! I need to add the network mapping to the command, so I pick the appropriate target network (in this case, site-VLAN210)\nC:\\apps\\ovftool\u0026gt;ovftool.exe -ds=vxrail-vsan1 vi://root:password@source.server.hostname/AMT-1800.BuildServer vi://h488735a@global.ds.honeywell.com@destination.server.hostname/site/host/site-vxrail/ -nw=site-VLAN210 Error: Unexpected option: -nw=site-VLAN210 Completed with errors Grrrr\u0026hellip; foiled again by the order of the information I provide. One more time!\nC:\\apps\\ovftool\u0026gt;ovftool.exe -ds=vxrail-vsan1 -nw=site-VLAN210 vi://root:password@source.server.hostname/AMT-1800.BuildServer vi://h488735a@global.ds.honeywell.com@destination.server.hostname/site/host/site-vxrail/ Opening VI source: vi://root@ip.address:443/AMT-1800.BuildServer Opening VI source: vi://root@ip.address:443/AMT-1800.BuildServer Enter login information for target vi://destination.server.hostname/ Username: username@hostname Password: **************** Opening VI target: vi://username@hostname@destination.server.hostname:443/site/host/site-vxrail/ Deploying to VI: vi://username@hostname@destination.server.hostname:443/site/host/site-vxrail/ Transfer Completed Completed successfully C:\\apps\\ovftool\u0026gt; Hurrah! A quick check on the VxRail and I can see that the VM has been migrated, stored on the right datastore and joined to the right network. I power it on as a quick test and confirm that I can ping it. Success!\n","date":"2023-03-13T15:38:51-04:00","permalink":"https://gbeifuss.github.io/p/using-ovftool-to-migrate-vms-between-esxi-hosts/","title":"Using ovftool to migrate VMs between ESXi hosts"},{"content":"I ran into an issue this week where a work server stopped booting after a linux kernel patch (3.10.0-xxxx.83.xxxx) was deployed.\nThe server would always get \u0026lsquo;stuck\u0026rsquo; trying to load drivers for an Avocent USB Keyboard/Mouse. These seemed to be the Dell iDRAC drivers, because the virtual console lacked keyboard/mouse inputs.\nIf the old kernel (3.10.0-1160.80.xxxx) was selected during boot, the OS loaded properly. I\u0026rsquo;m not a Unix expert, so I looped in one of my colleagues.\nHe explained that the kernel was trying to use old (or corrupted) drivers from the initrd (initial ram disk) scheme. These drivers can be rebuilt by running yum install kernel.\nAfter running the command, I rebooted the server which booted properly.\nDell seems to have several similar KBs on this issue, like this one about Drivers available in OEMDRV drive are not installed during the operating system installation\n","date":"2023-03-06T15:21:01-05:00","permalink":"https://gbeifuss.github.io/p/dell-usb-driver-halts-linux-boot/","title":"Dell USB driver halts Linux boot"},{"content":"I was patching ESXi hosts at work (again!) and ran into an issue with dependencies.\nUpdate Manager generated an error when trying to patch ESXi 6.5, so I tried updating from the CLI:\n[root@server00:~] esxcli software vib update -d \u0026#34;/vmfs/volumes/ecd3afaf-295422fc/VMware-VMvisor-Installer-6.5.0.update03-20502893.x86_64-DellEMC_Customized-A10.zip\u0026#34; --dry-run [DependencyError] VIB Brocade_bootbank_scsi-bfa_3.1.0.0-1OEM.500.0.0.472560 requires vmkapi_2_0_0_0, but the requirement cannot be satisfied within the ImageProfile. VIB Brocade_bootbank_net-bna_3.1.0.0-1OEM.500.0.0.472560 requires com.vmware.driverAPI-9.2.0.0, but the requirement cannot be satisfied within the ImageProfile. VIB Brocade_bootbank_net-bna_3.1.0.0-1OEM.500.0.0.472560 requires vmkapi_2_0_0_0, but the requirement cannot be satisfied within the ImageProfile. VIB Brocade_bootbank_scsi-bfa_3.1.0.0-1OEM.500.0.0.472560 requires com.vmware.driverAPI-9.2.0.0, but the requirement cannot be satisfied within the ImageProfile. Please refer to the log file for more details. [root@server00:~] One server was patched by downloading the DellEMC-ESXi-6.5U3-20502893-A10.iso image to the Update Manager and creating a baseline from it. I then attached it to the host and let it Remediate.\nAnother server didn\u0026rsquo;t upgrade with the custom baseline I created, so I tried to remove the dependencies via. CLI:\n[root@server00:~] esxcli software vib list | grep -i brocade net-bna 3.1.0.0-1OEM.500.0.0.472560 Brocade VMwareCertified 2014-02-19 scsi-bfa 3.1.0.0-1OEM.500.0.0.472560 Brocade VMwareCertified 2014-02-19 [root@server00:~] esxcli software vib remove -n net-bna [DependencyError] VIB Brocade_bootbank_scsi-bfa_3.1.0.0-1OEM.500.0.0.472560 requires com.vmware.driverAPI-9.2.0.0, but the requirement cannot be satisfied within the ImageProfile. VIB Brocade_bootbank_scsi-bfa_3.1.0.0-1OEM.500.0.0.472560 requires vmkapi_2_0_0_0, but the requirement cannot be satisfied within the ImageProfile. Please refer to the log file for more details. [root@server00:~] esxcli software vib remove -n scsi-bfa [DependencyError] VIB Brocade_bootbank_net-bna_3.1.0.0-1OEM.500.0.0.472560 requires com.vmware.driverAPI-9.2.0.0, but the requirement cannot be satisfied within the ImageProfile. VIB Brocade_bootbank_net-bna_3.1.0.0-1OEM.500.0.0.472560 requires vmkapi_2_0_0_0, but the requirement cannot be satisfied within the ImageProfile. Please refer to the log file for more details. As the error above shows, that didn\u0026rsquo;t work, because there were sub-requirements that were not met.\nI finally managed to apply the update via. CLI by instructing ESXi that it was okay to remove VIBs not associated with the profile being installed: esxcli software profile install -p DellEMC-ESXi-6.5U3-14320405-A03 -d /vmfs/volumes/datastore/ISO/VMware-VMvisor-Installer-6.5.0.update03-14320405.x86_64-DellEMC_Customized-A03.zip --ok-to-remove\n","date":"2023-03-06T14:27:52-05:00","permalink":"https://gbeifuss.github.io/p/esxi-upgrade-generates-dependency-errors/","title":"ESXi upgrade generates Dependency Errors"},{"content":"Another tale of ESXi patching adventures. My company has some ESXi hosts in remote locations that don\u0026rsquo;t get much care. As long as they\u0026rsquo;re running the VMs and business users don\u0026rsquo;t complain, the hosts don\u0026rsquo;t get much attention.\nToday I was trying to patch a server with all the ESXi 6.5 patches released to date, but it was failing with this error:\n02 / 27 / 2023, 2:14:57 PM The VFAT filesystem mpx.vmhba32:C0:T0:L0:8 (UUID 577b5b9f-bf67afef-822f-246e960d8318) is ... 02 / 27 / 2023, 2:56:00 PM Could not stage image profile \u0026#39;(Updated) ESXi-5.5.0-20150104001-standard\u0026#39;: (\u0026#39;VMware_locker_tools-light_6.5.0-3.191.20448942\u0026#39;, \u0026#39;[Errno 32] Broken pipe\u0026#39;) Some quick searching revealed that this error is likely because the locker partition is out of space. Lo and behold, it was:\n[root@server02:~] ls -ltrh / | grep store lrwxrwxrwx 1 root root 6 Feb 27 20:21 locker -\u0026gt; /store lrwxrwxrwx 1 root root 49 Feb 27 20:21 store -\u0026gt; /vmfs/volumes/577b5b9f-bf67afef-822f-246e960d8318 [root@server02:~] [root@server02: / vmfs/volumes/577b5b9f-bf67afef-822f-246e960d8318/packages] df -h Filesystem Size Used Available Use% Mounted on NFS 4.0T 2.2T 1.8T 55% /vmfs/volumes/vmds05 NFS 4.0T 2.9T 1.1T 72% /vmfs/volumes/vmds06 NFS 4.0T 2.4T 1.6T 61% /vmfs/volumes/vmds04 NFS 4.0T 2.9T 1.1T 73% /vmfs/volumes/vmds03 NFS 4.0T 2.3T 1.7T 56% /vmfs/volumes/vmds02 NFS 7.0T 6.5T 517.3G 93% /vmfs/volumes/vmds01 vfat 285.8M 285.8M 56.0K 100% /vmfs/volumes/577b5b9f-bf67afef-822f-246e960d8318 vfat 249.7M 166.1M 83.6M 67% /vmfs/volumes/930098e3-722a9454-fdea-b663aa3b7053 vfat 249.7M 166.1M 83.6M 67% /vmfs/volumes/3bc3cee8-10f1c748-e8e5-d729b7af1e64 [root@server02:~] df /locker Filesystem Bytes Used Available Use% Mounted on vfat 299712512 299655168 57344 100% /vmfs/volumes/577b5b9f-bf67afef-822f-246e960d8318 Using the disk and partition information in the error, I formatted the partition with vfat:\n[root@server02:~] vmkfstools -C vfat /dev/disks/mpx.vmhba32:C0:T0:L0:8 create fs deviceName:\u0026#39;/dev/disks/mpx.vmhba32:C0:T0:L0:8\u0026#39;, fsShortName:\u0026#39;vfat\u0026#39;, fsName:\u0026#39;(null)\u0026#39; deviceFullPath:/dev/disks/mpx.vmhba32:C0:T0:L0:8 deviceFile:mpx.vmhba32:C0:T0:L0:8 Checking if remote hosts are using this device as a valid file system. This may take a few seconds... Creating vfat file system on \u0026#34;mpx.vmhba32:C0:T0:L0:8\u0026#34; with blockSize 1048576 and volume label \u0026#34;none\u0026#34;. Successfully created new volume: 577b5b9f-bf67afef-822f-246e960d8318 Then, I recreated the symlinks:\nln -snf /vmfs/volumes/577b5b9f-bf67afef-822f-246e960d8318 /store ln -snf /vmfs/volumes/577b5b9f-bf67afef-822f-246e960d8318 /locker After a reboot, I found another host server running the same ESXi version and path level, then copied the contents of the store to my newly formatted partition. After another reboot (just to be safe), I was able to patch the host server without any problems.\nReferences\rAn article I referenced when figuring this out was:\nFixing the broken/corrupt Locker Partition on Esxi\n","date":"2023-03-06T14:27:15-05:00","permalink":"https://gbeifuss.github.io/p/esxi-staging-volume-full/","title":"ESXi Staging Volume Full"},{"content":"I ran into an issue at work where a migraton was halted because of a VM waiting for a VMWare Tools install to finish\u0026hellip; but I didn\u0026rsquo;t want to deal with it right then. Tracking down the owner and getting permission to reboot it outside of the normal schedule was going to be a hassle.\nLuckily, these installs can be cancelled via. SSH on the host server:\nList all the VMs on the host: vim-cmd vmsvc/getallvms Cancel the installation for the specific guest VM by using the ID (from above): vim-cmd vmsvc/tools.cancelinstall \u0026lt;VM ID\u0026gt;\n[root@server01:~] vim-cmd vmsvc/getallvms Vmid Name File Guest OS Version Annotation 1 GUEST24 [vmds03] GUEST24/GUEST24.vmx windows8Server64Guest vmx-13 2 GUEST38 [vmds03] GUEST38/GUEST38.vmx windows8Server64Guest vmx-13 38 GUEST00 [vmds04] GUEST00/GUEST00.vmx rhel6_64Guest vmx-08 Initially created to refresh guest03 - cancelled later Now used as test server. 40 GUEST16 [vmds01] GUEST16_1/GUEST16.vmx rhel7_64Guest vmx-13 45 GUEST09 [vmds01] GUEST09/GUEST09.vmx rhel7_64Guest vmx-13 47 GUEST43 [vmds05] GUEST43/GUEST43.vmx rhel7_64Guest vmx-13 48 GUEST02 [vmds01] GUEST02/GUEST02.vmx rhel5Guest vmx-10 49 GUEST39 [vmds06] GUEST39/GUEST39.vmx windows7Server64Guest vmx-10 [root@server01:~] vim-cmd vmsvc/tools.cancelinstall 48 [root@server01:~] After cancelling the install, I was able to finish the migration.\n","date":"2023-03-06T14:26:42-05:00","permalink":"https://gbeifuss.github.io/p/cancel-vmware-tools-installation-on-esxi/","title":"Cancel Vmware Tools Installation on ESXi"},{"content":"ESXi vulnerablities are in the news recently as they are being exploited en masse\u0026hellip; not that active news coverage should be the sole driver for patching a server. The SLP (Service Location Protocol) service runs as root and parses network input without authentication. Since 2021 the recommendation from VMWare has been to disable the service. Newer versions (ie. 7.0 U2c and 8.0) disable it by default.\nVMWare has an article - How to Disable/Enable the SLP Service on VMware ESXi (76372) - describing how to disable the service.\nEssentially, enable SSH on the host server, then connect to it with pUTTY or a similar tool. Stop the SLD service: /etc/init.d/slpd stop Disable the SLP service: esxcli network firewall ruleset set -r CIMSLP -e 0 Make the change persistent across reboots: chkconfig slpd off Confirm the change is persistent: chkconfig --list | grep slpd\nThe server does not need a reboot for this change to take effect. ","date":"2023-03-06T14:26:07-05:00","permalink":"https://gbeifuss.github.io/p/vmware-esxi-disable-slp/","title":"VMWare ESXi - Disable SLP"},{"content":"I\u0026rsquo;m in the midst of migrating a number of VMs from ESXi 6.5 hosts to ESXi 7.0.3, which means there are newer versions of the VMWare Tools available for the guests. Usually, selecting \u0026ldquo;Upgrade VMWare Tools\u0026rdquo; works seamlessly, but occassionally, the installer cannot properly uninstall the old version of the Tools, which results in the error Error upgrading VMWare Tools.\nAttempting to remove it directly from the guest OS (x64 Windows, in my case) results in the installer prompting for a location for missing file VMWare Tools64.msi.\nThe fix for this is easy, but convoluted.\nDetermine the version of VMWare Tools currently installed. I found the VMWare version-mapping file helpful. Download that version of the VMWare Tools package from VMWare.com. I prefer to use the ISO. Copy the ISO to your guest VM. Mount this ISO (in Windows, this is done by right-clicking and choosing \u0026lsquo;mount\u0026rsquo;) Open a command prompt and browse to the newly mounted drive. Locate the setup.exe file, which should be at the root Type setup.exe /a and press \u0026lt;ENTER\u0026gt;. You\u0026rsquo;ll be prompted for a location to save the extracted files After the files are extracted, re-uninstall VMWare Tools inside the guest. When prompted for the VMWare Tools64.msi file, browse to the location you just finished extracting the files to The uninstall should complete Reboot the guest VM, then install the updated version of VMWare Tools!\n","date":"2022-10-13T09:03:35-04:00","permalink":"https://gbeifuss.github.io/p/vmware-tools-update/install/uninstall-fails/","title":"VMWare Tools Update/Install/Uninstall fails"},{"content":"It seems that a lot of my recent posts are related to my \u0026ldquo;adventures\u0026rdquo; at work. Oh well\u0026hellip; \u0026lt;shrugs\u0026gt; Here\u0026rsquo;s another!\nWe have a legacy SQL server (Windows 2012R2, SQL 2012) that was unmaintained for a number of years until I came along. We had a developer who was nominally responsible for it, but I discovered that even his credentials didn\u0026rsquo;t have full sa rights, and some databases were simply not accessible by him. I decided to turn to a life of crime and break in.\nIs it really breaking in when I follow MS documentation? I suppose not.\nYou can start an instance of SQL Server in single-user mode with either the -m or -f options from the command line. Any member of the computer\u0026rsquo;s local Administrators group can then connect to the instance of SQL Server as a member of the sysadmin fixed server role. When you start the instance in single-user mode, first stop the SQL Server Agent service. Otherwise, SQL Server Agent might connect first, taking the only available connection to the server and blocking you from logging in.\nOpen a Windows PowerShell command. Run it as an Administrator Define the variables to use later: $service_name = \u0026#34;MSSQL`$instancename\u0026#34; $sql_server_instance = \u0026#34;machine_name\\instance\u0026#34; $login_to_be_granted_access = \u0026#34;[CONTOSO\\PatK]\u0026#34; Stop SQL Server service: `net stop $SERVICENAME` Execute a CREATE LOGIN command followed by ALTER SERVER ROLE command\u0026quot; `sqlcmd.exe -E -S $sql_server_instance -Q \u0026#34;CREATE LOGIN $login_to_be_granted_access FROM WINDOWS; ALTER SERVER ROLE sysadmin ADD MEMBER $login_to_be_granted_access; \u0026#34;` Exit, and restart the service normally References\rConnect to SQL Server when system administrators are locked out\n","date":"2022-08-09T15:49:19-04:00","permalink":"https://gbeifuss.github.io/p/recovering-sql-system-admin-rights/","title":"Recovering SQL System Admin rights"},{"content":"We recently installed a VMWare/Dell VXRail deployment at my work to replace older ESXi 6.5 servers. I\u0026rsquo;m transitioning existing systems in the server room to the new VXRail through physical-to-virtual (P2V) or virtual-to-virtual (V2V) migrations. Since VMWare has removed their standalone converter, I\u0026rsquo;m using Veeam for the P2V.\nI had an issue a couple of days ago where I performed a P2V as usual, but the VM was showing up with thick provisioned disks on the vSAN, instead of thin provisioned ones, as the assigned storage policy mandates. I was alerted to this by a vSAN online health check, as the \u0026ldquo;Thick-provisioned VMs on vSAN\u0026rdquo; alert tripped. This alarm triggers when vSAN detects thick-provisioned VMs, which in my case was occurring despite the presence of Object Space Reservation = 0 (thin provisioned) in the applied vSAN Storage Policy.provisioned.\nThere is no way in ESXi 7 to forcibly re-apply the currently-assigned storage policy, so I fixed this by cloning the existing policy, applying that, then reapplying the original policy again.\nOpen the Policies and Profiles menu, and select VM Storage Policies. Select the existing vSAN storage policy and click Clone. We\u0026rsquo;ll create an exact copy of the existing profile and apply it to the problem disks. Assign a new name. I appended \u0026ldquo;Clone\u0026rdquo; to the end of my existing policy name. Press Next. There is nothing to change under 2 Policy Structure. Press Next. Under 3 vSAN, select the Advanced Policy Rules tab. Ensure that Object Space Reservation = Thin Provisioning. Press Next. There is nothing to change under 4 Storage Compatibility. Press Next. There is nothing to change under 5 Review and Finish. Verify that Object Space Reservation = Thin Provisioning. Press Finish. Right-click the VM that has Thick Provisioned disks, select VM Policies \u0026gt; Edit VM Storage Policies. Change the applied Storage Policy to the newly created one. After selecting the new policy, the edit policy wizard will display the change (decrease) in storage consumption. Click OK to apply the change.\nWait for the policy to apply, which could take 10-15 minutes, depending on the size of your disks. The disk should now show that it\u0026rsquo;s Thin Provisioned.\nAfter this is complete, reapply the original policy.\n","date":"2022-08-08T20:06:08-04:00","permalink":"https://gbeifuss.github.io/p/thick-provisioned-vms-on-vsan-fixing-a-vsan-alert/","title":"Thick-provisioned VMs on vSAN - Fixing a vSAN Alert"},{"content":"I came across a Linux Database VM at work that desperately needed more disk space. The DBA outlined how they wanted the storage to be allocated, so I set about with the work!\nFirst, I added new disks in ESXi to match the disk space requested. I added space in 500 or 1000 GB blocks to give flexibility with storage and vmotion. Disks were balanced across SCSI controllers as well.\nThen, I logged into the linux VM with admin credentials.\nAt a high-level, I\u0026rsquo;ll:\nCapture the existing disk statistics \u0026amp; configuration for reference: fdisk -l lvs df -TH | grep Disk lsblk Add space to the disk group, then expand the logical volume to make use of that space: sudo vgextend oralogvg /dev/sdaa sudo pvresize /dev/sdaa sudo lvextend -l+100%FREE /dev/oralogvg/oraloglv sudo xfs_growfs /dev/oralogvg/oraloglv VMWare has an article outlining this process (Extending a logical volume in a virtual machine running Red Hat or Cent OS), but it assumes the creation of a disk partition prior to adding it to the disk group. The existing disks I dealt with aren\u0026rsquo;t configured like that, so I didn\u0026rsquo;t want to deviate from what\u0026rsquo;s already in place.\nI dumped the current disks to determine how Linux has labelled the new disks that I added in ESXi. They should be at the end of the list and not be mapped to any logical volumes or disk groups:\n[admin@DB030035 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 68G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 1G 0 part /boot └─sda3 8:3 0 62G 0 part ├─vg00-LogVol00_root 253:0 0 5G 0 lvm / ├─vg00-LogVol00_swap 253:1 0 22G 0 lvm [SWAP] ├─vg00-LogVol00_usr 253:2 0 10G 0 lvm /usr ├─vg00-LogVol00_vlogaudit 253:10 0 2G 0 lvm /var/log/audit ├─vg00-LogVol00_vlog 253:11 0 2G 0 lvm /var/log ├─vg00-LogVol00_vtmp 253:12 0 2G 0 lvm /var/tmp ├─vg00-LogVol00_tmp 253:13 0 5G 0 lvm /tmp ├─vg00-LogVol00_var 253:14 0 5G 0 lvm /var ├─vg00-LogVol00_opt 253:15 0 6G 0 lvm /opt └─vg00-LogVol00_home 253:16 0 2G 0 lvm /home sdb 8:16 0 220G 0 disk └─oraredo1vg-oraredo1lv 253:20 0 370G 0 lvm /oraredo1 sdc 8:32 0 12G 0 disk └─oratempvg-oratemplv 253:5 0 12G 0 lvm /oratemp sdd 8:48 0 10G 0 disk └─flash_recovery_area_vg-flash_recovery_area_lv 253:6 0 10G 0 lvm /flash_recovery_area sde 8:64 0 256G 0 disk └─oraredo3vg-oraredo3lv 253:4 0 406G 0 lvm /oraredo3 sdf 8:80 0 257G 0 disk └─oraredo4vg-oraredo4lv 253:18 0 407G 0 lvm /oraredo4 sdg 8:96 0 70G 0 disk └─oraclevg-oraclelv 253:21 0 70G 0 lvm /oracle sdh 8:112 0 30G 0 disk └─oradatavg-oradatalv 253:17 0 30G 0 lvm /oradata sdi 8:128 0 220G 0 disk └─oraredo2vg-oraredo2lv 253:19 0 370G 0 lvm /oraredo2 sdj 8:144 0 1.4T 0 disk └─oradata1vg-oradata1lv 253:8 0 11T 0 lvm /oradata1 sdk 8:160 0 1.4T 0 disk └─oradata1vg-oradata1lv 253:8 0 11T 0 lvm /oradata1 sdl 8:176 0 1.4T 0 disk └─oradata1vg-oradata1lv 253:8 0 11T 0 lvm /oradata1 sdm 8:192 0 20G 0 disk └─oraidxvg-oraidxlv 253:9 0 20G 0 lvm /oraidx sdn 8:208 0 22G 0 disk └─oralogvg-oraloglv 253:7 0 22G 0 lvm /oralog sdo 8:224 0 25G 0 disk └─oraexportvg-oraexportlv 253:3 0 2.7T 0 lvm /oraexport sdp 8:240 0 1.4T 0 disk └─oradata1vg-oradata1lv 253:8 0 11T 0 lvm /oradata1 sdq 65:0 0 1.4T 0 disk └─oradata1vg-oradata1lv 253:8 0 11T 0 lvm /oradata1 sdr 65:16 0 1.4T 0 disk └─oradata1vg-oradata1lv 253:8 0 11T 0 lvm /oradata1 sds 65:32 0 1000G 0 disk └─sds1 65:33 0 1000G 0 part └─oradata1vg-oradata1lv 253:8 0 11T 0 lvm /oradata1 sdt 65:48 0 1.5T 0 disk └─oradata1vg-oradata1lv 253:8 0 11T 0 lvm /oradata1 sdu 65:64 0 1.5T 0 disk └─oraexportvg-oraexportlv 253:3 0 2.7T 0 lvm /oraexport sdv 65:80 0 150G 0 disk └─oraredo1vg-oraredo1lv 253:20 0 370G 0 lvm /oraredo1 sdw 65:96 0 150G 0 disk └─oraredo2vg-oraredo2lv 253:19 0 370G 0 lvm /oraredo2 sdx 65:112 0 150G 0 disk └─oraredo3vg-oraredo3lv 253:4 0 406G 0 lvm /oraredo3 sdy 65:128 0 150G 0 disk └─oraredo4vg-oraredo4lv 253:18 0 407G 0 lvm /oraredo4 sdz 65:144 0 1.2T 0 disk └─oraexportvg-oraexportlv 253:3 0 2.7T 0 lvm /oraexport sr0 11:0 1 1024M 0 rom sdaa 65:160 0 1.5T 0 disk sdab 65:176 0 1.5T 0 disk [admin@DB030035 ~]$ Next, extend the existing group to include the new disk:\n[admin@DB030035 ~]$ sudo vgextend oralogvg /dev/sdaa WARNING: Device for PV ovZDIW-axwB-1YVM-rZVF-V90Y-5cMl-tdyU3m not found or rejected by a filter. Physical volume \u0026#34;/dev/sdaa\u0026#34; successfully created. Volume group \u0026#34;oralogvg\u0026#34; successfully extended [admin@DB030035 ~]$ sudo vgextend oralogvg /dev/sdab WARNING: Device for PV ovZDIW-axwB-1YVM-rZVF-V90Y-5cMl-tdyU3m not found or rejected by a filter. Physical volume \u0026#34;/dev/sdab\u0026#34; successfully created. Volume group \u0026#34;oralogvg\u0026#34; successfully extended Then, resize the physical volume:\n[admin@DB030035 ~]$ sudo pvresize /dev/sdaa WARNING: Device for PV ovZDIW-axwB-1YVM-rZVF-V90Y-5cMl-tdyU3m not found or rejected by a filter. Physical volume \u0026#34;/dev/sdaa\u0026#34; changed 1 physical volume(s) resized or updated / 0 physical volume(s) not resized [admin@DB030035 ~]$ sudo pvresize /dev/sdab WARNING: Device for PV ovZDIW-axwB-1YVM-rZVF-V90Y-5cMl-tdyU3m not found or rejected by a filter. Physical volume \u0026#34;/dev/sdab\u0026#34; changed 1 physical volume(s) resized or updated / 0 physical volume(s) not resized Extend the logical volume to consume 100% of the free space in the disk group:\n[admin@DB030035 ~]$ sudo lvextend -l+100%FREE /dev/oralogvg/oraloglv Size of logical volume oralogvg/oraloglv changed from \u0026lt;22.00 GiB (5631 extents) to 2.95 TiB (773629 extents). Logical volume oralogvg/oraloglv successfully resized. Finally, resize the file system to consume all the new space in the logical volume:\n[admin@DB030035 ~]$ sudo xfs_growfs /dev/oralogvg/oraloglv meta-data=/dev/mapper/oralogvg-oraloglv isize=512 agcount=4, agsize=1441536 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0 spinodes=0 data = bsize=4096 blocks=5766144, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=2815, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 5766144 to 792196096 At this point, the new space should be available on the volume. Run fdisk to confirm the new size of the mount point:\n[admin@DB030035 ~]$ sudo fdisk -l | grep \u0026#34;Disk /dev/mapper/oralogvg-oraloglv\u0026#34; Disk /dev/mapper/oralogvg-oraloglv: 3244.8 GB, 3244835209216 bytes, 6337568768 sectors ","date":"2022-05-11T10:51:54-04:00","permalink":"https://gbeifuss.github.io/p/expand-a-linux-disk-in-esxi/","title":"Expand a Linux Disk in ESXi"},{"content":"I set out to apply patches today on a relatively-newly-installed VCSA 6.7 virtual appliance today.\nI started with taking a backup, as recommended by VMWare. The FTP backup would start, but then sit at Creating backup target directory until it reported the error: Structure com.vmware.appliance.recovery.backup.job.details.info has a union with a field not required for this case = end_time\nMost of the articles I found on the topic were related to permissions being incorrectly set. However, my account had full permissions both within the FileZilla FTP Server software and within Windows. The logs showed the VCSA logging in and creating directories, but the backup would fail with the above error before transferring data.\nIt turns out that the VCSA backup uses passive FTP for the actual transfer, and strict Windows Firewall rules prevented this from being established. Of course, security policies prevented me from temporarily disabling the firewall\u0026hellip; thanks a lot, corporate IT Security! I ended up creating a temporary Firewall rule to permit TCP 1024-1048, then configured the passive ports used by FileZilla FTP Server to use the same range.\nAfter this, the backup completed successfully, and I proceeded with patching the VCSA.\nIn case anyone wonders, I reverted all my changes so that IT Security stays happy :)\n","date":"2022-05-05T09:44:16-04:00","permalink":"https://gbeifuss.github.io/p/vmware-vcsa-6.7-backup-failure/","title":"VMWare VCSA 6.7 Backup Failure"},{"content":"I was looking at the configuration of 3 VSphere servers at work today and determined that they were running an old BIOS. I upgraded their HP DL380 G9 BIOS from P89 v1.50 (07/20/2015) to P89 v2.72 (03/25/2019) using iLO.\nThe first two updates went out without a hitch. The last server? Not so good, as there was a cascading chain of errors.\nAfter rebooting the server (which is necessary for the BIOS update to take effect), the ESXi cluster complained that the server was not reachable. I couldn\u0026rsquo;t ping it either.\nI connected via. the Direct Console User Interface (DCUI) and found that the ESXi IP \u0026amp; DNS settings had reverted to old settings from several years ago. I manually reset them to the proper values and rebooted.\nI then reconnected the server to the cluster, which failed because of a SSL certificate naming error. I had to rejoin it again, which resulted in an IO error:\nRegistration/Unregistration of third-party IO filter storage providers fails on a hosts (yes, the error message contains a subject/verb mismatch)\nDell has KB000058590: Dell EMC VxRail: Alarm for Registration/Unregistration of third-party IO filter storage providers fails. I followed the steps to replace the castore.pem file with one from a working server, and rebooted.\nThat resulted in \u0026quot;error loading /sb.v00 fatal error 33 (inconsistent data)\u0026quot; when trying to boot the ESXi OS.\nAt this point I reverted back to the prior hypervisor via. DCUI:\nIn the console screen of the ESXi host, press Ctrl+Alt+F2 to see the Direct Console User Interface (DCUI) screen.\nPress F12 to view the shutdown options for the ESXi host.\nPress F11 to reboot.\nWhen the Hypervisor progress bar starts loading, press Shift+R (This must be done while the bar is loading, and not after). You will see the warning:\nCurrent hypervisor will permanently be replaced with build: X.X.X-XXXXXX. Are you sure? [y/n] Press Y to roll back the build.\nPress Enter to boot.\nESXi came back online properly, but still complained about the IO filters. At this point, I noticed that while the IP was correct, DNS reverted to old entries. I set it back to the proper values.\nThe ESXi host is finally healthy in the cluster!\u0026hellip; but now I have to upgrade it from 6.0 to 6.5 again.\n","date":"2022-05-01T10:51:36-04:00","permalink":"https://gbeifuss.github.io/p/reverting-an-esxi-upgrade/","title":"Reverting an ESXi Upgrade"},{"content":"One of the legacy systems I inherited at my job was a standalone Red Hat Enterprise Linux 4.0 server with no documented credentials or purpose\u0026hellip; at least, none that I could find, or that anyone in the business could recall. It\u0026rsquo;s difficult to decide if a server is still necessary when there\u0026rsquo;s no information! L33T hacking skills to the rescue!\nReboot the server. We need to edit the startup configuration in order to reset the ROOT password. As the server begins to boot again, the GRUB screen will appear for a few seconds. Press a key in order to interrupt the boot sequence.\nIf you interrupted the boot in time, you\u0026rsquo;ll be presented with a menu. Use the arrow keys to select the top option and press E. Use the arrow keys to highlight the line starting with kernel and press E. Add the word single to the end of the line you\u0026rsquo;re editing, and press ENTER to save the change. This instructs the boot process to load in single user mode:\nkernel /vmlinuz-2.6.9-55.ELsmp ro root=LABEL=/1 nousb single Press b to continue the boot process. Once the boot process is finished, you\u0026rsquo;ll be left at the prompt. Use the passwd command to reset the password for the current user (Root). Issue the reboot command to restart the computer. It should boot normally, at which point you can use the new Root password to log in. I logged in and found that this server seemed to only run a very old copy of sendmail (which no longer had a valid configuration), so I took the server offline for a scream test. In a few days, I\u0026rsquo;ll formally decommission it!\n","date":"2022-04-02T09:35:22-04:00","permalink":"https://gbeifuss.github.io/p/reset-root-on-rhel-4/","title":"Reset Root on RHEL 4"},{"content":"One of the legacy ecosystems that I inherited is an IBM Tivoli Storage Manager install. Ours consists of:\n5 servers 2 tape libraries (totalling 8x LTO-6 drives) IBM V5000 storage array Our site has a 25 year retention policy, driven by customer requirements \u0026amp; contracts. For the past 2-5 years, this install has not been cared for in any way. One of the tape libraries has been online for 65,930 hours\u0026hellip; that\u0026rsquo;s 7.5 years without a reboot! According to the people I spoke with, we apparently migrated away from it a few years ago \u0026hellip; but nobody could give me a 100% guarantee that all the backups were properly migrated to the new system, that retention policies were followed, or whether the TSM system could be restored from if the need ever arose.\nI decided that the best course of action was to power down the systems and let everything age in place. At least that would give us a chance at restoring data if it turned out something was improperly transitioned.\nWe have about 10 Windows Servers still with the IBM TSM Client installed and running, even though it wasn\u0026rsquo;t being used. We have installations ranging from v.06.02.0301 (April 2014) to v.07.01.0800 (March 2018).\nI opened up Add/Remove Programs and removed the software\u0026hellip; which ended up actually doing nothing other than removing the entry. The TSM services were still installed (and running!), even after a reboot:\nPS C:\\ \u0026gt; get-service -Name TSM* | ft -a Status Name DisplayName ------ ---- ----------- Running TSM Client Acceptor TSM Client Acceptor Running TSM Client Scheduler TSM Client Scheduler Stopped TSM Journal Service TSM Journal Service Stopped TSM Remote Client Agent TSM Remote Client Agent More comprehensive methods were clearly needed!\nRemove Services\nLaunch Command Prompt (as Administrator)\nsc delete \u0026#34;TSM Client Scheduler\u0026#34; sc delete \u0026#34;TSM Client Acceptor\u0026#34; sc delete \u0026#34;TSM Client Agent\u0026#34; Remove Files and Directories\nDelete the main software folder (ie. C:\\Program Files\\Tivoli). If Windows states that some files are in use, reboot and retry.\nDelete the installation folder (ie. C:\\tsm_images).\nDelete Registry Entries\nLaunch Regedit as administrator:\nDelete HKEY_LOCAL_MACHINE\\SOFTWARE\\IBM\\ASDM\nDelete HKEY_LOCAL_MACHINE\\SOFTWARE\\Tivoli\nAfter a reboot, there was no trace of the TSM Client any more!\n","date":"2022-04-02T09:34:57-04:00","permalink":"https://gbeifuss.github.io/p/uninstalling-tivoli-storage-manager-client/","title":"Uninstalling Tivoli Storage Manager Client"},{"content":"I recently had a request to grant a new user access to a legacy Confluence install at work. This particular server was in active use from 2013-2018ish and runs Confluence 3.1 standalone (Apache \u0026amp; mySQL).\nThe big problem was that nobody knew any credentials.\nI started out by browsing to the configuration in C:\\confluence\\confluence-3.1-std\\conf\\server.xml. Luckily, I found some credentials to connect to the mySQL instance:\n\u0026lt;property name=\u0026#34;hibernate.connection.driver_class\u0026#34;\u0026gt;com.mysql.jdbc.Driver\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.password\u0026#34;\u0026gt;conf*w0rk\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.url\u0026#34;\u0026gt;jdbc:mysql://localhost/confluence?autoReconnect=true\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.connection.username\u0026#34;\u0026gt;confluenceuser\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.database.lower_non_ascii_supported\u0026#34;\u0026gt;true\u0026lt;/property\u0026gt; \u0026lt;property name=\u0026#34;hibernate.dialect\u0026#34;\u0026gt;com.atlassian.hibernate.dialect.MySQLDialect\u0026lt;/property\u0026gt; I was then able to follow Atlassian\u0026rsquo;s documentation on Restoring Passwords To Recover Admin User Rights.\nFirst, I ran a query against the database using MySQL Query Browser to identify existing administrators. This returns the users who belong to the Confluence-Administrators group:\nselect name from users u, local_members l, groups g where g.groupname = \u0026#39;confluence-administrators\u0026#39; and g.id=l.groupid and u.id=l.userid; administrator confadmin wikiadmin Let\u0026rsquo;s reset the password for \u0026lsquo;administrator\u0026rsquo;. I began by stopping Confluence via. the shutdown.bat script.\nThen, I used mySQL Query Browser again to reset the administrator account\u0026rsquo;s password to admin:\nupdate users set password = 'x61Ey612Kl2gpFL56FT9weDnpSo4AV8j8+qx2AuTHdRyY036xxzTTrw10Wq3+4qQyB+XURPWx1ONxp3Y3pB37A==' where name='administrator';\nAfter the password was reset, I started Confluence again via. the \u0026lsquo;startup.bat\u0026rsquo; script. I then logged in as administrator:admin and used Confluence\u0026rsquo;s interface to add a new user account.\n","date":"2022-03-22T13:26:12-04:00","permalink":"https://gbeifuss.github.io/p/reset-confluence-password-for-an-administrator-id/","title":"Reset Confluence Password for an Administrator ID"},{"content":"Our organization ran an ADFS instance, but it was configured with a Service Account, not with a Group-Managed Service Account (gMSA), which is Microsoft\u0026rsquo;s recommendation for security reasons. I wanted to change it, without losing any of our configuration. The ADFSToolbox module didn\u0026rsquo;t seem to support a change to a gMSA, and I could find no supported way to backup our configuration (certificates, transformations, relying trusts) and restore them to a properly configured instance.\nTo start, my environment is 2 ADFS servers (Server 2019) running WID and ADFS Farm Behaviour 4.0.\nI needed these tools:\nADFS Toolbox\nVisual C+++ redistributable (2015-2019)\nODBC Driver 17\nSQLCMD\nI also referenced the ServiceAccount Module documentation on the old ADFS Toolbox site.\nCreate the gMSA you\u0026rsquo;re going to use, and configure it, including the altering the local security policy on both 2 ADFS servers.\nThe gMSA needs rights to both Generate Security Audits and Log On As A Service.\nInstall Visual C++ on both ADFS servers\nInstall ODBC Driver 17 on both servers\nInstall SQLCMD on both servers\nInstall ActiveDirectory module for Powershell on both servers:\nAdd Roles\\Features \u0026gt; RSAT \u0026gt; Remote Administrator Tools \u0026gt; AD DS \u0026amp; AD LDS \u0026gt; AD for Powershell Module\nInstall ADFS Toolbox on both servers via. Powershell.\nMake sure Powershell is using TLS 1.2 before trying to install the ADFS Toolbox!\nImport the ADFS Toolbox module on both servers:\nimport-module adfstoolbox\nInstall AdfsServiceAccountModule on both servers:\nImport-Module \u0026quot;C:\\Program Files\\WindowsPowerShell\\Modules\\ADFSToolbox\\2.0.17.0\\serviceAccountModule\\AdfsServiceAccountModule.psm1\u0026quot;\nOn the Primary ADFS server, add the GMSA account:\nadd-AdfsServiceAccountRule -ServiceAccount adfs-gmsa$ -SecondaryServers adfs02.company.com\nOn the Secondary server, run: Update-AdfsServiceAccount\nWhen prompted, set the Operating Mode to #1 - Federation Server\nOn the Primary server, run: Update-AdfsServiceAccount\nWhen prompted, set the Operating Mode to #2 - Final Federation Server\nThe script errored out when trying to update the SPN.\nIf necessary, delete the old SPN: setspn -D HOST/STS.COMPANY.COM DOMAIN\\adfssvc\nGenerate the new SPN: setspn -S HOST/STS.COMPANY.COM DOMAIN\\ADFS-GMSA$\nStart ADFSSRV service on Primary\nStart ADFSSRV service on Secondary\nValidate that the service is running properly under the new GMSA and that replication is occurring (Get-AdfsSyncProperties).\nRemove the old service account information via. the Primary Server:\nremove-AdfsServiceAccountRule -ServiceAccount DOMAIN\\adfssvc -SecondaryServers adfs02.COMPANY.com\nCleanup tools and powershell modules.\nReferences\rI derived this article from my Reddit posts on the matter:\nService Account to gMSA?\nService Account to GMSA - success!\n","date":"2022-03-22T11:14:45-04:00","permalink":"https://gbeifuss.github.io/p/adfs-change-service-account-to-gmsa/","title":"ADFS - Change Service Account to gMSA"},{"content":"I built a new Windows 2019 Server VM the other day on our ESXi platform. I\u0026rsquo;ll migrate some services currently running on a Windows 2008R2 Server over, then decommission the old VM. One of the challenges I faced was getting our corporate image, which is stored as an image file in Phoenix, Arizona. Because of the way our storage is setup, I couldn\u0026rsquo;t directly transfer it to our storage system.\nI downloaded the VMDK file to my laptop first. ESXi had to expand the file from 24GB to the full 80GB during the download, which took an hour or so. Next, I uploaded the 80GB VMDK file to our local ESXi cluster. A couple of hours later, it was finished!\nI attached it to a new VM, and then did a storage vMotion. Fortunately, I have a couple of datastores to choose from!\nRight-click the virtual machine and select Migrate. In Step 1 of the Wizard (Select a Migration Type), select Change Storage Only and click Next. We don\u0026rsquo;t need to move the VM to a new host, only the files. In Step 2 of the Wizard (Select Storage) select the option Configure Per Disk in the upper right. Select the disk to modify. Under Storage category, select the destination datastore. Under the Disk Format option select Thin Provision, and click Next. In Step 3 of the Wizard (Ready to Complete), review the information and click Finish. References\rChanging the thick or thin provisioning of a virtual disk (2014832)\n","date":"2022-02-26T09:34:35-04:00","permalink":"https://gbeifuss.github.io/p/creating-an-esxi-thin-disk/","title":"Creating an ESXi Thin Disk"},{"content":"As previously mentioned in ESXi 6.0 to 6.5 Upgrade, I recently took over a manufacturing site where patches had been sorely neglected for a couple of years. There were a number of HP DL380 G9 servers still running iLO 1.4. Once I gained access to iLO, I started by upgrading those HP servers to iLO 2.79, the most recent version.\nAfter the mandatory reboot to apply the new iLO firmware, 2/3 of the servers now displayed an iLO login screen warning of Embedded Flash\\NAND error: I found HP Knowledge Base article Document ID: c04996097 outlining my symptoms.\nI started by resetting iLO and cycling the server, which had no effect.\nThe next step was to format the NAND:\nLog into iLO Expand Information \u0026gt; Diagnostics Click iLO Health. The status should read something like:\nEmbedded Flash/SD-CARD | Controller firmware revision 2.10.00 Embedded media initialization failed due to media write-verify test failure Read the warning: WARNING: Formatting the Embedded Flash erases all data on the iLO partition and cannot be undone. External providers (like BIOS, Intelligent Provisioning, OneView and FLM) will need to be re-configured. Click Format Embedded Flash and reset iLO Accept the confirmation. iLO will reset. After the mandatory reboot, the same error was displayed on the iLO login page. The next suggestion in the article (if the NAND reset didn\u0026rsquo;t work) was to perform a cold boot of the server. I shut down the server, removed the power cord for 15 seconds, plugged in the cord again and started the server.\nThis time iLO didn\u0026rsquo;t complain about problems with the flash memory. Hurrah!\nReferences\rThe articles I referenced when figuring this out were: HPE Integrated Lights-Out 4 (iLO 4) - HPE Active Health System (AHS) Logs and HPE OneView Profiles May Be Unavailable Causing iLO Self-Test Error 8192, Embedded Media Manager and Other Errors\nHPE Integrated Lights-Out 4 (iLO 4) - How to Format the NAND Used to Store AHS logs, OneView Profiles, and Intelligent Provisioning\n","date":"2021-12-27T00:09:54-05:00","permalink":"https://gbeifuss.github.io/p/resolving-hp-ilo-nand-error/","title":"Resolving HP iLO NAND Error"},{"content":"As previously mentioned in ESXi 6.0 to 6.5 Upgrade, I recently took over a manufacturing site where some administrative passwords that weren\u0026rsquo;t known. In particular, there were two HP Blade C7000 chassis with no credentials for the Blade Onboard Administrator.\nThe easiest way to reset the Administrator credential is by plugging a USB key (with a reset script) in the chassis, then use the Insight LCD to run the script.\nCreate a new text file containing only the following string: SET USER PASSWORD \u0026quot;Administrator\u0026quot; \u0026quot;password\u0026quot; and save it with a .cfg extension on a USB key. On the Insight LCD, select USB Menu \u0026gt; Restore Configuration \u0026gt; usb://d1/resetadmin.cfg\nThis will reset the Administrator password to password. Launch your browser of choice, connect to the BOA and log in with Administrator: password\nresetadmin.cfg SET USER PASSWORD \u0026#34;Administrator\u0026#34; \u0026#34;password\u0026#34; ","date":"2021-12-27T00:09:41-05:00","permalink":"https://gbeifuss.github.io/p/hp-ilo-c7000-credential-reset/","title":"HP iLO C7000 Credential Reset"},{"content":"The log3j vulnerability in an Apache Java component is one of the more serious and widespread software/security flaws to be identified in some time\u0026hellip; although, it does seem like that\u0026rsquo;s said every few months. Basically, the flaw is triggered when a specific string is sent to the logging module. This string can be used to trigger a lookup on a remote server, returning a Java script that is then injected into the process. This then lets the attacker execute commands at the same privilege level as the application using the logging library. In short, it allows for remote code execution, and complete control by an attacker.\nI had a number of ESXi 6.5 servers that needed to be patched. Fortunately, VMWare has a Python script that can be used for automated remediation.\nEnable SSH login on the VCenter appliance:\nBrowse to https://vcenterappliance:5480/, then choose Access \u0026gt; Edit \u0026gt; Enable SSH login. Download the current Python script from VMWare:\nWorkaround instructions to address CVE-2021-44228 and CVE-2021-45046 in vCenter Server and vCenter Cloud Gateway (87081) Launch Kitty, or your SSH application of choice. Connect to your VCenter appliance and load the shell: login as: root Pre-authentication banner message from server: | VMware vCenter Server Appliance 6.5.0.35000 | | Type: vCenter Server with an embedded Platform Services Controller End of banner message from server Keyboard-interactive authentication prompts from server: | Password: End of keyboard-interactive prompts from server Connected to service * List APIS: \u0026#34;help api list\u0026#34; * List Plugins: \u0026#34;help pi list\u0026#34; * Launch BASH: \u0026#34;shell\u0026#34; Command\u0026gt; shell Shell access is granted to root root@servername [ ~ ]# Create a new file in VI. Press I for Insert mode, then paste in the script contents that were downloaded from VMWare by right-clicking in the terminal. After the paste is complete, press ESC, then :wq to save the file and quit VI. I also validate the size of the new file: root@servername [ ~ ]# vi /tmp/vmsa-2021-0028-kb87081.py root@servername [ ~ ]# ls /tmp/vmsa* -l -rw-r--r-- 1 root root 44290 Dec 21 20:43 /tmp/vmsa-2021-0028-kb87081.py root@servername [ ~ ]# Run the script with the command python /tmp/vmsa-2021-0028-kb87081.py: root@servername [ ~ ]# python /tmp/vmsa-2021-0028-kb87081.py 2021-12-21T20:45:37 INFO main: Script version: 1.6.0 2021-12-21T20:45:37 INFO main: VCenter type: Version: 6.5.0.35000; Build: 179949 27; Deployment type: embedded; Gateway: False; VCHA: False; Windows: False; A service stop and start is required to complete this operation. Continue? [y] y 2021-12-21T20:45:44 INFO Stop: stopping services The script will run for some time. After it\u0026rsquo;s complete, review the output to make sure it completed successfully. For my own sanity, I run the script a second time with the -r flag in order to validate the results: root@servername [ ~ ] python /tmp/vmsa-2021-0028-ib87081.py -r 2021-12-21T20:55:17 INFO main: Script version: 1.6.0 2021-12-21T20:55:17 INFO main: vCenter type: Version: 6.5.0.35000: Build: 17994927: Deployment type: embedded: Ga teway: False: VCHA: False: Windows: False: 2021-12-21T20:55:17 INFO main: Running in dryrun mode. 2021-12-21T20:56:01 INFO print_summary: Summary No vulnerable files found! Total found: 0 Log file: /var/log/vmsa-2021-0028_2021_12_21_20_55_17.log 2021-12-21T20:56:01 INFO main: Done. ROOT@servername [ ~ ]: References\rThe articles I referenced when figuring this out were:\nWorkaround instructions to address CVE-2021-44228 and CVE-2021-45046 in vCenter Server and vCenter Cloud Gateway (87081)\nHow to Mitigate Log4j VMware Vulnerability – Workaround\n","date":"2021-12-22T16:17:50-05:00","permalink":"https://gbeifuss.github.io/p/remediating-log4j-in-vmware-vcenter-6.5/","title":"Remediating Log4j in VMWare VCenter 6.5"},{"content":"Almost two months ago, I assumed responsibility for a new site. It turned out that no one had really performed any administration at the site in about two years and there was a lot of basic work to do. There were failed UPSes, cables left hanging from racks when servers had been decommissioned, no administrative IDs for some hardware (iLO and iDRAC) and no patching. They had several ESXi clusters still running 6.0 (which has no HTML 5 support) so access and administration was a nightmare, to say nothing of the security risks. Updating to 6.5 was high on my priority list!\nDownload the offline bundle ZIP from VMWare for my HP DL380 G9 hardware. Upload the offline bundle zip to a datastore accessible by the host via. VCenter GUI Enable SSH mode for the host hardware. This can be done in the VCenter GUI via. Settings -\u0026gt; Configure -\u0026gt; System/Services/SSH. SSH to the host with your terminal of choice and sign in. I\u0026rsquo;m partial to Kitty If you haven\u0026rsquo;t already done so, move all guests to other hosts, then place the host server in Migration Mode: [root@vsphere-sap01:~] esxcli system maintenanceMode set --enable true Update the software: [root@vsphere-sap01:~] esxcli software vib update -d /vmfs/volumes/Tintri02-SAP/ISO/VMware-ESXi-6.5.0-Update3-18678235-HPE-Gen9plus-650.U3.10.8.0.36-Oct2021-depot.zip [DependencyError] VIB VMW_bootbank_ehci-ehci-hcd_1.0-4vmw.650.0.14.5146846 requires com.vmware.usb-10.0, but the requirement cannot be satisfied within the ImageProfile. VIB VMW_bootbank_ata-pata-pdc2027x_1.0-3vmw.650.0.0.4564106 requires com.vmware.libata-9.2.3.0, but the requirement cannot be satisfied within the ImageProfile. VIB Intel_bootbank_net-igb_5.2.10-1OEM.550.0.0.1331820 requires com.vmware.driverAPI-9.2.2.0, but the requirement cannot be satisfied within the ImageProfile. VIB Broadcom_bootbank_scsi-bnx2fc_1.710.70.v55.3-1OEM.550.0.0.1331820 requires com.vmware.libfc-9.2.2.0, but the requirement cannot be satisfied within the ImageProfile. VIB VMW_bootbank_ata-pata-serverworks_0.4.3-3vmw.650.0.0.4564106 requires com.vmware.driverAPI-9.2.3.0, but the requirement cannot be satisfied within the ImageProfile. VIB VMW_bootbank_ipmi-ipmi-msghandler_39.1-5vmw.650.2.50.8294253 requires com.vmware.driverAPI-9.2.3.0, but the requirement cannot be satisfied within the ImageProfile. Ugh. Of course this can\u0026rsquo;t be a straightforward upgrade! I tried to work around the error. A quick search revealed that apparently, if I specify the profile, I can override these prerequistes. First, I determined the profile in the package, then tried to update the profile by using the -p flag:\n[root@vsphere-sap01:~] esxcli software sources profile list -d /vmfs/volumes/Tintri02-SAP/ISO/VMware-ESXi-6.5.0-Update3-18678235-HPE-Gen9plus-650.U3.10.8.0.36-Oct2021-depot.zip Name Vendor Acceptance Level --------------------------------------------------------- -------------------------- ---------------- HPE-ESXi-6.5.0-Update3-18678235-Gen9plus-650.U3.10.8.0.36 Hewlett Packard Enterprise PartnerSupported [root@vsphere-sap01:~] esxcli software profile update -d /vmfs/volumes/Tintri02-SAP/ISO/VMware-ESXi-6.5.0-Update3-18678235-HPE-Gen9plus-650.U3.10.8.0.36-Oct2021-depot.zip -p HPE-ESXi-6.5.0-Update3-18678235-Gen9plus-650.U3.10.8.0.36 [InstallationError] (\u0026#39;VMware_bootbank_esx-ui_1.34.2-16361878\u0026#39;, \u0026#39;Could not find a trusted signer.\u0026#39;) vibs = VMware_bootbank_esx-ui_1.34.2-16361878 Please refer to the log file for more details. That clearly didn\u0026rsquo;t work. I decided to tell ESXi to ignore the signing errors with the \u0026ndash;no-sig-check flag and tried again to update the profile:\n[root@vsphere-sap01:~] esxcli software profile update -d /vmfs/volumes/Tintri02-SAP/ISO/VMware-ESXi-6.5.0-Update3-18678235-HPE-Gen9plus-650.U3.10.8.0.36-Oct2021-depot.zip -p HPE-ESXi-6.5.0-Update3-18678235-Gen9plus-650.U3.10.8.0.36 --no-sig-check Update Result Message: The update completed successfully, but the system needs to be rebooted for the changes to be effective. Reboot Required: true VIBs Installed: Avago_bootbank_lsi-mr3_7.706.09.00-1OEM.650.0.0.4598673, Avago_bootbank_scsi-mpt2sas_15.10.07.00-1OEM.550.0.0.1331820, BCM_bootbank_bnxtnet_218.0.38.0-1OEM.650.0.0.4598673, BCM_bootbank_bnxtroce_218.0.16.0-1OEM.650.0.0.4598673, ELX_bootbank_elx-esx-libelxima.so_12.0.1108.0-03, EMU_bootbank_brcmfcoe_12.0.1278.0-1OEM.650.0.0.4598673, EMU_bootbank_elxiscsi_12.0.1108.0-1OEM.650.0.0.4598673, EMU_bootbank_elxnet_12.0.1216.4-1OEM.650.0.0.4598673, EMU_bootbank_lpfc_12.8.317.0-1OEM.650.0.0.4598673, HPE_bootbank_amsd_650.11.8.0.15-1.4240417, HPE_bootbank_amshelpr_650.11.8.0.8-1.4240417, HPE_bootbank_bootcfg_6.0.0.02-06.00.16.2494585, HPE_bootbank_conrep_650.10.8.0.12-6.5.0.2494585, HPE_bootbank_cru_650.6.5.10.4-1OEM.650.0.0.4240417, HPE_bootbank_fc-enablement_650.3.8.0.6-4240417, HPE_bootbank_hponcfg_6.0.0.5.5-0.37.2494585, HPE_bootbank_ilo_650.10.7.5.2-1OEM.650.0.0.4240417, HPE_bootbank_ilorest_650.3.3.0.2-4240417, HPE_bootbank_oem-build_650.U3.10.8.0.2-4240417, HPE_bootbank_scsi-hpdsa_6.0.0.76-1OEM.600.0.0.2494585, HPE_bootbank_scsi-hpvsa_5.5.0.102-1OEM.550.0.0.1331820, HPE_bootbank_smx-provider_650.03.16.00.4-4240417, HPE_bootbank_sut_6.5.0.2.9.0.0-23, HPE_bootbank_sutesxcli_6.5.0.2.9.0.0-18, HPE_bootbank_testevent_6.5.0.02-01.00.9.4240417, INT_bootbank_i40en_1.10.6-1OEM.650.0.0.4598673, INT_bootbank_icen_1.5.5.0-1OEM.650.0.0.4598673, INT_bootbank_igbn_1.5.2.0-1OEM.650.0.0.4598673, INT_bootbank_ixgben_1.9.11.0-1OEM.650.0.0.4598673, MEL_bootbank_nmlx4-core_3.16.70.2-1OEM.650.0.0.4598673, MEL_bootbank_nmlx4-en_3.16.70.2-1OEM.650.0.0.4598673, MEL_bootbank_nmlx4-rdma_3.16.70.2-1OEM.650.0.0.4598673, MEL_bootbank_nmlx5-core_4.16.71.1-1OEM.650.0.0.4598673, MEL_bootbank_nmlx5-rdma_4.16.71.1-1OEM.650.0.0.4598673, MEL_bootbank_nmst_4.12.0.105-1OEM.650.0.0.4598673, MIS_bootbank_hpessacli_5.20.8.0-6.5.0.4240417.oem, MSCC_bootbank_smartpqi_65.4150.0.119-1OEM.650.0.0.4598673, Microsemi_bootbank_nhpsa_65.0072.0.149-1OEM.650.0.0.4598673, PEN_bootbank_ionic-en_18.0.0-1OEM.650.0.0.4598673, QLC_bootbank_qcnic_1.0.59.0-1OEM.650.0.0.4598673, QLC_bootbank_qedentv_3.11.16.0-1OEM.650.0.0.4598673, QLC_bootbank_qedf_1.3.42.50-1OEM.600.0.0.2768847, QLC_bootbank_qedrntv_3.11.16.0-1OEM.650.0.0.4598673, QLC_bootbank_qfle3_1.1.17.0-1OEM.650.0.0.4598673, QLC_bootbank_qfle3f_1.1.25.0-1OEM.650.0.0.4598673, QLC_bootbank_qfle3i_1.1.5.0-1OEM.650.0.0.4598673, QLC_bootbank_scsi-qedil_1.15.15.0-1OEM.600.0.0.2494585, QLogic_bootbank_qlnativefc_2.1.101.0-1OEM.600.0.0.2768847, VMW_bootbank_ata-libata-92_3.00.9.2-16vmw.650.0.0.4564106, VMW_bootbank_ata-pata-amd_0.3.10-3vmw.650.0.0.4564106, VMW_bootbank_ata-pata-atiixp_0.4.6-4vmw.650.0.0.4564106, VMW_bootbank_ata-pata-cmd64x_0.2.5-3vmw.650.0.0.4564106, VMW_bootbank_ata-pata-hpt3x2n_0.3.4-3vmw.650.0.0.4564106, VMW_bootbank_ata-pata-pdc2027x_1.0-3vmw.650.0.0.4564106, VMW_bootbank_ata-pata-serverworks_0.4.3-3vmw.650.0.0.4564106, VMW_bootbank_ata-pata-sil680_0.4.8-3vmw.650.0.0.4564106, VMW_bootbank_ata-pata-via_0.3.3-2vmw.650.0.0.4564106, VMW_bootbank_block-cciss_3.6.14-10vmw.650.0.0.4564106, VMW_bootbank_char-random_1.0-3vmw.650.0.0.4564106, VMW_bootbank_ehci-ehci-hcd_1.0-4vmw.650.0.14.5146846, VMW_bootbank_hid-hid_1.0-3vmw.650.0.0.4564106, VMW_bootbank_ipmi-ipmi-devintf_39.1-5vmw.650.2.50.8294253, VMW_bootbank_ipmi-ipmi-msghandler_39.1-5vmw.650.2.50.8294253, VMW_bootbank_ipmi-ipmi-si-drv_39.1-4vmw.650.0.0.4564106, VMW_bootbank_lsi-msgpt35_09.00.00.00-5vmw.650.3.96.13932383, VMW_bootbank_lsi-msgpt3_17.00.02.00-1vmw.650.3.96.13932383, VMW_bootbank_misc-drivers_6.5.0-3.96.13932383, VMW_bootbank_mtip32xx-native_3.9.5-1vmw.650.0.0.4564106, VMW_bootbank_ne1000_0.8.3-8vmw.650.2.75.10884925, VMW_bootbank_nenic_1.0.29.0-1vmw.650.3.96.13932383, VMW_bootbank_net-cdc-ether_1.0-3vmw.650.0.0.4564106, VMW_bootbank_net-e1000_8.0.3.1-5vmw.650.3.153.17459147, VMW_bootbank_net-e1000e_3.2.2.1-2vmw.650.0.0.4564106, VMW_bootbank_net-enic_2.1.2.38-2vmw.650.0.0.4564106, VMW_bootbank_net-fcoe_1.0.29.9.3-7vmw.650.0.0.4564106, VMW_bootbank_net-forcedeth_0.61-2vmw.650.0.0.4564106, VMW_bootbank_net-libfcoe-92_1.0.24.9.4-8vmw.650.0.0.4564106, VMW_bootbank_net-usbnet_1.0-3vmw.650.0.0.4564106, VMW_bootbank_net-vmxnet3_1.1.3.0-3vmw.650.3.138.16576891, VMW_bootbank_ntg3_4.1.3.2-1vmw.650.2.75.10884925, VMW_bootbank_nvme_1.2.2.28-5vmw.650.3.170.18678235, VMW_bootbank_nvmxnet3_2.0.0.23-1vmw.650.1.36.7388607, VMW_bootbank_ohci-usb-ohci_1.0-3vmw.650.0.0.4564106, VMW_bootbank_pvscsi_0.1-1vmw.650.1.26.5969303, VMW_bootbank_qflge_1.1.0.3-1vmw.650.0.0.4564106, VMW_bootbank_sata-ahci_3.0-26vmw.650.1.26.5969303, VMW_bootbank_sata-ata-piix_2.12-10vmw.650.0.0.4564106, VMW_bootbank_sata-sata-nv_3.5-4vmw.650.0.0.4564106, VMW_bootbank_sata-sata-promise_2.12-3vmw.650.0.0.4564106, VMW_bootbank_sata-sata-sil24_1.1-1vmw.650.0.0.4564106, VMW_bootbank_sata-sata-sil_2.3-4vmw.650.0.0.4564106, VMW_bootbank_sata-sata-svw_2.3-3vmw.650.0.0.4564106, VMW_bootbank_scsi-aacraid_1.1.5.1-9vmw.650.0.0.4564106, VMW_bootbank_scsi-adp94xx_1.0.8.12-6vmw.650.0.0.4564106, VMW_bootbank_scsi-aic79xx_3.1-5vmw.650.0.0.4564106, VMW_bootbank_scsi-fnic_1.5.0.45-3vmw.650.0.0.4564106, VMW_bootbank_scsi-ips_7.12.05-4vmw.650.0.0.4564106, VMW_bootbank_scsi-iscsi-linux-92_1.0.0.2-3vmw.650.0.0.4564106, VMW_bootbank_scsi-libfc-92_1.0.40.9.3-5vmw.650.0.0.4564106, VMW_bootbank_scsi-megaraid-mbox_2.20.5.1-6vmw.650.0.0.4564106, VMW_bootbank_scsi-megaraid-sas_6.603.55.00-2vmw.650.0.0.4564106, VMW_bootbank_scsi-megaraid2_2.00.4-9vmw.650.0.0.4564106, VMW_bootbank_scsi-mptsas_4.23.01.00-10vmw.650.0.0.4564106, VMW_bootbank_scsi-mptspi_4.23.01.00-10vmw.650.0.0.4564106, VMW_bootbank_shim-iscsi-linux-9-2-1-0_6.5.0-0.0.4564106, VMW_bootbank_shim-iscsi-linux-9-2-2-0_6.5.0-0.0.4564106, VMW_bootbank_shim-libata-9-2-1-0_6.5.0-0.0.4564106, VMW_bootbank_shim-libata-9-2-2-0_6.5.0-0.0.4564106, VMW_bootbank_shim-libfc-9-2-1-0_6.5.0-0.0.4564106, VMW_bootbank_shim-libfc-9-2-2-0_6.5.0-0.0.4564106, VMW_bootbank_shim-libfcoe-9-2-1-0_6.5.0-0.0.4564106, VMW_bootbank_shim-libfcoe-9-2-2-0_6.5.0-0.0.4564106, VMW_bootbank_shim-vmklinux-9-2-1-0_6.5.0-0.0.4564106, VMW_bootbank_shim-vmklinux-9-2-2-0_6.5.0-0.0.4564106, VMW_bootbank_shim-vmklinux-9-2-3-0_6.5.0-0.0.4564106, VMW_bootbank_uhci-usb-uhci_1.0-3vmw.650.0.0.4564106, VMW_bootbank_usb-storage-usb-storage_1.0-3vmw.650.0.0.4564106, VMW_bootbank_usbcore-usb_1.0-3vmw.650.2.50.8294253, VMW_bootbank_vmkata_0.1-1vmw.650.1.36.7388607, VMW_bootbank_vmkplexer-vmkplexer_6.5.0-0.0.4564106, VMW_bootbank_vmkusb_0.1-1vmw.650.3.170.18678235, VMW_bootbank_vmw-ahci_1.1.6-1vmw.650.3.96.13932383, VMW_bootbank_xhci-xhci_1.0-3vmw.650.0.0.4564106, VMware_bootbank_cpu-microcode_6.5.0-3.134.16576879, VMware_bootbank_emulex-esx-elxnetcli_11.1.28.0-0.0.4564106, VMware_bootbank_esx-base_6.5.0-3.170.18678235, VMware_bootbank_esx-dvfilter-generic-fastpath_6.5.0-1.36.7388607, VMware_bootbank_esx-tboot_6.5.0-3.170.18678235, VMware_bootbank_esx-ui_1.34.2-16361878, VMware_bootbank_esx-xserver_6.5.0-3.120.15256549, VMware_bootbank_lsu-hp-hpsa-plugin_2.0.0-16vmw.650.3.96.13932383, VMware_bootbank_lsu-lsi-drivers-plugin_1.0.0-1vmw.650.2.79.11925212, VMware_bootbank_lsu-lsi-lsi-mr3-plugin_1.0.0-11vmw.650.2.75.10884925, VMware_bootbank_lsu-lsi-lsi-msgpt3-plugin_1.0.0-8vmw.650.2.79.11925212, VMware_bootbank_lsu-lsi-megaraid-sas-plugin_1.0.0-8vmw.650.1.26.5969303, VMware_bootbank_lsu-lsi-mpt2sas-plugin_2.0.0-6vmw.650.1.26.5969303, VMware_bootbank_native-misc-drivers_6.5.0-3.120.15256549, VMware_bootbank_rste_2.0.2.0088-4vmw.650.0.0.4564106, VMware_bootbank_vmware-esx-esxcli-nvme-plugin_1.2.0.36-3.96.13932383, VMware_bootbank_vsan_6.5.0-3.170.18569621, VMware_bootbank_vsanhealth_6.5.0-3.170.18569625, VMware_locker_tools-light_6.5.0-3.166.18677441 VIBs Removed: EMU_bootbank_lpfc_10.4.236.0-1OEM.600.0.0.2159203, Emulex_bootbank_elxnet_10.5.65.4-1OEM.550.0.0.1331820, Hewlett-Packard_bootbank_char-hpcru_6.0.6.14-1OEM.600.0.0.2159203, Hewlett-Packard_bootbank_char-hpilo_600.9.0.2.8-1OEM.600.0.0.2159203, Hewlett-Packard_bootbank_hp-ams_600.10.2.0-22.2159203, Hewlett-Packard_bootbank_hp-build_600.9.3.30.2-2159203, Hewlett-Packard_bootbank_hp-conrep_6.0.0.1-0.0.13.2159203, Hewlett-Packard_bootbank_hp-esxi-fc-enablement_550.2.3.16-1198610, Hewlett-Packard_bootbank_hp-smx-provider_600.03.08.00.13-2159203, Hewlett-Packard_bootbank_hpbootcfg_6.0.0.02-01.00.11.2159203, Hewlett-Packard_bootbank_hponcfg_6.0.0.04-00.13.17.2159203, Hewlett-Packard_bootbank_hpssacli_2.20.11.0-6.0.0.2159203, Hewlett-Packard_bootbank_hptestevent_6.0.0.01-00.00.8.2159203, Hewlett-Packard_bootbank_scsi-hpdsa_5.5.0.36-1OEM.550.0.0.1331820, Hewlett-Packard_bootbank_scsi-hpvsa_5.5.0.100-1OEM.550.0.0.1331820, LSI_bootbank_scsi-mpt2sas_15.10.06.00.1vmw-1OEM.550.0.0.1198610, QLogic_bootbank_qlnativefc_2.1.20.0-1OEM.600.0.0.2159203, VMWARE_bootbank_mtip32xx-native_3.8.5-1vmw.600.0.0.2494585, VMware_bootbank_ata-pata-serverworks_0.4.3-3vmw.600.0.0.2494585, VMware_bootbank_ata-pata-sil680_0.4.8-3vmw.600.0.0.2494585, VMware_bootbank_ata-pata-via_0.3.3-2vmw.600.0.0.2494585, VMware_bootbank_block-cciss_3.6.14-10vmw.600.0.0.2494585, VMware_bootbank_cpu-microcode_6.0.0-0.0.2494585, VMware_bootbank_ehci-ehci-hcd_1.0-3vmw.600.2.52.4600944, VMware_bootbank_emulex-esx-elxnetcli_10.2.309.6v-0.0.2494585, VMware_bootbank_esx-base_6.0.0-2.52.4600944, VMware_bootbank_esx-dvfilter-generic-fastpath_6.0.0-0.0.2494585, VMware_bootbank_esx-tboot_6.0.0-2.34.3620759, VMware_bootbank_esx-ui_1.9.0-4392584, VMware_bootbank_esx-xserver_6.0.0-0.0.2494585, VMware_bootbank_ipmi-ipmi-devintf_39.1-4vmw.600.0.0.2494585, VMware_bootbank_ipmi-ipmi-msghandler_39.1-4vmw.600.0.0.2494585, VMware_bootbank_ipmi-ipmi-si-drv_39.1-4vmw.600.0.0.2494585, VMware_bootbank_lsi-mr3_6.605.08.00-7vmw.600.1.17.3029758, VMware_bootbank_lsi-msgpt3_06.255.12.00-8vmw.600.1.17.3029758, VMware_bootbank_lsu-hp-hpsa-plugin_2.0.0-3vmw.600.2.52.4600944, VMware_bootbank_lsu-lsi-lsi-mr3-plugin_1.0.0-2vmw.600.0.11.2809209, VMware_bootbank_lsu-lsi-lsi-msgpt3-plugin_1.0.0-1vmw.600.0.0.2494585, VMware_bootbank_lsu-lsi-megaraid-sas-plugin_1.0.0-2vmw.600.0.11.2809209, VMware_bootbank_lsu-lsi-mpt2sas-plugin_1.0.0-4vmw.600.1.17.3029758, VMware_bootbank_misc-drivers_6.0.0-2.52.4600944, VMware_bootbank_net-e1000_8.0.3.1-5vmw.600.0.0.2494585, VMware_bootbank_net-e1000e_3.2.2.1-1vmw.600.1.26.3380124, VMware_bootbank_net-forcedeth_0.61-2vmw.600.0.0.2494585, VMware_bootbank_nmlx4-core_3.0.0.0-1vmw.600.0.0.2494585, VMware_bootbank_nmlx4-en_3.0.0.0-1vmw.600.0.0.2494585, VMware_bootbank_nmlx4-rdma_3.0.0.0-1vmw.600.0.0.2494585, VMware_bootbank_nvme_1.0e.0.35-1vmw.600.2.34.3620759, VMware_bootbank_ohci-usb-ohci_1.0-3vmw.600.0.0.2494585, VMware_bootbank_rste_2.0.2.0088-4vmw.600.0.0.2494585, VMware_bootbank_sata-ahci_3.0-22vmw.600.2.34.3620759, VMware_bootbank_sata-ata-piix_2.12-10vmw.600.0.0.2494585, VMware_bootbank_sata-sata-nv_3.5-4vmw.600.0.0.2494585, VMware_bootbank_sata-sata-sil24_1.1-1vmw.600.0.0.2494585, VMware_bootbank_sata-sata-sil_2.3-4vmw.600.0.0.2494585, VMware_bootbank_scsi-aacraid_1.1.5.1-9vmw.600.0.0.2494585, VMware_bootbank_scsi-adp94xx_1.0.8.12-6vmw.600.0.0.2494585, VMware_bootbank_scsi-aic79xx_3.1-5vmw.600.0.0.2494585, VMware_bootbank_scsi-fnic_1.5.0.45-3vmw.600.0.0.2494585, VMware_bootbank_scsi-ips_7.12.05-4vmw.600.0.0.2494585, VMware_bootbank_scsi-megaraid-mbox_2.20.5.1-6vmw.600.0.0.2494585, VMware_bootbank_scsi-megaraid-sas_6.603.55.00-2vmw.600.0.0.2494585, VMware_bootbank_scsi-megaraid2_2.00.4-9vmw.600.0.0.2494585, VMware_bootbank_scsi-mptsas_4.23.01.00-9vmw.600.0.0.2494585, VMware_bootbank_vsan_6.0.0-2.52.4590152, VMware_bootbank_vsanhealth_6.0.0-3000000.3.0.2.52.4527730, VMware_bootbank_xhci-xhci_1.0-3vmw.600.2.52.4600944, VMware_locker_tools-light_6.0.0-2.43.4192238 VIBs Skipped: VMW_bootbank_ima-qla4xxx_2.02.18-1vmw.650.0.0.4564106, VMW_bootbank_net-nx-nic_5.0.621-5vmw.650.0.0.4564106, VMW_bootbank_net-tg3_3.131d.v60.4-2vmw.650.0.0.4564106, VMW_bootbank_scsi-qla4xxx_5.01.03.2-7vmw.650.0.0.4564106 Much better. Let\u0026rsquo;s issue the reboot:\n[root@vsphere-sap01:~] reboot\n\u0026hellip; and validate the version after the system comes back online:\n[root@vsphere-sap01:~] vmware -v VMware ESXi 6.5.0 build-18678235 Since it\u0026rsquo;s been two years since this cluster had any attention, there must be patches that should be applied. Since the host is already in maintenance mode with the guests hosted elsewhere, let\u0026rsquo;s take advantage and use the GUI to patch this system.\nOnce all the patching and rebooting is finished, exit maintenance mode:\n[root@vsphere-sap01:~] esxcli system maintenanceMode set --enable false\n","date":"2021-12-20T23:38:30-05:00","permalink":"https://gbeifuss.github.io/p/esxi-6.0-to-6.5-upgrade/","title":"ESXi 6.0 to 6.5 Upgrade"},{"content":"I ran into an issue earlier this week where I had to remove a computer from a particular domain\u0026hellip; but the domain no longer existed. This meant that I couldn\u0026rsquo;t provide \u0026lsquo;domain admin\u0026rsquo; credentials to nicely remove the computer and join it back to WORKGROUP.\nFortunately, this can be done from the command prompt via. WMI.\nI started by logging into the computer as a local administrator.\nstart /B /W wmic.exe /interactive:off ComputerSystem Where \u0026#34;Name=\u0026#39;%computername%\u0026#39;\u0026#34; Call UnJoinDomainOrWorkgroup FUnjoinOptions=0 start /B /W wmic.exe /interactive:off ComputerSystem Where \u0026#34;Name=\u0026#39;%computername%\u0026#39;\u0026#34; Call JoinDomainOrWorkgroup name=\u0026#34;WORKGROUP\u0026#34; After a reboot, the computer was no longer a domain member, but a member of WORKGROUP.\n","date":"2021-11-04T20:52:28-04:00","permalink":"https://gbeifuss.github.io/p/forcibly-remove-computer-from-domain/","title":"Forcibly Remove Computer from Domain"},{"content":"I was training a colleague on using git last week with my Powershell script repository. When I reviewed the git history, I realized that there were lots of entries that were no longer necessary: primarily scripts that I deleted because they were obsolete, or never moved past development. I wanted to find a way to purge all entries related to these files from the git repository.\ngit filter-branch --index-filter \u0026quot;git rm -rf --cached --ignore-unmatch FILEPATH\u0026quot; HEAD\nThis took about a dozen minutes to walk through my approx. 400 commits. Each commit has to be checked, and then removed if it referenced the file.\n","date":"2021-11-02T20:16:01-04:00","permalink":"https://gbeifuss.github.io/p/removing-git-history/","title":"Removing GIT History"},{"content":"I manage a 5-server Exchange 2016 DAG at my workplace. Recently, I needed to migrate about 60 users from one database to another. I happen to hate the browser-based Exchange Administration Console (EAC) because it\u0026rsquo;s not very helpful - there\u0026rsquo;s no way to see the status for many \u0026ldquo;in progress\u0026rdquo; tasks, and cancelling a task generally means you have no way of knowing if was completely cancelled, or partially cancelled\u0026hellip; very frustrating!\nI have a love for Powershell, so I fired up the EMC - hurrah!\nThe default number of active migrations is configurable via. Powershell, but I really don\u0026rsquo;t like changing server defaults if there\u0026rsquo;s another way to accomplish what I want. Changing server defaults means that there\u0026rsquo;s one more configuration variation that needs to be tracked in case I need to rebuild a server, or add another server to the DAG.\nGenerally speaking, I like to:\nMigrate all mailboxes Suspend all Move Requests Have Powershell report on the status of the migration every 30 seconds: While (Get-MoveRequest | Where-Object {$_.status -ne \u0026#34;InProgress\u0026#34;}){ Clear-Host; Get-MoveRequest | Where-Object {$_.status -eq \u0026#34;InProgress\u0026#34;} | Get-MoveRequestStatistics | sort PercentComplete -Descending Start-Sleep -Seconds 30 } Open a second EMC instance and have Powershell ensure there are 5 migrations happening concurrently. If there aren\u0026rsquo;t, resume another one: While ((Get-MoveRequest | Where-Object {$_.status -eq \u0026#34;suspended\u0026#34;}).count -gt 0){ If ((Get-MoveRequest | Where-Object {$_.status -eq \u0026#34;inprogress\u0026#34;}).count -lt 5){ (Get-MoveRequest | Where-Object {$_.status -eq \u0026#34;suspended\u0026#34;})[0] | Resume-MoveRequest } Start-Sleep -Seconds 300 } I also like to make sure that Indexing is disabled when migrating mailboxes: Set-MailboxDatabase tier3-d -IndexEnabled:$false\nThis helps to speed up the process, since Exchange isn\u0026rsquo;t trying to index the mailbox as the move is occurring.\nIf you don\u0026rsquo;t disable the indexing, you may see your migration slow to a crawl, with the error StalledDueToTarget_ContentIndexing. Oof.\nAfter the move is complete, be sure to enable Indexing again, or your users won\u0026rsquo;t be able to search: Set-MailboxDatabase tier3-d -IndexEnabled:$true\nThen, validate that Indexing is set correctly on all the databases: Get-MailboxDatabase | Select Name,IndexEnabled\n","date":"2021-10-28T08:54:40-04:00","permalink":"https://gbeifuss.github.io/p/managing-mailbox-moves-in-exchange-2016/","title":"Managing Mailbox moves in Exchange 2016"},{"content":"I recently completed the Linux Upskill Challenge, a month-long, self-paced, self-led course curated on Reddit. It\u0026rsquo;s hands-on and is best run with a publically accessible remote server, so that you\u0026rsquo;re mimicing a production server that your organization might host online. (There\u0026rsquo;s less pressure in administering a server that\u0026rsquo;s only locally available!)\nThese are my thoughts and notes about each day\u0026rsquo;s content - not formal study notes, but informal jot notes on the things I found interesting and important.\n/r/linuxupskillschallenge\rDay 0 - Getting Started\nCreated new Azure Ubuntu VM. Used SSH key for authentication.\nAzure returned a .PEM file. Kitty won\u0026rsquo;t take this - I had to use puttygen.exe to load the PEM and output a PPK. I then created a Kitty profile using this PPK (Connection -\u0026gt; SSH -\u0026gt; Auth) and username azureuser (Connection -\u0026gt; Data -\u0026gt; Autologin username)\nhttps://docs.bitnami.com/azure/faq/get-started/connect-ssh/\nlinux is case-sensitive\nDay 1 - Getting Connected\nLog in with Windows Terminal (Powershell): ssh -i .\\Downloads\\Ubuntu_SSH.pem azureuser@23.96.4.131\nuname -a shows some server/OS information\nuptime shows the uptime\nfree shows memory information\ndf -h shows disk space information\nDay 2 - Navigation\ncd and cd ~ do the same thing - return you to your home directory\nman brings up the manual/help for commands\npushd is a way to create a trail of where you\u0026rsquo;ve been. dirs -v will list the trail easily. popd will move you around the stack of directories\nDay 3 - Working as Administrator\n/etc/shadow is where the hashed passwords are kept\nsudo will let you run a command as root\nsudo -i will pop an interactive sudo session if you\u0026rsquo;ve a number of commands to run. logoff or exit will bring you back. The command prompt will change from $ to # to show you\u0026rsquo;re running as sudo.\nAll sudo uses are logged in /var/log/auth.log\nsudo hostnamectl set-hostname XXXX will change the hostname of the server\ntimedatectl will adjust the time/date settings. TAB will autocomplete and/or show other subsequent commands\nDay 4 - Package Management \u0026amp; File Structure\napt is the Ubuntu package manager for working with applications\napt search \u0026quot;search term\u0026quot; will find matching packages\napt list --installed will list all installed packages\napt install \u0026quot;packagename\u0026quot; will install the package\nLinux uses a different tree structure than Windows.\n/ is the mountpoint - this is eqivalent to C:\\\n/root is the home directory for the root user\n/usr/ is where user directories are kept\n/var is where variable files are stored. /var/log is primarily where logs are stored.\n/bin is where binary files (non-system level) are stored\n/sbin is where system-level binaries are stored\n/etc is where most configuration files are stored\n/etc/passwd is the file containing userids, SSIDs and passwords\nDay 5 - Less\nless is similar to more, except it doesn\u0026rsquo;t read the whole file at load. In less:\nG skips to the end g skipts to the beginning q quits The bash shell stores your command history, writing it all to /usr/azureuser/.bash_history (files beginning with a . are hidden). history will list the contents of the file and allow you to manipulate it. Pressing up or down will navigate the history from the prompt.\nDay 6 - vi and vim\nvi is the classic unix editor. On most modern *nix systems, vi is an alias for vim - vi improved.\nh, j, k, l act as the arrow keys within vim, although modern vim instances also have support for the arrow keys\nvi has two modes: normal mode and insert mode. insert mode actually allows editing of what\u0026rsquo;s on the screen, while normal mode is used for inputing commands. esc esc will always return to normal mode.\nMany vi commands are made from an operator and a motion. For example, dw will delete (d) a word (w). Quick list of motions:\nw will jump to the next word e will jump to the end of the current word $ will jump to the end of the line Typing a number before a motion will repeat it that many times\nu will undo actions x will delete the character your cursor is on i (insert), a (append), c (change), r (replace) ce will delete the remaining letters in the current word, after the cursor\nc$ will delete the remaining text in the line\n33dd will cut the line 33 times (equivalent to deleting 33 rows). They can be pasted wherever your cursor is with p\ngg will jump to the start of the file, while G will jump to the end.\n:s/old/new will substitute \u0026rsquo;new\u0026rsquo; for the first instance of \u0026lsquo;old\u0026rsquo; in the current line :s/old/new/g will substitute \u0026rsquo;new\u0026rsquo; for all instances of \u0026lsquo;old\u0026rsquo; in the current line #,#s/old/new/g will substitute \u0026rsquo;new\u0026rsquo; for all instances of \u0026lsquo;old\u0026rsquo; between the two line numbers provides %s/old/new/g will substitute \u0026rsquo;new\u0026rsquo; for all instances of \u0026lsquo;old\u0026rsquo; in the file /SEARCHTERM will search for the first instance of the searchterm in the file. n will jump to the next instance\n% while on any bracket/parenthesis will jump to its match\nv starts a Visual selection. Then use an operator (ie. :w to write the selected text to a file)\n:r FILENAME will merge the contents of FILENAME into the open file.\n:r !ls will merge the output of ls into the current file.\n:!COMMAND will execute an external program in vi. For example :!ls will print the directory listing.\n:q! will quit without saving\n:w will write/save the file with the current file name. :w FILENAME will save it with the specified filename.\n:wq will save and quit\nDay 7 - Server \u0026amp; Services\ninstall apache2 (httpd)\nstop the apache service: sudo systemctl stop apache2\nstart the apache service: sudo systemctl start apache2\ncheck the status: sudo systemctl status apache2. This pulls information from the logs.\nApplication configuration is controlled by files under the /etc directory for most Linux distros. The Apache config is in /etc/apache2/apache2.conf\nIn /etc/apache2/apache2.conf there\u0026rsquo;s the line with the text: IncludeOptional conf-enabled/*.conf This tells Apache that the *.conf files in the subdirectory conf-enabled should be merged in with those from /etc/apache2/apache2.conf at load.\nThe location of the default webpage is defined by the DocumentRoot parameter in the file /etc/apache2/sites-enabled/000-default.conf \u0026hellip; in this case, it\u0026rsquo;s /var/www/html, so /var/www/html/index.html\nsudo apt update, then sudo apt upgrade will install all upgraded packages for any software that\u0026rsquo;s been installed with the apt package manager.\nDay 8 - grep and others\nDay 9 - networking \u0026amp; ports\nss is the replacement for the netstat comment. ss -ltpn will print the open ports. (use sudo for it to list the Processes that control these listeners)\niptables, nftables and ufw are all firewall utilities for Linux.\nsudo iptables -L will list firewall rules in place.\nSet UFW rules to allow SSH but deny HTTP: sudo ufw allow ssh; sudo ufw deny http\nEnable UFW: sudo ufw enable\nDay 10 - scheduled tasks\nsee your scheduled tasks with crontab -l, or root with sudo crontab -l\n/etc/cron lists the timing of daily, weekly \u0026amp; monthly jobs that run from /etc/cron.daily, etc/cron.weekly, etc.\nsystemd starts and stops services, but can also be used to run tasks via. timers: systemctl list-timers\nDay 11 - finding things\nlocate will search an index for \u0026quot;*something*\u0026quot; by default. The index is built by a nightly cron job but can be updated with sudo updatedb\nfind will try to match a file(s) based on criteria: find /var -name access.log or find /home -mtime -3 (find any file under /home modified in the last 3 days)\ngrep will search within plaintext files for specific text. grep -R -i \u0026quot;PermitRootLogin\u0026quot; /etc/* will search recursively through /etc/ for files containing the case-insensitive string \u0026lsquo;permitrootlogin\u0026rsquo;\nwhich will list the patch from the PATH statement that a utility runs from: which nano\nDay 12 - transferring things\nwinscp\u0026gt; open sftp://azureuser@23.96.4.131 -privatekey=\u0026quot;c:\\users\\gbeifuss\\downloads\\ubuntu_ssh.ppk\u0026quot;\nDay 13 - permissions\nls -l will list permissions: owner(user), group, others\nchmod u-w filename.txt will remove the W permission for the user\nchmod g-w filename.txt will remove the W permission for the group\nchmod o-w filename.txt will remove the W permission for others\nchmod u+w filename.txt will add the W permission for the user\ngroups will list the groups a user belongs to\ngroupadd will add a new group (sudo groupadd newgroup)\nusermod -a -G group username will add username to group\nDay 14 - users \u0026amp; groups\nAdd a new user with sudo adduser helen. (If not prompted by a password, set one for helen: sudo passwd helen)\nLogin as helen: sudo su helen\nPermissions for sudo are controlled by the visudo utility\nDay 15 - repositories\u0026hellip;\nLinux versions control the versions of applications that can be installed. Ubuntu 18.04 ships with Apache 2.4.29 - even if you install apache with apt 5 years later, you\u0026rsquo;ll get 2.4.29 installed. (Security patches are made to repositories, but by backporting fixes (from fixed versions) into older versions).\n/etc/apt/sources.list is the default apt repository source. This can be edited to enable multiverse/universe repositories, which \u0026ldquo;*may contain software which has been classified as non-free \u0026hellip; and may not include security updates\u0026rdquo;.\nUbuntu 20.04 ships with 99589 packages (apt-cache dump | grep \u0026quot;Package:\u0026quot; | wc -l)\nDay 16 - archiving \u0026amp; compressing\nUnlike Windows, Linux gathers content in one step, and compresses in another.\ntar stands for \u0026lsquo;Tape ARchive\u0026rsquo;\ntar -cvf myinits.tar /etc/init.d takes a snapshof of the files currently in /etc/init.d\nThis file can then be compressed: gzip myinits.tar, which will create myinits.tar.gz, a tarball. Tarballs often use the extension .tgz\n.bz2 files use a different compression than gzip - it uses higher compression, but takes longer. A .bz2 file can only contain a single file, so it\u0026rsquo;s usually used to futher compress a tarball or other archive. In practice, these two steps can be condensed into on: tar -cvzf myinits.tgz /etc/init.d\nc = create v = verbose z = zip/compress f = specify the output file x = extract (expand) j = uncompress a .bz2 file first Day 17 - from the source\nbuild-essential is a standard bundle of complilers and similar tools: sudo apt install build-essential\nDownload the latest nmap: wget -v https:/nmap.org/dist/nmap-7.92.tar.bz2\nExtract the files: tar -jxvf nmap-7.92.tarb.bz2\n./configure is a script which checks the server\nmake complies the software, typically via the GNU complier gcc\nsudo make install will take the compiled files, install them plus documentation, and in some cases setup services and scheduled tasks.\nAny software installed outside of apt won\u0026rsquo;t be updated by apt update, so new releases and security fixes need to be manually tracked.\nDay 18 - log rotation\nlogrotate is used by cron to rotate logs (/etc/cron.daily/logrotate), (/etc/logrotate.conf + /etc/logrotate.d)\nFor example, edit /etc/logrotate.d/apache2 to adjust the configuration for all apache logs.\nDay 19 - inodes, symlinks and other shortcuts\nLinux disks use ext3, ext4, zfs, or perhapsntrfs. Above that sits the VFS - Linux Virtual FileSystem\nEach filename points to an inode (a numerical value), which is seen most easily in two places: ls -i and stat:\nls -li /etc/hosts 35356766 -rw------- 1 root root 260 Nov 25 04:59 /etc/hosts stat /etc/hosts File: `/etc/hosts\u0026#39; Size: 260 Blocks: 8 IO Block: 4096 regular file Device: 2ch/44d Inode: 35356766 Links: 1 Access: (0600/-rw-------) Uid: ( 0/ root) Gid: (\t0/\troot) Access: 2012-11-28 13:09:10.000000000 +0400 Modify: 2012-11-25 04:59:55.000000000 +0400 Change: 2012-11-25 04:59:55.000000000 +0400 Several filenames could point to the same inode, and have the same contents/permissions/ownerships.dates. These attributes are stored at the inode level, hard links. ln is used to create hard and symbolic links (symlinks):\nln /etc/passwd link1 ln -s /etc/passwd link2 Hard links:\npoint to files, not directories can\u0026rsquo;t reference a file on another disk/volume link references work even if it is moved refer to physical locations/inodes on disk Symbolic links (symlinks):\ncan link to directories can reference files/directories on another disk/volume remain if the original file is deleted will NOT reference the file anymore if it\u0026rsquo;s moved reference abstract names and not physical locations have their own inode Day 20 - scripts\nScripts are regular files with the X permission chmod +x filename\nScripts typically start with #!/bin/bash\n","date":"2021-10-28T08:54:12-04:00","permalink":"https://gbeifuss.github.io/p/linux-upskill-challenge/","title":"Linux Upskill Challenge"},{"content":"Hugo has integrated support (via. Internal Templates) for Google Analytics. An internal template adds support for some very common use cases on static websites. For example: Google Analytics, Disqus comments, Google News, and Twitter Cards. Internal templates certainly exist in v.0.87 and higher, but I\u0026rsquo;m not sure when support for them was first added. If your install of Hugo is out of date, now might be a good time to update it.\nThe first step, is to sign up for Google Analytics and adding your particular site (aka. property), if you\u0026rsquo;ve not done so already. I\u0026rsquo;ll wait while you do that\u0026hellip; \u0026hellip;twiddles thumbs\u0026hellip;\nNext, we need to find the Measurement ID (previously, Tracking ID) that Google has assigned to the property that you have setup.\nClick Admin Click Data Streams Click on your property from the list Measurement ID should appear in the upper-right portion of the panel Copy or make a note of the Measurement ID The Measurement ID should have the format G-HMPAE821CR. The ID is unique! With this in hand, let\u0026rsquo;s tell Hugo to use it for all our webpages.\nOpen the config file. (This will be located in the root directory of your Hugo site. The config file can be variety of different formats - mine is YAML so it\u0026rsquo;s named config.yaml).\nThere will be a line that looks something like this:\n# GA Tracking ID googleAnalytics: Add the Measurement ID that you\u0026rsquo;d previously noted to the googleAnalytics: field:\n# GA Tracking ID googleAnalytics: G-HMPAE821CR Save the file.\nThat’s really all you have to do to set the Google Analytics Tracking ID in Hugo. My theme - hugo-stack-theme] - automatically has support for this internal template, so my site started sending tracking data to Google Analytics right away.\nReferences\rAn article I referenced when figuring this out was:\nSetting Up Google Analytics on Hugo\n","date":"2021-10-05T14:54:15-04:00","permalink":"https://gbeifuss.github.io/p/hugo-google-analytics/","title":"Hugo \u0026 Google Analytics"},{"content":"VS Code automatically trims any trailing whitespace in a file, which makes sense for a lot of formats, but in markdown, a double trailing whitespace has special meaning - it acts as a LF and creates a line break without a hard return (CRLF). This automatic trim also occurs when formatting a file according to the language\u0026rsquo;s rules.\nLet\u0026rsquo;s configure VS Code to continue automatically trimming trailing whitespace, except for markdown files:\nOpen the Command Palette (View, Command Palette…. or CTRL+SHIFT+P) Select Preferences: Open Settings (JSON) Add the snippet: \u0026#34;[markdown]\u0026#34;: { \u0026#34;files.trimTrailingWhitespace\u0026#34;: false } Save settings.json ","date":"2021-10-05T09:51:28-04:00","permalink":"https://gbeifuss.github.io/p/vs-code-markdown/","title":"VS Code \u0026 Markdown"},{"content":"I\u0026rsquo;ve recently been editing a Powershell script that manages our Let\u0026rsquo;s Encrypt certificates and their integration with Netscaler. I\u0026rsquo;ve been using John Billeken\u0026rsquo;s excellent GenLeCertForNS script to do the heavy lifting, but I\u0026rsquo;ve written a front-end wrapper for it to handle the way I need it to work in my environment. [I started using this with v.2.63, which didn\u0026rsquo;t have email capabilities. My front-end wrapper adds email reporting, plus obfuscuating any credentials in the log, and loading the credentials from an encypted XML file]. I was rewriting my wrapper to include SAN support and to update two Netscaler appliances with the same certificate: if the CN matched certain values, the script will find the same Let\u0026rsquo;s Encrypt certificate .pfx, upload it to the Netscaler at our warm Disaster Recovery site and properly bind it.\nTo do that, I needed to make a number of API calls to Netscaler, which I generally discuss in my article Powershell \u0026amp; Netscaler API. This time, I needed to pass specific parameters in JSON format in order to link and unlink the certificate from others in the chain.\nAccording to the Citrix ADC NITRO API Reference, I need:\n**URL:** http://\u0026lt;Citrix-ADC-IP-address(NSIP)\u0026gt;/nitro/v1/config/sslcertkey?action=link **HTTP Method:** POST **Request Headers:** Cookie:NITRO_AUTH_TOKEN=\u0026lt;tokenvalue\u0026gt; Content-Type:application/json **Request Payload:** { \u0026#34;sslcertkey\u0026#34;: { \u0026#34;certkey\u0026#34;:\u0026lt;String_value\u0026gt;, \u0026#34;linkcertkeyname\u0026#34;:\u0026lt;String_value\u0026gt; } } This is a JSON format. While I could just use a straight string (ie. $string=\u0026quot;{\u0026quot;sslcertkey\u0026quot;:{\u0026quot;linkcertkeyname\u0026quot;:\u0026quot;Sectigo RSA CA 2018\u0026quot;,\u0026quot;certkey\u0026quot;:\u0026quot;sts.agricorp.com\u0026quot;}}\u0026quot;)) or a multiline string (ie. $string=\u0026quot;@TEXT@\u0026quot;), I figured that I should take the effort to learn a bit about taking an object and converting it to JSON.\nMy initial attempts to get this working were all wrong. I finally managed to get the key/value pairs properly written to a hashtable, and then converted it to JSON.\n$array = @{\u0026#34;certkey\u0026#34; = \u0026#34;$CertificateCertKeyName\u0026#34;; \u0026#34;linkcertkeyname\u0026#34; = \u0026#34;$IntermediateCACertKeyName\u0026#34; } $hashtablePayload = @{\u0026#34;sslcertkey\u0026#34; = $array } $jsonPayload = ConvertTo-Json -InputObject $hashtablePayload -Depth 100 -Compress The output of $jsonPayload looks like this:\n{\u0026quot;sslcertkey\u0026quot;:{\u0026quot;linkcertkeyname\u0026quot;:\u0026quot;Sectigo RSA CA 2018\u0026quot;,\u0026quot;certkey\u0026quot;:\u0026quot;sts.company.com\u0026quot;}}\n\u0026hellip;perfectly formatted JSON that I can pass through to the Netscaler:\n$URI = \u0026#34;https://netscaler03.corp.company.com/nitro/v1/config\u0026#34; $headers = @{\u0026#39;X-NITRO-USER\u0026#39; = \u0026#34;$netscalerID\u0026#34;; \u0026#39;X-NITRO-PASS\u0026#39; = \u0026#34;$netscalerPwd\u0026#34; } ... $hashtablePayload = @{ } $hashtablePayload = @{\u0026#34;sslcertkey\u0026#34; = @{\u0026#34;certkey\u0026#34; = \u0026#34;$CertificateCertKeyName\u0026#34;; \u0026#34;linkcertkeyname\u0026#34; = \u0026#34;$IntermediateCACertKeyName\u0026#34; } } $jsonPayload = ConvertTo-Json -InputObject $hashtablePayload -Depth 100 -Compress $response = (Invoke-RestMethod -Method POST -Uri \u0026#34;$uri`/sslcertkey?action=link\u0026#34; -ContentType application/json -Headers $headers -Body $JsonPayload) ","date":"2021-10-04T15:47:05-04:00","permalink":"https://gbeifuss.github.io/p/working-with-json-in-powershell/","title":"Working with JSON in Powershell"},{"content":"This is part 3 of a 3-part series.\nCisco Management Tunnel. Cisco Management Tunnel - NDES Setup Cisco Management Tunnel - ASA Setup If you\u0026rsquo;ve stuck with me so far, now we come to the payoff - a working Management Tunnel! Let\u0026rsquo;s jump in.\nThis article is based on my jot notes. There may be missing steps or information, although I\u0026rsquo;ve tried my best to make sure everything is here, at least at a high-level. I use a mix of CLI and ASDM when working on the ASA, so be prepared to jump around a little.\nRequirements\rRequires ASA 9.0.1 (or later) and ASDM 7.10.1 (or later) Connects whenever the user initiated VPN tunnel is disconnected, before or after user login. The Management VPN tunnel is not established when a trusted network is detected by the Trusted Network Detection (TND) feature or when an AnyConnect software update is in progress. Disconnects whenever the user initiates a VPN tunnel, before or after user login. Uses only machine store certificate authentication. Requires split-tunneling configuration, by default, to avoid impacting user initiated network communication (since the management VPN tunnel is meant to be transparent to the end user). Works with backup server list. Currently available only on Windows and macOS. Linux support will be added in subsequent releases. Install Certificates\rIf you don\u0026rsquo;t already have your Issuing CA certificate installed on the ASA, you\u0026rsquo;ll need to do that. I used the ASDM: Device Management \u0026gt; Certificate Management \u0026gt; CA Certificates. We can import it directly from the NDES/SCEP server we just set up by clicking \u0026lsquo;Add\u0026rsquo; and entering the proper information. It should be something like: http://10.0.0.1/certsrv/mscep/mscep.dll\nNote: We will need two Profiles - one for Users to authenticate to and get the certificate, and one for the actual Management Tunnel. I\u0026rsquo;ll call it the User Tunnel just to be clear, and we\u0026rsquo;ll work on it first.\nCreate a new VPN Connection Profile\rCreate a new VPN Connection profile on the ASA via. ASDM. I called mine \u0026ldquo;Helpdesk\u0026rdquo;, since that\u0026rsquo;s who my pilot testers will be. I don\u0026rsquo;t want to mess around with the production VPN that 100+ users are connected to!\nSet the Authentiction method AAA and certificate. Also configure the AAA Server group. You can use LOCAL, or if you\u0026rsquo;re tied into another authentication service for MFA, like RSA, you can select it. Configure DHCP, DNS and the domain name Enable SCEP Enrollment (Advanced\\General) Set Group Policy to use SCEP\rThe Group Policy will need to be edited as well. Either edit an existing one, or create a new one and associate it with the previously created Connection Profile. I did this via. CLI, under a group policy named Helpdesk: group-policy Helpdesk attributes scep-forwarding-url value http://server.corp.company.com/certsrv/mscep/mscep.dll exit I prefer the CLI for this because it gives instant feedback about the success or failure. If you happen to disregard the requirement that NDES is NOT installed on your CA, then this step will repeatedly fail. \u0026hellip;Or so I\u0026rsquo;ve heard :)\nEdit Client Profile\rThese next steps are best done in the ASDM, because the output is XML files. CLI cannot manipulate them directly.\nOpen the Client Profile that you\u0026rsquo;ll use for the User Tunnel - mine is called \u0026lsquo;Helpdesk\u0026rsquo;. Open the Preferences (Part 2) page. Enable \u0026lsquo;Automatic VPN Policy\u0026rsquo; Add your internal Trusted DNS Domains and Servers. If you have multiple, separate them with commas. Trusted Network Policy = Disconnect Untrusted Network Policy = Connect Open the Certificate Enrolment page. Enable Certificate Enrollment Set the Certificate Expiration Threshold (days). Enter %USER% or %MACHINEID% in the Name (CN) box. Either will work, but will affect whom the certificate is issued to (machine name or user id), which in turn will affect how the endpoint shows up in the VPN monitoring logs. Enter whatever you\u0026rsquo;d like under Department, but we will reuse this entry later on. I chose AnyConnect for the sake of simplicity. Enter your domain name in the CA Domain field: corp.company.com Enter the keysize. I recommend 2048, which is what NDES should be expecting in the certificate request. Complete any other certificate fields you\u0026rsquo;d like as they\u0026rsquo;re optional, but nice to have completed. Open the Certificate Matching page. Click \u0026lsquo;Add\u0026rsquo; under the \u0026lsquo;Distinguished Name (Max 10)\u0026rsquo; section. Select OU in the Name drop down box. Then type in the value you entered for OU in the last step (under Certificate Enrollment) ito the Pattern field. For me, it’s AnyConnect. Ensure \u0026lsquo;Match Case\u0026rsquo; is enabled. Click OK and you’ll see the entry appear under \u0026lsquo;Distinguished Name (Max 10)\u0026rsquo;. Click OK as we\u0026rsquo;re done. This process will allow the ASA to select the correct cert during authentication. Create the Management Tunnel Group, Group Policy\rBack to the CLI! We\u0026rsquo;ll create a Management Tunnel Profile, and link it to our existing Split-Tunnel list. Please note, the Profile name is case-sensitive!\ngroup-policy Management internal group-policy Management attributes dns-server value 192.168.1.23 192.168.1.24 vpn-tunnel-protocol ssl-client split-tunnel-policy tunnelspecified split-tunnel-network-list value VPNSplitTunnelList default-domain value corp.agricorp.com client-bypass-protocol enable address-pools value SSLVPNDHCP anyconnect-custom ManagementTunnelAllAllowed value Value tunnel-group Management type remote-access tunnel-group Management general-attributes default-group-policy Management tunnel-group Management webvpn-attributes group-alias Management enable group-url https://vpn.company.com/Management enable The \u0026lsquo;Client Bypass Protocol\u0026rsquo; needs to be enabled if you don\u0026rsquo;t have BOTH IPv4 and IPv6 address pools configured to issue IP addresses. If you only have IPv4, like me, then the Client Bypass Protocol must be enabled. If it’s not, and the everything else is configured correctly, you’ll see the Management Connection State show as \u0026ldquo;Disconnected (invalid VPN configuration)\u0026rdquo;\nCreate the Management Tunnel Profile\rBack to ASDM! Add an AnyConnect Client Profile under Remote Access VPN \u0026gt; Network (Client) Access \u0026gt; AnyConnect Client Profile:\nEnter a Profile Name Select \u0026lsquo;AnyConnect Management VPN Profile\u0026rsquo; from the Profile Usage dropdown box. The resulting profile will have an .vpnm extension. Select the previously created User VPN Group Policy. In my case, it\u0026rsquo;s Helpdesk. This associates the two policies, and when a user authenticates to Helpdesk, it\u0026rsquo;ll download the Management Tunnel profile as well. Now, edit the Management VPN Profile:\nOpen the Preferences (Part 2) page. Per Cisco: \u0026ldquo;For a consistent user experience, we recommend that you use identical Trusted Network Detection settings in both user and management VPN tunnel profiles.\u0026rdquo; Enable \u0026lsquo;Automatic VPN Policy\u0026rsquo; Add your internal Trusted DNS Domains and Servers. If you have multiple, separate them with commas. Trusted Network Policy = Disconnect Untrusted Network Policy = Connect Open the Certificate Matching page. Click \u0026lsquo;Add\u0026rsquo; under the \u0026lsquo;Distinguished Name (Max 10)\u0026rsquo; section. Select OU in the Name drop down box. Then type in the value you entered for OU in the last step (under Certificate Enrollment) ito the Pattern field. For me, it’s AnyConnect. Ensure \u0026lsquo;Match Case\u0026rsquo; is enabled. Click OK and you’ll see the entry appear under \u0026lsquo;Distinguished Name (Max 10)\u0026rsquo;. Open the Server List page. Click Add. Enter a Display Name (which doesn\u0026rsquo;t seem to actually be used anywhere else). Under FQDN, enter the same value that\u0026rsquo;s in your User Tunnel - vpn.company.com for example. Under User Group, enter the Management VPN Tunnel Alias (configured under the Management Tunnel Group). In my case, it\u0026rsquo;s Management .\nClick OK until you\u0026rsquo;re back at the main ASDM page, then click Apply to write the changes. At this point, everything should be setup. Let\u0026rsquo;s test.\nManagement Tunnel Behaviour\rLoad AnyConnect and establish a connection to your User Tunnel as usual. After successfully connecting, the ASA will contact the NDES for certificate enrollment on your behalf. After a few seconds, AnyConnect will issue a notice: popup-certificate-enrollment-succeeded.png) Click OK to acknowledge the notice. AnyConnect will indeed disconnect you and try to re-establish a connection. You can either authenticate and establish a User Tunnel, or click Cancel. If you click Cancel, AnyConnect will take a few minutes to perform Trusted Network Detection, determine you\u0026rsquo;re not on the corporate network, then transparently establish the Management Tunnel using your certificate. You can see the Management Tunnel status in the AnyConnect client Statistics: When you connect back to a User Tunnel, the Management Tunnel will disconnect and show Disconnected (user tunnel active).\nBehind The Scenes\rAnyConnect actually grabs 2 certificates based on your VPN username and stores them in Local User Certificates\\Personal\\Certificates and Local Machine Certificates\\Personal\\Certificates. If you created a standard certificate template, NDES issues them with a 1 year validity period. However, AnyConnect inspects the \u0026lsquo;Date Issued\u0026rsquo; field and compares the age to the value set under Certificate Expiration Threshold (days). If the value = 2, then if the certificate is older than 2 days, AnyConnect will request another certificate the next time a VPN session is established.\nIf you need to delete your certificates during testing and pilot use:\nStop the Cisco AnyConnect service Open up the certificates mmc (mmc.exe). Add both the Current User plugin and the Current Machine plugin. Delete the certificate in Local User Certificates\\Personal\\Certificates Delete the certificate in Local Machine Certificates\\Personal\\Certificates Start the Cisco AnyConnect service again I also had to adjust the Dynamic Access Policy in use by the User Profile. We were checking for the presence of a certain AV product, but it was causing problems. Now we don\u0026rsquo;t check that, but check for other things.\nI also originally set anyconnect-custom-data ManagementTunnelAllAllowed Value true/true based on another article I\u0026rsquo;d read, but this is not necessary.\nReferences\rSome articles I referenced when figuring this out were:\nCisco AnyConnect Management VPN Tunnel (Microsoft CA)\nConfigure AnyConnect Management VPN Tunnel on ASA\n","date":"2021-09-24T10:12:43-04:00","permalink":"https://gbeifuss.github.io/p/cisco-management-tunnel-asa-configuration/","title":"Cisco Management Tunnel - ASA Configuration"},{"content":"This is part 2 of a 3-part series.\nCisco Management Tunnel. Cisco Management Tunnel - NDES Setup Cisco Management Tunnel - ASA Setup. Since an AnyConnect Management Tunnel seems like it will help resolve my organization\u0026rsquo;s work-from-home challenges, let\u0026rsquo;s setup a pilot.\nThe first thing we need to do is configure the certificates that are needed for user authentication.\nInstall and Configure NDES\rWe already have a dedicated server running AD Certificate Services (AD CS), acting as our Certificate Authority (CA). Network Device Enrollment Service (NDES) acts as a registration authority for a CA using Simple Certificate Enrollment Protocol (SCEP). The CA has to fully trust the NDES to verify inbound certificate requests. The result is that NDES owns an extremely powerful type of certificate (Exchange Enrollment Agent (Offline request)), which allows NDES to request certificates with almost any subject from the CA. All of that to say, it\u0026rsquo;s important to secure NDES as much as warranted.\nThe most important advice I can give is to pay attention to the warning that NDES cannot run on a CA Server! Don\u0026rsquo;t ask me how I know.\nI decided to piggyback NDES on an existing Windows 2016 server we have, so I was able to use Powershell for most of the heavy lifting. I usually roll with Powershell 7, but there are steps that require Powershell 5.1. I\u0026rsquo;ll try to be clear about when Powershell 5.1 is needed.\nCreate gMSA\rLet\u0026rsquo;s start by creating (and testing the creation of) the gMSA:\nNew-ADServiceAccount -Name NDES-gMSA -DNSHostName NDES-gMSA.corp.company.com -PrincipalsAllowedToRetrieveManagedPassword SERVER$ Enter-PSSession corpca Add-WindowsFeature RSAT-AD-PowerShell Install-ADServiceAccount NDES-gMSA Test-ADServiceAccount NDES-gMSA exit Install NDES\rYou can\u0026rsquo;t install NDES as a gMSA, but we\u0026rsquo;ll convert it later. I\u0026rsquo;m going to use my admin account for the initial installation, and convert it to a gMSA later. You cannot initially install it as a gMSA. Let\u0026rsquo;s install the NDES role first:\n#Install-AdcsNetworkDeviceEnrollmentService -Force -ServiceAccountName \u0026#34;CORP\\admin\u0026#34; -ServiceAccountPassword \u0026#34;System.Security.SecureString\u0026#34; -RAName \u0026#34;SERVER-MSCEP-RA\u0026#34; -RAEmail \u0026#34;helpdesk@company.com\u0026#34; -RACompany \u0026#34;Company\u0026#34; -RADepartment \u0026#34;IT-CNS\u0026#34; -RACity \u0026#34;Guelph\u0026#34; -RAState \u0026#34;Ontario\u0026#34; -RACountry \u0026#34;CA\u0026#34; -SigningProviderName \u0026#34;Microsoft Strong Cryptographic Provider\u0026#34; -SigningKeyLength \u0026#34;2048\u0026#34; -EncryptionProviderName \u0026#34;Microsoft Strong Cryptographic Provider\u0026#34; -EncryptionKeyLength \u0026#34;2048\u0026#34; -CAConfig \u0026#34;corpca.corp.company.com\\Company Corporate Issuing CA\u0026#34; Configure IIS\rNow, add the installation account to the IIS_IUSRS group:\nAdd-LocalGroupMember -Group IIS_IUSRS -Member corp\\admin, corp\\NDES-gMSA Since NDES was installed on a server with the Web Enrollment server role, the IIS virtual directories won\u0026rsquo;t show. However, the site is still working and vsible under the IIS Applications, and IIS Application Pools pages. We need to use Powershell 5.1 to check that SSL is properly set on the MSCEP_ADMIN site by first querying the current values, then setting them:\nGet-WebConfigurationProperty -pspath ‘MACHINE/WEBROOT/APPHOST’ -location ‘Default Web Site/CertSrv/mscep_admin’ -filter “system.webServer/security/access” -name “sslFlags” Set-WebConfigurationProperty -pspath ‘MACHINE/WEBROOT/APPHOST’ -location ‘Default Web Site/CertSrv/mscep_admin’ -filter “system.webServer/security/access” -name “sslFlags” -value “Ssl” To return to the stock IIS settings, run this command from Powershell 5.1:\nSet-WebConfigurationProperty -pspath ‘MACHINE/WEBROOT/APPHOST’ -location ‘Default Web Site/CertSrv/mscep_admin’ -filter “system.webServer/security/access” -name “sslFlags” -value “None” ` The SCEP site (MSCEP) must remain accessible via. HTTP, as that\u0026rsquo;s the only mechanism that the ASA supports.\nRun NDES under a gMSA\rNext, I used the IIS GUI to change the SCEP application pool identity (which is actually the NDES Application Pool) to the gMSA (under Advanced Settings), and restarted IIS.\nSince we\u0026rsquo;re using a gMSA, I need to grant that account permissions to manage the private key for the certs we have, but it needs to be done via. powershell since remote GUI access doesn\u0026rsquo;t support it. I modified a script from Assigning Read Access to Windows Private Key:\nforeach ($certobj in (gci cert:\\localmachine\\my\\ | where {$_.subject -like \u0026#34;E=helpdesk*\u0026#34;})){ $rsaCert = [System.Security.Cryptography.X509Certificates.RSACertificateExtensions]::GetRSAPrivateKey($CertObj) $fileName = $rsaCert.key.UniqueName $path = \u0026#34;$env:ALLUSERSPROFILE\\Microsoft\\Crypto\\rsa\\machinekeys\\$filename\u0026#34; $permissions = Get-Acl -Path $path $rule = new-object security.accesscontrol.filesystemaccessrule \u0026#34;corp\\ndes-gmsa$\u0026#34;, \u0026#34;read\u0026#34;, allow $permissions.AddAccessRule($rule) Set-Acl -Path $path -AclObject $permissions (get-acl -path $path).access #*validate that the settings worked } FileSystemRights : FullControl AccessControlType : Allow IdentityReference : NT AUTHORITY\\SYSTEM IsInherited : False InheritanceFlags : None PropagationFlags : None FileSystemRights : FullControl AccessControlType : Allow IdentityReference : BUILTIN\\Administrators IsInherited : False InheritanceFlags : None PropagationFlags : None FileSystemRights : Read, Synchronize AccessControlType : Allow IdentityReference : CORP\\ndes-gmsa$ IsInherited : False InheritanceFlags : None PropagationFlags : None Now, configure NDES to use the certificate template we\u0026rsquo;ll create in the next step. I\u0026rsquo;m going to create a certificate template named \u0026lsquo;AnyConnect\u0026rsquo; for NDES to issue to my ASA via. SCEP. Change the default GeneralTemplateProperty registry value from \u0026ldquo;IPSECIntermediateOffline\u0026rdquo; to \u0026ldquo;AnyConnect\u0026rdquo;, and set NDES to not require a password:\nSet-ItemProperty -Path HKLM:SOFTWARE\\Microsoft\\Cryptography\\MSCEP\\ -Name GeneralPurposeTemplate -Value \u0026#34;AnyConnect\u0026#34; Set-ItemProperty -Path HKLM:SOFTWARE\\Microsoft\\Cryptography\\MSCEP\\EnforcePassword -Name EnforcePassword -Value 0 Create Certificate Template\rLet\u0026rsquo;s create a new template to use for issuing to AnyConnect. I prefer to create new template because we can modify it in many ways not available when we just use the default \u0026lsquo;User\u0026rsquo; template.\nOpen the Certificate Templates MMC. Make a copy of the \u0026lsquo;User\u0026rsquo; template, and configure these settings: rename to \u0026lsquo;AnyConnect\u0026rsquo; (General tab) set OS compatibility (Compatibility tab) edit Extensions/Applition Policies and remove all entries EXCEPT Client Authentication edit Security and add the CORP\\NDES-gMSA$ account. (If using the GUI to browse for this account, you may need to change the account filtering options to list gMSAs). Allow the account Enroll permission only. set Subject Name to \u0026lsquo;supply in the request\u0026rsquo;. Accept the popup notification. Clikc OK and close the Templates MMC. In the CA MMC, right-click Certificate Templates, select New, Certificate Template to Issue, then select AnyConnect. Your new template is now ready to issue! Reboot the NDES server for the registry settings made earlier to take effect.\nNow, let\u0026rsquo;s tackle the ASA configuration in Cisco Management Tunnel - ASA Setup.\nReferences\rSome articles I referenced when figuring this out were:\nNDES Deployment and troubleshooting\nSetting up NDES using a Group Managed Service Account (gMSA)\n","date":"2021-09-24T08:25:46-04:00","permalink":"https://gbeifuss.github.io/p/cisco-management-tunnel-ndes-setup/","title":"Cisco Management Tunnel - NDES Setup"},{"content":"This is part 1 of a 3-part series.\nCisco Management Tunnel Cisco Management Tunnel - NDES Setup Cisco Management Tunnel - ASA Setup. When the Covid-19 pandemic started, our organization sent everyone to work from home. Of course, with everyone on VPN and not continually connected, like in the office, that\u0026rsquo;s posed some problems for login scripts, and some of our automated processes like WSUS patch deployment, NTP time sync and SCCM. I recently heard this was an issue - I\u0026rsquo;m not sure why the sysadmins didn\u0026rsquo;t raise it sooner - and did some research into ways I can address those problems for our team. We use Cisco AnyConnect for VPN, and AnyConnect has two features that seemed to meet our needs:\nAnyConnect Start Before Login (SBL) In essence, this forces the user to authenticate to VPN before signing to their machine with domain credentials. This would mean that a VPN connection exists before the user sign in, which should address login script issues. AnyConnect Management VPN In this scenario, a VPN Management Tunnel would establish whenever the user/computer is disconnected from VPN. If the user reauthenticates, the Management Tunnel drops as the VPN connection is already present. Certificates are used for authentication. This would address our ability to patch machines that are powered on but not signed into VPN, and it sounds like it’d help with login scripts too. Here’s how Cisco describes the Management Tunnel feature in Configure AnyConnect Management VPN Tunnel on ASA:\nBackground Information A management VPN tunnel ensures connectivity to the corporate network whenever the client system is powered up, not just when a VPN connection is established by the end-user. You can perform patch management on out-of-the-office endpoints, especially devices that are infrequently connected by the user, via VPN, to the office network. Endpoint OS login scripts that require corporate network connectivity also benefits from this feature.\nAnyConnect Management Tunnel allows administrators to have AnyConnect connected without user intervention prior to the user log in. AnyConnect Management tunnel can work in conjunction with Trusted Network Detection and therefore is triggered only when the endpoint is off-premise and disconnected from User-initiated VPN. AnyConnect Management tunnel is transparent to the end-user and disconnects automatically when the user initiates VPN.\nWorking of Management Tunnel AnyConnect VPN agent service is automatically started upon system boot-up. It detects that the management tunnel feature is enabled (via the management VPN profile), therefore it launches the management client application to initiate a management tunnel connection. The management client application uses the host entry from the management VPN profile to initiate the connection. Then the VPN tunnel is established as usual, with one exception: no software update is performed during a management tunnel connection since the management tunnel is meant to be transparent to the user.\nThe user initiates a VPN tunnel via the AnyConnect UI, which triggers the management tunnel termination. Upon management tunnel termination, the user tunnel establishment continues as usual.\nThe user disconnects the VPN tunnel, which triggers the automatic re-establishment of the management tunnel.\nLimitations\nUser interaction is not supported. Certificate-based authentication through Machine Certificate Store (Windows) is only supported. Strict Server Certificate checking is enforced. Private Proxy is not supported. A public proxy is not supported (ProxyNative value is supported on platforms where Native Proxy settings are not retrieved from the browser). AnyConnect Customization Scripts are not supported. The biggest risk I see to deploying either of these is how it might be affected by our web filtering proxy.\nThere\u0026rsquo;s really only one way to find out how/if this will work, and that\u0026rsquo;s to set it up! I\u0026rsquo;ll do that in Cisco Management Tunnel - NDES Setup.\n","date":"2021-09-24T07:26:05-04:00","permalink":"https://gbeifuss.github.io/p/cisco-management-tunnel/","title":"Cisco Management Tunnel"},{"content":"My organization has a Microsoft Dynamics PowerApps-based website to allow a subset of our customers to manage their applications to us. It\u0026rsquo;s secured using HTTPS, as all good website should be. That means I need to update the certificate annually, and it\u0026rsquo;s a time-consuming process because we don\u0026rsquo;t actually own the domain that our PowerApps site is listening on. Here\u0026rsquo;s a high-level overview of what a renewal entails:\nGenerate a CSR Complete paperwork for our parent organization\u0026rsquo;s certificate management department: Information about the requested certificate type, my contact information, my manager Send this to our parent company for signoff from our representative Attach the CSR to the form and send it to our parent company\u0026rsquo;s certificate management department. I don\u0026rsquo;t know why our rep. cannot sign this after we\u0026rsquo;ve submitted it to the certificate management department. Get the new certificate Merge the CSR with the private KEY and create a PFX Upload the PFX to Netscaler Bind to the VSRV object Upload to PowerApps Bind to the proper portal Confirm that the certificate is now in use and working Honestly, getting the paperwork filled out is the hardest part of the whole process because it gets bounced around and needs to be escalated a certain way (politics). Documenting that hassle isn\u0026rsquo;t going to be useful for anyone else, so let\u0026rsquo;s focus on the technical parts.\nGenerate a CSR\rI\u0026rsquo;ve used the CertificateTools.com X509 Certificate Generator site in the past because it can handle SANs (Subject Alternative Name). This particular site needs 4 domains:\ncompany.com company.ca www.company.com www.company.ca I enter all of the required information and then a Certificate Request (CSR) is generated, along with the Private Key. This can also be done in openssl, if you\u0026rsquo;re worried about having the private key for your certificate generated by a server outside your control. Let\u0026rsquo;s walk through generating this with openssl! First, let\u0026rsquo;s generate the random file and set the config file:\nD:\\\u0026gt;set OPENSSL_CONF=d:\\apps\\OpenSSL\\bin\\openssl.cfg D:\\\u0026gt;set RANDFILE=.rnd Now, enable support for SAN in your request. Edit the config file d:\\apps\\OpenSSL\\bin\\openssl.cfg to add the requirement for req_ext, and add SAN/DNS names to the req_ext section.\n[ req_ext ] subjectAltName = DNS: www.company.com, DNS: company.com, DNS: www.company.ca Let\u0026rsquo;s generate the private key for this request:\nD:\\Apps\\OpenSSL\\bin\u0026gt;openssl genrsa -out rsa.private 2048 Generating RSA private key, 2048 bit long modulus (2 primes) ................................................................ ...........................................................+++++ ..........................................+++++ e is 65537 (0x010001) Then, we generate the CSR:\nD:\\Apps\\OpenSSL\\bin\u0026gt;openssl req -new -key rsa.private -out company.csr.txt You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [CA]: State or Province Name (full name) [Ontario]: Locality Name (eg, city) [Port Colbourne]: Organization Name (eg, company) [Company]: Organizational Unit Name (eg, section) [IT-CNS]: Common Name (e.g. server FQDN or YOUR name) []:company.com Email Address [helpdesk@company.com]: Please enter the following \u0026#39;extra\u0026#39; attributes to be sent with your certificate request A challenge password []: An optional company name []: D:\\Apps\\OpenSSL\\bin\u0026gt;openssl.exe req -noout -text -in company.csr.txt Certificate Request: Data: Version: 1 (0x0) Subject: C = CA, ST = Ontario, L = Port Colbourne, O = company, OU = IT-CNS, CN = company.com, emailAddress = helpdesk@company.com Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public-Key: (2048 bit) Modulus: 00:e8:cb:2a:71:99:b1:5f:17:fd:c7:db:7e:82:f9: 1a:a4:fc:7b:ae:d8:2b:1c:e6:06:96:8b:13:e8:77: 6d:28:ed:ba:5a:09:79:7b:58:32:7d:97:05:51:8c: eb:ea:63:c3:c8:11:15:ce:3c:b5:6d:e3:12:22:cb: 8e:1d:c9:0a:01:12:2c:1a:34:59:f1:fc:48:c5:32: 9a:5c:93:85:e8:75:33:c2:fc:b6:3a:37:71:85:9a: e7:b3:b4:f0:9c:86:b2:a1:d6:3f:49:c3:05:9c:ab: 1b:6b:94:f7:66:90:69:48:40:dc:cb:2e:f1:04:8a: b9:4b:da:7c:34:f1:a2:a9:9b:a5:4e:41:47:0a:f9: 63:ff:f3:23:c0:be:01:45:92:57:48:6b:27:40:e5: af:a0:fd:b1:80:5c:a4:6e:26:34:a1:1b:4e:10:c0: de:3b:0e:1d:0c:48:e6:4f:0c:3a:2b:92:9c:60:6a: 20:35:8a:b0:c1:af:81:ae:3f:fe:c6:90:25:7d:b6: 09:a6:9e:47:88:dc:68:72:fe:7f:2d:4d:0f:95:77: 00:bf:7f:a0:8e:aa:00:98:90:32:93:ae:da:54:52: 79:6d:6c:d0:4e:63:fb:93:e3:c8:a9:3f:17:29:33: 56:1c:3c:3c:0e:29:99:23:ff:44:ac:03:de:c9:eb: 16:73 Exponent: 65537 (0x10001) Attributes: Requested Extensions: X509v3 Subject Alternative Name: DNS:company.com, DNS:www.company.com, DNS:www.company.ca, DNS:company.ca Signature Algorithm: sha256WithRSAEncryption cd:39:8f:f7:9d:30:84:ab:0f:7e:60:a1:ee:bd:78:a8:ad:95: b0:cc:a4:df:64:8c:ec:94:9f:a1:79:36:ed:b9:74:00:8c:29: 9b:72:d8:4b:5a:24:0a:c5:65:36:6b:8a:0d:80:24:7b:6f:f7: 65:08:f9:60:6c:d8:58:3a:62:cd:df:e7:00:2a:7c:13:41:df: 51:e8:32:3a:c4:55:ca:2d:1d:fb:ed:f4:43:e4:55:0a:b4:f0: cc:3a:ca:6d:42:70:c9:03:f5:83:b4:be:6e:e5:ce:06:70:ea: 99:3c:59:cd:6e:75:ce:c2:c6:7b:20:32:62:3c:63:12:03:e3: 69:5d:ab:9c:9c:be:c1:4c:be:9e:98:63:a9:5a:b0:75:25:c3: 17:0a:47:e7:91:c4:10:01:5f:13:8f:7b:96:e1:cf:63:39:87: 8f:e4:2f:92:02:b6:68:9a:cb:d8:aa:28:8f:b4:e7:fe:4e:18: 83:cb:a8:9e:1e:46:72:bb:b6:f6:f1:9e:3a:b6:f9:f6:07:b6: 69:92:0e:18:35:d4:00:48:5a:71:21:9f:61:ae:69:4d:34:d2: dd:d7:24:1d:7f:e9:2b:cd:a4:9b:70:82:8f:ee:8e:1a:49:43: a3:54:b2:e7:78:4d:47:5e:f3:08:71:d7:07:a5:af:9e:89:e0: bb:50:1a:1f Merging the CER and KEY to create a PFX\rAfter sending off the CSR, I received back a certificate (CER) file from the certificate management department. This is the certificate I need, but it does not have the private key attached. If I were to import this, I\u0026rsquo;d get a notice saying that it\u0026rsquo;s missing the private key (\u0026ldquo;You do not have the private key for this certificate\u0026rdquo;). Let\u0026rsquo;s merge the KEY with the CER and generate a PFX, with everything nicely bundled together and protected by a password. Back to openssl:\nD:\\Apps\\OpenSSL\\bin\u0026gt;openssl pkcs12 -export -out company.pfx -inkey rsa.private -in ServerCertificate.crt Enter Export Password: Verifying - Enter Export Password: Now that I have the PFX, I can import into Netscaler and bind it to my VSRV.\nBind to PowerApps\rI also have to upload this to PowerApps and bind it to the proper portal:\nLaunch the PowerApps Portals Admin Centre in a browser Click \u0026ldquo;Manage SSL Certificates\u0026rdquo;. You\u0026rsquo;ll see a list of certificates that PowerApps knows about. Click \u0026ldquo;Add New\u0026rdquo;. Browse to the PFX you\u0026rsquo;d generated and upload it, along with the password. Verify your new certificate is now listed with the proper expiry date and SAN. Make a note of the Thumbprint! Click \u0026ldquo;Setup Custom Domains and SSL\u0026rdquo;. Under SSL Bindings, you\u0026rsquo;ll see the currently bound certificate. Delete the currently bound certificate Click \u0026ldquo;Add New\u0026rdquo; and add the certificate you\u0026rsquo;d previously uploaded. Verify the bound certificate\u0026rsquo;s Thumbprint matches what you\u0026rsquo;d previously recorded (above) Now use a tool like Qualys\u0026rsquo; SSL Server Test to make sure that the new certificate is actually in use.\n","date":"2021-09-23T08:43:28-04:00","permalink":"https://gbeifuss.github.io/p/installing-a-certificate-in-powerapps/","title":"Installing a certificate in PowerApps"},{"content":"I\u0026rsquo;ve already written about Remediating an Internal IP Disclosure with Netscaler, but I found out today that the route I\u0026rsquo;d chosen prevents Outlook from authenticating. Oops!\nTo summarize the earlier post, Exchange was leaking an internal IP and servername when an HTTP/1.0 packet was set without a host header to /autodiscover/autodiscover.xml. I initially tried to fix it by deleting the WWW-Authenticate and X-FEServer headers from the replies using Netscaler.\nHowever, while that prevents the information from being leaked, it also prevents Outlook from properly loading when the traffic flows through Netscaler. In our case, internal traffic was fine, but people who rely on Outlook and autodiscover outside of our network couldn\u0026rsquo;t log in. I tested and removing the X-FEServer header permitted Outlook to load normally; it was removing the WWW-Authenticate header that caused the problem.\nI tried out some alternate methods of blocking an internal IP in the WWW-Authenticate field (specifically, a reply containing basic realm=\u0026quot;10.10.10.10), but it seems like there\u0026rsquo;s important authentication using that field. I came up with another approach which seems to be working: Since this vulnerability is the result of a HTTP/1.0 request, rewrite HTTP/1.0 requests to HTTP/1.1. HTTP/1.1 requires the host header field, which means that IIS won\u0026rsquo;t happily disclose internal IPs in response to a missing host header. HTTP/1.1 has been a standard since 1997 and there should literally be no device in the past 20 years which does not support it.\nLet\u0026rsquo;s get to work. I\u0026rsquo;ll create a rewrite policy that looks for HTTP/1.0. If it\u0026rsquo;s found, replace it with HTTP/1.1.\nadd rewrite action RW_ACTION_HTTP1.0 replace HTTP.REQ.VERSION.MINOR \u0026#34;\\\u0026#34;1\\\u0026#34;\u0026#34; add rewrite policy RW_POLICY_HTTP1.0 \u0026#34;HTTP.REQ.VERSION.MINOR.EQ(0)\u0026#34; RW_ACTION_HTTP1.0 bind lb vserver VSRV-Exchange-OWA-External -policyName RW_POLICY_HTTP1.0 -priority 110 -gotoPriorityExpression END -type REQUEST Let\u0026rsquo;s test and make sure that the internal information is no longer returned:\nGET /autodiscover/autodiscover.xml HTTP/1.0 HTTP/1.1 400 Bad Request Content-Type: text/html; charset=us-ascii Server: Microsoft-HTTPAPI/2.0 Date: Tue, 21 Sep 2021 14:16:46 GMT Connection: close Content-Length: 334 Strict-Transport-Security: max-age=15552000 \u0026lt;!DOCTYPE HTML PUBLIC \u0026#34;-//W3C//DTD HTML 4.01//EN\u0026#34;\u0026#34;http://www.w3.org/TR/html4/strict.dtd\u0026#34;\u0026gt; \u0026lt;HTML\u0026gt;\u0026lt;HEAD\u0026gt;\u0026lt;TITLE\u0026gt;Bad Request\u0026lt;/TITLE\u0026gt; \u0026lt;META HTTP-EQUIV=\u0026#34;Content-Type\u0026#34; Content=\u0026#34;text/html; charset=us-ascii\u0026#34;\u0026gt;\u0026lt;/HEAD\u0026gt; \u0026lt;BODY\u0026gt;\u0026lt;h2\u0026gt;Bad Request - Invalid Hostname\u0026lt;/h2\u0026gt; \u0026lt;hr\u0026gt;\u0026lt;p\u0026gt;HTTP Error 400. The request hostname is invalid.\u0026lt;/p\u0026gt; \u0026lt;/BODY\u0026gt;\u0026lt;/HTML\u0026gt; closed Great! The request is rejected (404) because HTTP/1.1 requires a host header, and this rewritten HTTP/1.0 request doesn\u0026rsquo;t have one.\n","date":"2021-09-21T10:18:20-04:00","permalink":"https://gbeifuss.github.io/p/rewrite-http/1.0-requests-to-http/1.1-with-netscaler/","title":"Rewrite HTTP/1.0 requests to HTTP/1.1 with Netscaler"},{"content":"Our security administrator sent over a few security \u0026lsquo;vulnerabilities\u0026rsquo; that had been flagged during a recent audit. One of them was a finding that one of our websites was leaking an internal IP. All I was given to track this down was the output from the auditor\u0026rsquo;s Nessus scan:\nNessus was able to verify the issue with the following request : GET autodiscover/autodiscover.xml HTTP/1.0 Accept-Charset: iso-8859-1,utf-8;q=0.9,*;q=0.1 Accept-Language: en Connection: Close User-Agent: Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0) Pragma: no-cache AcceptL image/gif, image/x-xbitmap, image/jpeg, impage/pjpeg, image/png, */* Which returned the following IP address : 10.10.0.31 After some quick research, I learned that this is actually Microsoft Exchange Client Access Server Information Disclosure. That it was being termed a vulnerability was pretty generous use of the term by our auditors in the PCI Compliance Report. Knowing an internal IP isn\u0026rsquo;t actually going to open the door for an attacker, but it could be useful as part of the enumeration or in a wider attack, the internal IP address might be very useful.\nThis vulnerability is usually found in conjunction with Web Server HTTP Header Internal IP Disclosure, which returns the internal IP in the location field. However, a previous audit had found that and I\u0026rsquo;d remediated it. I wonder how come this first audit didn\u0026rsquo;t find the Microsoft Exchange Client Access Server Information Disclosure vulnerability? Hmm.\nIn both of these \u0026lsquo;vulnerabilities\u0026rsquo;, an attacker can send an IIS webserver a specially-crafted HTTP 1.0 GET request, without any host header set. This causes the server to return its internal IP address in the reply. The reason for this is that the HTTP 1.0 protocol does not require the host header to be set by the client in the request. The HTTP 1.1 protocol requires the client to specify a host in the header, so is not affected.\nOnce I sorted out what I was looking for, it was time to replicate the findings, so that I can self-test that the \u0026lsquo;vulnerability\u0026rsquo; has been remediated.\nI love the chocolatey package manager for Powershell, especially since Windows doesn\u0026rsquo;t ship with anything useful. (WinGet seems to be what Microsoft is positioning as their Package Manager, but it\u0026rsquo;s far behind chocolatey at this point). In any case, since I have chocolatey already installed, let\u0026rsquo;s use it to download and install the openssl package:\nchoco install openssl\nWow, that was pretty easy, right? Let\u0026rsquo;s get started by opening a connection to the Exchange server using SSL/TLS:\nC:\\Program Files\\OpenSSL-Win64\\bin\u0026gt;openssl s_client -host 206.98.14.28 -port 443 CONNECTED(00000130) Can\u0026#39;t use SSL_get_servername depth=1 C = GB, ST = Greater Manchester, L = Salford, O = Sectigo Limited, CN = Sectigo RSA Domain Validation Secure Server CA verify error:num=20:unable to get local issuer certificate verify return:1 depth=0 OU = Domain Control Validated, OU = PositiveSSL Multi-Domain, CN = webmail.company.com verify return:1 --- Certificate chain 0 s:OU = Domain Control Validated, OU = PositiveSSL Multi-Domain, CN = webmail.company.com i:C = GB, ST = Greater Manchester, L = Salford, O = Sectigo Limited, CN = Sectigo RSA Domain Validation Secure Server CA 1 s:C = GB, ST = Greater Manchester, L = Salford, O = Sectigo Limited, CN = Sectigo RSA Domain Validation Secure Server CA i:C = US, ST = New Jersey, L = Jersey City, O = The USERTRUST Network, CN = USERTrust RSA Certification Authority --- Server certificate -----BEGIN CERTIFICATE----- MIIGkzCCBXugAwIBAgIQeMhcbdyCJZhuKNK1eI9BnDANBgkqhkiG9w0BAQsFADCB jzELMAkGA1UEBhMCR0IxGzAZBgNVBAgTEkdyZWF0ZXIgTWFuY2hlc3RlcjEQMA4G A1UEBxMHU2FsZm9yZDEYMBYGA1UEChMPU2VjdGlnbyBMaW1pdGVkMTcwNQYDVQQD Ey5TZWN0aWdvIFJTQSBEb21haW4gVmFsaWRhdGlvbiBTZWN1cmUgU2VydmVyIENB MB4XDTE5MTEwNTAwMDAwMFoXDTIxMTEwNDIzNTk1OVowZTEhMB8GA1UECxMYRG9t YWluIENvbnRyb2wgVmFsaWRhdGVkMSEwHwYDVQQLExhQb3NpdGl2ZVNTTCBNdWx0 aS1Eb21haW4xHTAbBgNVBAMTFHdlYm1haWwuYWdyaWNvcnAuY29tMIIBIjANBgkq hkiG9w0BAQEFAAOCAQ8AM11BCgKCAQEAzkoRlNCs5fI4iiNvh2YcaV3y3KynScoj F7sk8frQY1iYBrQ4oLfDI2h+Np7a21kvdMyYufLzWeAz1n0EDu29/kkp8QBIVaDU CgCNdsocnvwC41E7Qyc9wumBRNWbz0c4B7tyDvhImgBfl6IIXUe+cy/KsrqWb3Tq mJKJ7acSBNzRcPgK8qUbUCdtNixhBRruWKjnQ7qwmzLwsRozFZGExUCms5/MX4Od uZ5UHWGYZed+w30vu86Hy1/jf0VWeDGSAsiPUySLq55fvwClyRftN1aS9bwROrvE sTJ1rAkR26wUFNyQi9ijkOEPCh2TqyclQNvRnokSI346ZMKKkaPpFwIDAQABo4ID EjCCAw4wHwYDVR0jBBgwFoAUjYxexFStiuF36Zv5mwXhuAGNYeEwHQYDVR0OBBYE FKb+bHu4DBzyNq5bUYs8Ppfr2EXFMA4GA1UdDwEB/wQEAwIFoDAMBgNVHRMBAf8E AjAAMB0GA1UdJQQWMBQGCCsGAQUFBwMBBggrBgEFBQcDAjBJBgNVHSAEQjBAMDQG CysGAQQBsjEBAgIHMCUwIwYIKwYBBQUHAgEWF2h0dHBzOi8vc2VjdGlnby5jb20v Q1BTMAgGBmeBDAECATCBhAYIKwYBBQUHAQEEeDB2ME8GCCsGAQUFBzAChkNodHRw Oi8vY3J0LnNlY3RpZ28uY29tL1NlY3RpZ29SU0FEb21haW5WYWxpZGF0aW9uU2Vj dXJlU2VydmVyQ0EuY3J0MCMGCCsGAQUFBzABhhdodHRwOi8vb2NzcC5zZWN0aWdv LmNvbTA6BgNVHREEMzAxghR3ZWJtYWlsLmFncmljb3JwLmNvbYIZYXV0b2Rpc2Nv dmVyLmFncmljb3JwLmNvbTCCAX8GCisGAQQB1nkCBAIEggFvBIIBawFpAHYAfT7y +I//iFVoJMLAyp5SiXkrxQ54CX8uapdomX4i8NcAAAFuPDgr2gAABAMARzBFAiAi 1bJ3H3IUyTUswT99xLn8P2xHsFjAm4FjcuqpbSdgxAIhAPZOftlPa1H1jQPQ1ESb IMox3lbDRSrPOOESq+oGMCPZAHcARJRlLrDuzq/EQAfYqP4owNrmgr7YyzG1P9Mz lrW2gagAAAFuPDgrzgAABAMASDBGAiEAgNOhZoOzgj8DwuR516eWmpms/RmaQ61x 5z8CAnfYOL0CIQD5ODhJYisT5s7oRmqXgEzCg84dWeysi5/DjYzWvlqzFQB2AFWB 1MIWkDYBSuoLm1c8U/DA5Dh4cCUIFy+jqh0HE9MMAAABbjw4K5wAAAQDAEcwRQIg adxDdH5iWyGh6tReXl/1t0zLEBcPwXe24St5WYnSwlcCIQC5RKhMAD63eyKpojIo 6fJocT/P6SBsfAykYhwThDFIPDANBgkqhkiG9w0BAQsFAAOCAQEAnUwWuOimEN5k zzXFVVDFdwRYmTdnW7fzOgXnQKVmtkqAXiL0Qp05OnUq7qPJuTaOR+KaGPS0nnLN b3/U3SimzH+tukkDZWcVch7tFzrEzpmWzighsQrzdCO0fv7SL3un7aKxQzf/u+se iCZ5F8W40+YmZq8mW7LUdYINbtR/vGgays/7xzjWioWVjbgNsWiFdpmZ/Fch1H9q m7UoFGNcTPDQrsg5rI2fKU/JBxWWqaUlWQ6DClhzwBp55DY3MGCoPesiiIU0HN/7 GyqwDIklkHOX6pRRmlXb5ZEK31Bo4qvwv/yolil83NL1q/Lk3C64jLmPEtH5rJy1 JaY6tu+MsA== -----END CERTIFICATE----- subject=OU = Domain Control Validated, OU = PositiveSSL Multi-Domain, CN = webmail.company.com issuer=C = GB, ST = Greater Manchester, L = Salford, O = Sectigo Limited, CN = Sectigo RSA Domain Validation Secure Server CA --- No client certificate CA names sent Peer signing digest: SHA256 Peer signature type: RSA-PSS Server Temp Key: ECDH, P-256, 256 bits --- SSL handshake has read 3885 bytes and written 683 bytes Verification error: unable to get local issuer certificate --- New, TLSv1.3, Cipher is TLS_AES_128_GCM_SHA256 Server public key is 2048 bit Secure Renegotiation IS NOT supported Compression: NONE Expansion: NONE No ALPN negotiated Early data was not sent Verify return code: 20 (unable to get local issuer certificate) --- At this point, openssl has successfully established the connection and is awaiting a case-sensitive command. Let\u0026rsquo;s send the HTTP 1.0 packet to the autodiscover URL. You may need to press ENTER a couple of times:\nGET /autodiscover/autodiscover.xml HTTP/1.0 HTTP/1.1 401 Unauthorized Server: Microsoft-IIS/10.0 request-id: e1722265-bc09-4d32-a25e-02bf98f7d8f9 X-SOAP-Enabled: True X-WSSecurity-Enabled: True X-WSSecurity-For: None X-OAuth-Enabled: True X-OWA-Version: 15.1.2308.14 WWW-Authenticate: Negotiate WWW-Authenticate: NTLM X-Powered-By: ASP.NET X-FEServer: EXCHANGE02 WWW-Authenticate: Basic realm=\u0026#34;10.1.0.31\u0026#34; Date: Fri, 17 Sep 2021 19:04:53 GMT Connection: keep-alive Content-Length: 0 Strict-Transport-Security: max-age=15552000 closed And there it is! The internal server name and IP, in black and white. Thanks for nothing, IIS.\nAlright, let\u0026rsquo;s go about fixing this. Instead of fixing this in IIS with the URL-Rewrite module, I decided to use Netscaler.\nThe same change is made on just 2 Netscaler devices, vs. 5 Exchange servers The changes are not dependant on IIS, which may possibly be reset by patches or other installed software Netscaler is where I generally do this sort of thing (consistency leads to ease of administration) In any case, this is what Netscaler will do:\nAny replies from the Webmail object are inspected for the WWW-Authenticate header. If it’s found, the WWW-Authenticate header is deleted as well as any X-FEServer header. I accomplished this with a couple of Responder Policies in Netscaler:\nadd rewrite action RW_ACTION_OWA_FEServer delete_http_header X-FEServer add rewrite action RW_ACTION_OWA_WWWAuthenticate delete_http_header WWW-Authenticate add rewrite policy RW_POLICY_OWA_FEServer \u0026#34;HTTP.RES.HEADER(\\\u0026#34;X-FEServer\\\u0026#34;).EXISTS\u0026#34; RW_ACTION_OWA_FEServer NOREWRITE add rewrite policy RW_POLICY_OWA_WWWAuthenticate \u0026#34;HTTP.RES.HEADER(\\\u0026#34;WWW-Authenticate\\\u0026#34;).EXISTS\u0026#34; RW_ACTION_OWA_WWWAuthenticate NOREWRITE bind lb vserver VSRV-Exchange-OWA-External -policyName RW_POLICY_OWA_WWWAuthenticate -priority 100 -gotoPriorityExpression NEXT -type RESPONSE bind lb vserver VSRV-Exchange-OWA-External -policyName RW_POLICY_OWA_FEServer -priority 110 -gotoPriorityExpression END -type RESPONSE Let\u0026rsquo;s repeat the test and make sure that the internal information is no longer returned:\nget /autodiscover/autodiscover.xml HTTP/1.0 HTTP/1.1 401 Unauthorized Server: Microsoft-IIS/10.0 request-id: 068763fb-67ff-4f09-8ca9-02139957f4c2 X-SOAP-Enabled: True X-WSSecurity-Enabled: True X-WSSecurity-For: None X-OAuth-Enabled: True X-OWA-Version: 15.1.2308.14 X-Powered-By: ASP.NET Date: Fri, 17 Sep 2021 19:16:43 GMT Connection: keep-alive Content-Length: 0 Strict-Transport-Security: max-age=15552000 closed ","date":"2021-09-17T15:49:45-04:00","permalink":"https://gbeifuss.github.io/p/remediating-an-internal-ip-disclosure-with-netscaler/","title":"Remediating an Internal IP Disclosure with Netscaler"},{"content":"CTRL-ALT-L enables multiple cursors in Visual Studio Code.\nThat is all.\n(Technically CTRL-ALT-L is the keyboard shortcut for \u0026lsquo;Select all occurrences of current selection\u0026rsquo;. Select a word, press CTRL-ALT-L and a cursor will appear to the left of every instance of that word. It\u0026rsquo;s a useful alternative to Find/Replace.)\n","date":"2021-09-13T23:35:20-04:00","permalink":"https://gbeifuss.github.io/p/using-multicursor-in-vs-code/","title":"Using multicursor in VS Code"},{"content":"I tried generating updated content for my website the other day and Hugo threw an error instead:\nhugo v0.88.1-5BC54738+extended windows/amd64 BuildDate=2021-09-04T09:39:19Z VendorInfo=gohugoio ERROR 2021/09/10 14:59:54 [en] REF_NOT_FOUND: Ref \u0026#34;exchange-clustergroup.md\u0026#34;: \u0026#34;C:\\myblog.dev.repo\\content\\post\\exchange-transaction-logs.md:15:458\u0026#34;: page not found I went back and triplechecked my code: the formatting of the ref block was proper and had no typographical errors.\nI finally realized that my Front Matter tag draft: on the destination page was set to true. This meant that when Hugo was building the code, the destination page wasn\u0026rsquo;t being generated, which meant that there was no destination page for this reference to link to.\nChanging the Front Matter to draft: false allowed Hugo to generate the content properly.\n","date":"2021-09-13T23:35:09-04:00","permalink":"https://gbeifuss.github.io/p/hugo-error-ref_not_found/","title":"Hugo error - REF_NOT_FOUND"},{"content":"I solved a problem today that\u0026rsquo;s been plaguing me for a two weeks now. Exchange transaction logs are a record of every single operation that changes the state of any data in the database. Adding a new item, deleting an old item, modifying an existing item - all of these are recorded in a transaction log at the same time they\u0026rsquo;re applied to the database itself. When an Exchange database is mounted, the transaction logs are reprocessed to make sure there aren\u0026rsquo;t any discrepancies or database errors. If there are, the transaction logs are used to rebuild the database state. This same process applied to the DAG as well. The entire purpose of transaction logs in Exchange is to provide information on the transactions that occurred since the last time you ran a complete backup of your Exchange environment.\nIf you\u0026rsquo;re using Exchange-aware backup software, Exchange will \u0026lsquo;know\u0026rsquo; when a full backup occurs and then it\u0026rsquo;ll delete all transaction logs prior to that point. They\u0026rsquo;re no longer needed for a database rebuild because there\u0026rsquo;s a backup.\nThe problem I ran into is that the transaction logs were not being purged despite a successful backup (by our Exchange-aware software) and filled up the volume, which caused a database dismount\u0026hellip; which led to some unhappy employees. Fortunately, it was just 1 of our 6 databases and I was able to fix it within 15 minutes, so not many people even noticed an issue. I wrote about workarounds and solutions for that in Exchange DAGs and Cluster Ownership. As I mentioned in that post, I believed that the root cause was that the Owner of our DAG cluster objects wasn\u0026rsquo;t the proper host. However, this wasn\u0026rsquo;t the case because the transaction logs continued to grow!\nThe Transaction Logs for Journaling are causing the problem. Despite no old logs (they\u0026rsquo;re all timestamped AFTER our daily backup occurs, which tells me that the backup is working properly and Exchange is aware of it), they\u0026rsquo;re consuming 10+ GB more disk space each passing day. Yesterday morning they occupied 60GB, and 24 hours later they\u0026rsquo;re already consuming 84GB.\nI checked the Queues and found dozens of messages in the Journaling queue:\n[PS] C:\\Windows\\system32\u0026gt;Get-QueueDigest -Dag Exchange2016 | ft -a GroupByValue MessageCount DeferredMessageCount LockedMessageCount StaleMessageCount Details ------------ ------------ -------------------- ------------------ ----------------- ------- journaling 85 85 0 0 {Exchange06\\4, EXCHANGE04\\4, Exchange05\\5} [PS] C:\\Windows\\system32\u0026gt;get-queue -Identity Exchange04\\4 Identity DeliveryType Status MessageCount Velocity RiskLevel OutboundIPPool NextHopDomain -------- ------------ ------ ------------ -------- --------- -------------- ------------- EXCHANGE04\\4 SmtpDeliveryToMailbox Ready 29 0 Normal 0 journaling I took a look at one of these messages via. the Exchange Toolbox\\Queue Viewer GUI. This message has been stuck in the Journaling queue for almost 30 days (since August 13/21):\nIdentity: EXCHANGE04\\4\\75801877807477 Subject: Fw:\u0026#39;RE: 137396- Ray \u0026amp; Rachel Application\u0026#39;. Internet Message ID: \u0026lt;59d5803a-273b-4123-a41f-dfb67fc6cf1e@journal.report.generator\u0026gt; From Address: \u0026lt;\u0026gt; Status: Retry Size (KB): 11170 Message Source Name: Journaling Source IP: 255.255.255.255 SCL: 0 Date Received: 8/13/2021 10:48:12 AM Expiration Time: Last Error: 400 4.4.7 The server responded with: 554 5.2.0 STOREDRV.Deliver.Exception:MessageSubmissionExceededException.MapiExceptionMaxSubmissionExceeded; Failed to process message due to a permanent exception with message Cannot save changes made to an item to store. 16.55847:09020000, 17.43559:0000000090000000000000000100000000000000, 20.52176:010F04890E00000000000000, 20.50032:010F04897E17103100000000, 0.35180:010F0489, 255.23226:00000000, 255.27962:0E000000, 255.31418:21000000, 16.55847:E3000000, 17.43559:0000000070010000000000000000000000000000, 20.52176:010F04890E00001080030400, 20.50032:010F04897E17F01F010F0489, 0.35180:2C000000, 255.23226:2C000000, 255.27962:0A000000, 255.27962:0C000000, 255.17082:DA040000, 0.18273:00000000, 4.21921:DA040000, 255.27962:FA000000, 255.1494:010F0489, 5.59176:0000A0004C696D69746174696F6E001005000780, 5.34600:A418B00043757272656E7453697A65000F010480, 4.42792:DA040000, 7.40748:000000000000000031343163, 7.57132:000000000000000005000780, 1.63016:0C000000, 4.39640:DA040000, 8.45434:7EFF3D197915D443BAF1677741440BB605000780, 5.10786:0000000031352E30312E323330382E3031343A45786368616E676530353A36623134316338652D663135382D346465612D616564652D3837656131656238336534310080, 255.1750:2C000000, 255.31418:2C000000, 0.21457:010F0489, 4.19665:DA040000, 0.37632:DA040000, 4.37888:DA040000 [Stage: CreateMessage]. The failure was replaced by a retry response because the message was marked for retry if rejected. Queue ID: EXCHANGE04\\4 Recipients: Tier3M.Journaling@Company.com;3;3;[{LED=400 4.4.7 The server responded with: 554 5.2.0 STOREDRV.Deliver.Exception:MessageSubmissionExceededException.MapiExceptionMaxSubmissionExceeded; Failed to process message due to a permanent exception with message Cannot save changes made to an item to store. 16.55847:09020000, 17.43559:0000000090000000000000000100000000000000, 20.52176:010F04890E00000000000000, 20.50032:010F04897E17103100000000, 0.35180:010F0489, 255.23226:00000000, 255.27962:0E000000, 255.31418:21000000, 16.55847:E3000000, 17.43559:0000000070010000000000000000000000000000, 20.52176:010F04890E00001080030400, 20.50032:010F04897E17F01F010F0489, 0.35180:2C000000, 255.23226:2C000000, 255.27962:0A000000, 255.27962:0C000000, 255.17082:DA040000, 0.18273:00000000, 4.21921:DA040000, 255.27962:FA000000, 255.1494:010F0489, 5.59176:0000A0004C696D69746174696F6E001005000780, 5.34600:A418B00043757272656E7453697A65000F010480, 4.42792:DA040000, 7.40748:000000000000000031343163, 7.57132:000000000000000005000780, 1.63016:0C000000, 4.39640:DA040000, 8.45434:7EFF3D197915D443BAF1677741440BB605000780, 5.10786:0000000031352E30312E323330382E3031343A45786368616E676530353A36623134316338652D663135382D346465612D616564652D3837656131656238336534310080, 255.1750:2C000000, 255.31418:2C000000, 0.21457:010F0489, 4.19665:DA040000, 0.37632:DA040000, 4.37888:DA040000 [Stage: CreateMessage]. The failure was replaced by a retry response because the message was marked for retry if rejected.};{MSG=};{FQDN=};{IP=};{LRT=}];0;CN=Journaling,CN=Databases,CN=Exchange Administrative Group (FYDIBOHF23SPDLT),CN=Administrative Groups,CN=Company,CN=Microsoft Exchange,CN=Services,CN=Configuration,DC=corp,DC=Company,DC=com;0 The error is MessageSubmissionExceededException, which made me suspect a size limit problem. Indeed, all 28 messages in the Journaling queue have a size \u0026gt; 10MB. Our standard maximum message size is 10GB across the organization (to discourage people from using email as a file transfer/file storage mechanism), but last month an exception was granted for one of our customer service mailboxes. That mailbox has a maximum message size of 30MB.\nI realized that this current problem is probably stemming from the changes made several weeks ago to allow large messages to this mailbox - they arrive there, but the Journaling process cannot handle them because the Journal mailbox limit is still 10MB. The Journaling process continually tries to add them to the Journaling mailbox which causes the retry process to generate a huge volume of transaction logs. I checked the mailbox message limits to be sure the sizes matched my hypothesis:\n[PS] C:\\Windows\\system32\u0026gt;get-mailbox contact | select name,*size* Name MaxSendSize MaxReceiveSize ---- ----------- -------------- contact 30 MB (31,457,280 bytes) 30 MB (31,457,280 bytes) [PS] C:\\Windows\\system32\u0026gt;get-mailbox Tier* | select name,*size* Name MaxSendSize MaxReceiveSize ---- ----------- -------------- Tier3A Journaling 10 MB (10,485,760 bytes) 10 MB (10,485,760 bytes) Tier3B Journaling 10 MB (10,485,760 bytes) 10 MB (10,485,760 bytes) Tier3C Journaling 10 MB (10,485,760 bytes) 10 MB (10,485,760 bytes) Tier3D Journaling 10 MB (10,485,760 bytes) 10 MB (10,485,760 bytes) Tier3L Journaling 10 MB (10,485,760 bytes) 10 MB (10,485,760 bytes) Tier3M Journaling 10 MB (10,485,760 bytes) 10 MB (10,485,760 bytes) I increased the max message size that Journaling mailboxes can handle:\n[PS] C:\\Windows\\system32\u0026gt;get-mailbox Tier* | Set-mailbox -MaxReceiveSize 30MB -MaxSendSize 30MB This flushed the 28 queued messages within 15 minutes.\nI then ran a pair of consecutive diskshadow backups on Exchange05, which is the server that has mounted the Journaling database. This cleared out the logs on disk within 30 minutes. Instead of taking 84GB, the logs now consume a much-more-reasonable 2GB.\n","date":"2021-09-09T09:29:17-04:00","permalink":"https://gbeifuss.github.io/p/exchange-transaction-logs-wont-clear/","title":"Exchange Transaction Logs won't clear"},{"content":"We run Exchange on a multiserver DAG, which behaves similarly to a cluster. It\u0026rsquo;s built on the same principles as a cluster, but it\u0026rsquo;s technically not a cluster and is administered differently than a cluster. It\u0026rsquo;s \u0026hellip; clusterish.\nOne of the issues that we run in to occasionally is that Cluster Owner changes to another DAG member server, which causes problems with our backup software. (The backup software can\u0026rsquo;t truely handle a cluster and wants to connect to the IP of a single server). The fix is pretty simple - change the owner back to the server it should be! Powershell makes this really easy:\nPS H:\\\u0026gt; Get-ClusterGroup | Move-ClusterGroup -Node Exchange05 Name OwnerNode State ---- --------- ----- Available Storage EXCHANGE04 Offline Cluster Group Exchange03 Offline PS H:\\\u0026gt; Get-ClusterGroup Name OwnerNode State ---- --------- ----- Available Storage Exchange05 Offline Cluster Group Exchange05 Online To prevent any complications with our backups, I would like:\nThe Owner Node to be monitored The Owner Node to be corrected to the proper values, if it\u0026rsquo;s not on Exchange05. We run PRTG for server monitoring\u0026hellip;. Fortunately, PRTG can do this for us! The (old) article Monitoring Active Cluster Nodes With PRTG outlines the process. I only used this as a guide, as I ended up sourcing Jannos-443\u0026rsquo;s PRTG-MSCluster-PrefNodes.ps1 script.\nI started by checking the Preferred Node setting for these two cluster objects:\n[PS] C:\\Windows\\system32\u0026gt;Get-ClusterGroup | Get-ClusterOwnerNode ClusterObject OwnerNodes ------------- ---------- Available Storage {} Cluster Group {} Just peachy. Neither object has a preferred owner. Let\u0026rsquo;s fix that:\n[PS] C:\\Windows\\system32\u0026gt;Get-ClusterGroup | set-ClusterOwnerNode Exchange05.corp.agricorp.com [PS] C:\\Windows\\system32\u0026gt;Get-ClusterGroup | Get-ClusterOwnerNode ClusterObject OwnerNodes ------------- ---------- Available Storage {Exchange05} Cluster Group {Exchange05} That\u0026rsquo;s better. With that out of the way, I created a new PRTG sensor to monitor that the Cluster was Owned by the Preferred Node:\nSave the Cluster-checking script to ${env:ProgramFiles(x86)}\\PRTG Network Monitor\\Custom Sensors\\EXEXML on the PRTG probe server. Open PRTG and browsed to the Exchange05 server object. This is the server that should be the Owner Create a new sensor. Select a sensor type \u0026ldquo;EXE/Script Advanced Sensor\u0026rdquo;. Select PRTG-MSCluster-PrefNodes.ps1 from the dropdown list. Provide the Exchange Cluster name as a parameter. In my case, I entered Exchange2016. Set the Security Context of the sensor to Use Windows credentials of parent device Save changes. Query the sensor a couple of times to make sure that it\u0026rsquo;s working properly. When one or more nodes isn\u0026rsquo;t on the Preferred Node, then the sensor will change to \u0026lsquo;Warning\u0026rsquo;. Next, I want PRTG\u0026rsquo;s Notification action to correct the Owner of the cluster object. We\u0026rsquo;ll create a custom notification that will run a short Powershell script I wrote:\n\u0026lt;# .SYNOPSIS Force Cluster Owner to a specific server .DESCRIPTION https://kb.paessler.com/en/topic/40713-can-i-automatically-restart-a-windows-service-with-prtg Copy this script to the main PRTG server\u0026#39;s Notifications\\EXE scripts folder (${env:ProgramFiles(x86)}\\PRTG Network Monitor\\Notifications\\EXE) and create a new Notification Template \u0026gt; Execute Program. Choose this script from the dropdown and provide the SERVERNAME as a parameter. The Credentials need to be generated under the instance that PRTG is running as (NT Authority\\System). These domain creds allow PRTG to remotely run code on the remote system and manipulate the Cluster Groups. .PARAMETER Server Server FQDN or NetBIOS Name .EXAMPLE Sample call from PRTG EXE/Script Advanced PRTG-MSCluster-MoveClusterGroup.ps1 EXCHANGE01 Author: Greg Beifuss 7:48 AM 9/8/2021 #\u0026gt; param( [string]$Server = $null ) $id = Import-Clixml \u0026#34;C:\\Program Files (x86)\\PRTG Network Monitor\\Notifications\\EXE\\powershell.xml\u0026#34; $Scriptblock = { Get-ClusterGroup | Move-ClusterGroup -Node $env:COMPUTERNAME } Invoke-Command -ComputerName $Server -ScriptBlock $Scriptblock -Credential $id Use Export-CLIXML to store credentials that can log into the destination server (in my case, Exchange05) Save this script to ${env:ProgramFiles(x86)}\\PRTG Network Monitor\\Notifications\\EXE)` on the main PRTG server. This is the server that Notifications run from - the other PRTG servers simply won\u0026rsquo;t have the Notifications directory. Create a new Notification Template in PRTG. Choose \u0026lsquo;Execute Program\u0026rsquo;, and select the PRTG-MSCluster-MoveClusterGroup.ps1 script. Provide a cluster member server name as the parameter. This will be passed into the script. Save. Browse to the PRTG sensor that was previously created. Select \u0026lsquo;Notification Triggers\u0026rsquo;. Enable a State Trigger that will perform the Notification Template you just created. I used: When sensor state is Warning for at least 60 seconds, perform PRTG-MSCluster-MoveClusterGroup Save \u0026amp; test. You can see below that at 7:49ish, both the cluster object owners changed from the preferred nodes. I know this since PRTG detected this as \u0026ldquo;2\u0026rdquo;, not \u0026ldquo;1\u0026rdquo;. In any case, after the sensor had been in this state for 60 seconds, the notification task started and moved the cluser objects back to their proper \u0026lsquo;home\u0026rsquo;. The sensor value reset to \u0026ldquo;0\u0026rdquo; clusters on a non-preferred node.\n","date":"2021-09-06T22:12:49-04:00","permalink":"https://gbeifuss.github.io/p/exchange-dags-and-cluster-ownership/","title":"Exchange DAGs and Cluster Ownership"},{"content":"I received an alert today that the disk drive with our Exchange transaction logs was low on disk space. How low? 160KB free of 190GB! In fact, it was so low that Exchange dismounted the database on that server and moved it over to another host in the DAG.\nSomething like this is usually the result of backup problems. Exchange won\u0026rsquo;t purge transaction logs until it knows that they\u0026rsquo;ve been successfully backed up, since a successful restore of backup data requires a full backup plus all incremental transaction logs since then.\nOf course, there\u0026rsquo;s a nice little trick to tell Exchange that a full backup was just performed\u0026hellip; without actually performing a full backup. That lets Exchange start purging the transaction logs, which frees up disk space, which means Exchange can remount your database(s). It\u0026rsquo;s all thanks to the magic of VSS and DiskShadow.exe.\nThis needs to be done in an elevated command prompt on the server where the databases are mounted. If your databases and transaction logs are on different volumes, you\u0026rsquo;ll need to add both of them. In my case, Transaction Logs are on D:\\ and Databases are on E:\\ .\nAt a high level:\nLoad diskshadow add volumes begin backup create end backup exit C:\\Windows\\system32\u0026gt;diskshadow Microsoft DiskShadow version 1.0 Copyright (C) 2013 Microsoft Corporation On computer: EXCHANGE04, 9/3/2021 2:00:14 PM DISKSHADOW\u0026gt; add volume d: DISKSHADOW\u0026gt; add volume e: DISKSHADOW\u0026gt; begin backup DISKSHADOW\u0026gt; create Alias VSS_SHADOW_1 for shadow ID {b7d4f4f0-8293-4dbd-8bae-3901023e3e6b} set as environment variable. Alias VSS_SHADOW_2 for shadow ID {c994095e-dbdb-4950-b285-cf73b3e5ca1a} set as environment variable. Alias VSS_SHADOW_SET for shadow set ID {a0f96cbf-f4f2-40d9-9453-e84e6986c932} set as environment variable. Querying all shadow copies with the shadow copy set ID {a0f96cbf-f4f2-40d9-9453-e84e6986c932} * Shadow copy ID = {b7d4f4f0-8293-4dbd-8bae-3901023e3e6b} %VSS_SHADOW_1% - Shadow copy set: {a0f96cbf-f4f2-40d9-9453-e84e6986c932} %VSS_SHADOW_SET% - Original count of shadow copies = 2 - Original volume name: \\\\?\\Volume{c19abc70-413e-4ea7-b4d4-1ed69c382d6f}\\ [D:\\] - Creation time: 9/3/2021 3:27:13 PM - Shadow copy device name: \\\\?\\GLOBALROOT\\Device\\HarddiskVolumeShadowCopy63 - Originating machine: EXCHANGE04.company.com - Service machine: EXCHANGE04.company.com - Not exposed - Provider ID: {b5946137-7b9f-4925-af80-51abd60b20d5} - Attributes: Auto_Release Differential * Shadow copy ID = {c994095e-dbdb-4950-b285-cf73b3e5ca1a} %VSS_SHADOW_2% - Shadow copy set: {a0f96cbf-f4f2-40d9-9453-e84e6986c932} %VSS_SHADOW_SET% - Original count of shadow copies = 2 - Original volume name: \\\\?\\Volume{64abb4bb-d15b-42f9-893f-224c5a2f25a5}\\ [E:\\] - Creation time: 9/3/2021 3:27:13 PM - Shadow copy device name: \\\\?\\GLOBALROOT\\Device\\HarddiskVolumeShadowCopy64 - Originating machine: EXCHANGE04.company.com - Service machine: EXCHANGE04.company.com - Not exposed - Provider ID: {b5946137-7b9f-4925-af80-51abd60b20d5} - Attributes: Auto_Release Differential Number of shadow copies listed: 2 DISKSHADOW\u0026gt; end backup DISKSHADOW\u0026gt; exit C:\\Windows\\system32\u0026gt; The EMC/Powershell can display the last backup that Exchange is aware of:\n[PS] C:\\Windows\\system32\u0026gt;Get-MailboxDatabase -status | select Name, LastFullBackup | ft Name LastFullBackup ---- -------------- Journaling 9/5/2021 5:35:06 PM Tier3-C 9/5/2021 5:35:07 PM Tier3-D 9/5/2021 5:35:06 PM Tier3-A 9/6/2021 10:04:43 AM Tier3-M 9/6/2021 10:04:43 AM Tier3-X 9/6/2021 10:04:43 AM Tier3-L 9/5/2021 5:35:06 PM Tier3-B 9/5/2021 5:34:48 PM An alternate trick is to delete any unmounted database copies on the server that\u0026rsquo;s out of Transaction Log space through proper Exchange methods (EAC or Powershell). Once that\u0026rsquo;s done, delete the Transaction Logs belonging to that database. It should free up a small amount of space - perhaps enough breathing room to get a database online, users up and running again, and time for you to take proper measures.\nIf you\u0026rsquo;re running Exchange as a VM or the disks are on a SAN, you could also provision more disk space, then extend the volume.\nThe DiskShadow/VSS method should result in the following entries in the Application Event Log. These are from Windows Server 2016:\nEvent ID Source Exchange VSS Writer metadata preparation 2021 MSExchangeRepl Exchange VSS Writer database preparation 2110 MSExchangeRepl Exchange VSS Writer prepared for backup 2023 MSExchangeRepl A Full Shadow copy was started 2005 ESE Backup starting 960 ESE BACKUP Shadow copy freeze started 2001 ESE Exchange VSS Writer freezes databases 2027 MSExchangeRepl Shadow copy freeze ended 2003 ESE Exchage VSS Writer freeze ended 2029 MSExchangeRepl VSS Writer backup success 2046 MSExchangeRepl Shadow copy success 2006 ESE Exchange VSS Writer backup completion 2033 MSExchangeRepl Exchange VSS Writer backup ended 2037 MSExchangeRepl Exchange VSS Writer post-backup processing 2035 MSExchangeRepl Tombstone table cleanup 40013 MSExchangeIS Tombstone table cleanup complete 40017 MSExchange IS You may also see events for:\nExchange VSS Writer preparation (Event ID 9811, MSExchangeIS) Logs are now purged (Event ID 224, ESE) Backup is now complete (Event ID 9780, MSExchangeIS) Log Name: Application Source: MSExchangeRepl Date: 9/6/2021 10:03:28 AM Event ID: 2021 Task Category: Exchange VSS Writer Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The Microsoft Exchange VSS Writer has successfully collected the metadata document in preparation for backup. Log Name: Application Source: MSExchangeRepl Date: 9/6/2021 10:04:40 AM Event ID: 2110 Task Category: Exchange VSS Writer Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The Microsoft Exchange VSS Writer instance 02f75633-f3a1-4ac2-8e6e-5fb60f168e42 has successfully prepared for a full or a copy backup of database \u0026#39;Tier3-L\u0026#39;. The following database will be backed up: Tier3-L. Log Name: Application Source: MSExchangeRepl Date: 9/6/2021 9:26:33 AM Event ID: 2023 Task Category: Exchange VSS Writer Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The Microsoft Exchange Replication service VSS Writer (Instance 39cca171-a3ed-43e5-85d4-4c66dfae6e21) successfully prepared for backup. Log Name: Application Source: ESE Date: 9/6/2021 9:26:35 AM Event ID: 2005 Task Category: ShadowCopy Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: Information Store - Journaling (23412,G,0,15.01.2308.014) Shadow copy instance 1 starting. This will be a Full shadow copy. For more information, click http://www.microsoft.com/contentredirect.asp. Log Name: Application Source: ESE BACKUP Date: 9/6/2021 9:26:40 AM Event ID: 960 Task Category: General Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: msexchangerepl (5596) This computer is performing a surrogate backup. The master server is Exchange04. Log Name: Application Source: ESE Date: 9/6/2021 9:26:40 AM Event ID: 2001 Task Category: ShadowCopy Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: Information Store - Tier3-D (37104,D,0,15.01.2308.014) Tier3-D: Shadow copy instance 1 freeze started. For more information, click http://www.microsoft.com/contentredirect.asp. Log Name: Application Source: MSExchangeRepl Date: 9/6/2021 9:26:40 AM Event ID: 2027 Task Category: Exchange VSS Writer Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The Microsoft Exchange VSS Writer instance 39cca171-a3ed-43e5-85d4-4c66dfae6e21 has successfully frozen the databases. Log Name: Application Source: ESE Date: 9/6/2021 9:26:42 AM Event ID: 2003 Task Category: ShadowCopy Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: Information Store - Tier3-C (8036,G,0,15.01.2308.014) Shadow copy instance 1 freeze ended. For more information, click http://www.microsoft.com/contentredirect.asp. Log Name: Application Source: MSExchangeRepl Date: 9/6/2021 9:26:42 AM Event ID: 2029 Task Category: Exchange VSS Writer Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The Microsoft Exchange VSS Writer instance 39cca171-a3ed-43e5-85d4-4c66dfae6e21 has successfully thawed the databases. Log Name: Application Source: MSExchangeRepl Date: 9/6/2021 10:09:08 AM Event ID: 2046 Task Category: Exchange VSS Writer Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The Microsoft Exchange Replication service VSS Writer instance 02f75633-f3a1-4ac2-8e6e-5fb60f168e42 has successfully completed the backup of database \u0026#39;Tier3-A\u0026#39;. Database log truncation has been requested for this database. Log truncation will occur on the active copy after the next log generation is created. Log truncation will occur automatically on the passive copies after that log file is copied. Log Name: Application Source: ESE Date: 9/6/2021 10:09:08 AM Event ID: 2006 Task Category: ShadowCopy Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: Information Store - Tier3-A (35276,G,0,15.01.2308.014) Shadow copy instance 1 completed successfully. For more information, click http://www.microsoft.com/contentredirect.asp. Log Name: Application Source: MSExchangeRepl Date: 9/6/2021 10:09:08 AM Event ID: 2033 Task Category: Exchange VSS Writer Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The Microsoft Exchange Replication service VSS Writer (Instance 02f75633-f3a1-4ac2-8e6e-5fb60f168e42) has successfully processed the backup completion event. Log Name: Application Source: MSExchangeRepl Date: 9/6/2021 10:09:08 AM Event ID: 2037 Task Category: Exchange VSS Writer Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The Microsoft Exchange Replication service VSS Writer (Instance 02f75633-f3a1-4ac2-8e6e-5fb60f168e42) backup has been successfully shut down. Log Name: Application Source: MSExchangeRepl Date: 9/6/2021 9:26:42 AM Event ID: 2035 Task Category: Exchange VSS Writer Level: Information Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The Microsoft Exchange Replication service VSS Writer (Instance 39cca171-a3ed-43e5-85d4-4c66dfae6e21) has successfully processed the post-snapshot event. Log Name: Application Source: MSExchangeIS Date: 9/6/2021 9:36:15 AM Event ID: 40013 Task Category: Logical Data Model Level: Warning Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The tombstone table has reached an excessive number of entries and/or total size. A maintenance task has been dispatched to perform urgent cleanup. Database: Journaling (6b141c8e-f158-4dea-aede-87ea1eb83e41) Number of entries (estimated): 1453 Total size of entries (estimated): 5369912232 bytes Log Name: Application Source: MSExchangeIS Date: 9/6/2021 9:36:26 AM Event ID: 40017 Task Category: Logical Data Model Level: Warning Keywords: Classic User: N/A Computer: Exchange04.company.com Description: The urgent tombstone table cleanup task has finished executing. Database: Journaling (6b141c8e-f158-4dea-aede-87ea1eb83e41) Message tombstones deleted: 134 Subobject tombstones deleted: 1315 Total size of deleted entries: 5358535877 bytes Remaining number of entries (estimated): 4 Total size of remaining entries (estimated): 11376355 bytes Elapsed time: 11.1328017 seconds Pass completed: True Subobjects in-use: 4 Mailboxes quarantined: 0 Mailboxes locked: 0 Mailboxes should stop maintenance: 0 ","date":"2021-09-03T21:40:09-04:00","permalink":"https://gbeifuss.github.io/p/truncate-exchange-transaction-logs/","title":"Truncate Exchange Transaction Logs"},{"content":"There are special shortcodes that tell Hugo to generate the proper link(s) to other pages in your site, or other sections on the current page.\nI originally learned this when I wanted to include a link to another post I\u0026rsquo;d written. I used a reference shortcode (ref) for this:\n[Querying PowerPlatform API via. Powershell]( {{\u0026lt; ref \u0026quot;powershell-powerplatform-api.md\u0026quot; \u0026gt;}})\nLinks to other artifacts in your site can be done with ref (reference) or relref (relative reference). The only difference between them is whether the resulting URL is absolute (http://site.com/about/) or relative (/about/). The ref and relref shortcodes require a single parameter: the path to a content document, with or without a file extension, with or without an anchor. Hugo will try to match the parameter to content in your site. The relref shortcode is handy when you have two pages with the same name, but in different locations, so that Hugo knows which one you want to refer to.\nLinks to a heading on the current page, or a completely difference page, can be specified with an anchor, which are specified with a pound/hashtag symbol (#).\nLink to a heading on the current page: {{\u0026lt; ref \u0026quot;#reference\u0026quot; \u0026gt;}}\nLink to a heading on a different page: {{\u0026lt; ref \u0026quot;document.md#reference\u0026quot; \u0026gt;}}\nThis can all be found in the official Hugo documentation Shortcodes for creating links to documents\nAnother shortcode trick I learned when writing this post is that escaping curly braces {{ in Hugo can be tricky when using the escape character. Using the backslash will result in some variation of this:\n\\{{\u0026lt; myshortcode \u0026gt;\\}}\nA simple way to escape this is by adding /* after the opening \u0026lt; and adding */ after the closing \u0026gt;.\n{{\u0026lt;/*/* myshortcode */\u0026gt;}} will produce {{\u0026lt; shortcode \u0026gt;}}\n","date":"2021-09-02T07:37:44-04:00","permalink":"https://gbeifuss.github.io/p/page-references-in-hugo/","title":"Page References in Hugo"},{"content":"Sometimes I make a hideous typo in my git commit -m \u0026quot;description here\u0026quot; message, or decide that I need to make a clearer commit message, before I\u0026rsquo;ve done a push.\nIt looks something like this when I\u0026rsquo;m working with hugo:\ngit add . git commit -m \u0026#34;Website update 2021-090-2\u0026#34; PANIC SETS IN Fortunately, it\u0026rsquo;s relatively simple to change the commit message before the push has happened:\ngit commit --amend -m \u0026#34;Website update 2021-09-02\u0026#34; git push origin main ","date":"2021-09-02T07:37:36-04:00","permalink":"https://gbeifuss.github.io/p/amending-a-git-commit-statement/","title":"Amending a Git Commit Statement"},{"content":"I\u0026rsquo;ve already written about Querying PowerPlatform API via. Powershell, but I received notice on Friday that Microsoft has now set a date when they will cease supporting the Azure AD Graph endpoint, which also means they\u0026rsquo;ll stop supporting license assignment operation from MSOnline and Azure AD Powershell modules. As of June 30, 2022, you\u0026rsquo;ll need to use the MS Graph API endpoint and/or the Microsoft Graph Powershell module.\nWhat does this announcement from Microsoft mean? It means that I need to move away from calls to Connect-MsolService and Get-MsolUser in my script. You can see my script in my prior post.\nNow, technically, they should still work because I\u0026rsquo;m not assigning licenses, only reading them. But I don\u0026rsquo;t want my scripts to be even more fragmented, or have different EOL dates.\nSidenote: Working with MS cloud services is a mess - some things are done with this module, others with that, some with this API, others with another API\u0026hellip; oh wait, that module has been deprecated\u0026hellip; get your act together Microsoft! Some modules work with Powershell 5.1, but others only work with 7. If Microsoft really wants people to embrace this (instead of having to use it out of necessessity), please, give us something well documented, mature and well-thought-out! Ranting aside, I figured that it\u0026rsquo;d be a good opportunity to practise API calls from Powershell.\nThe two licenses that I\u0026rsquo;m interested are are related to Dynamics 365. Per Microsoft\u0026rsquo;s Product names and service plan identifiers for licensing, I need to query for:\nGUID License Name 8e7a3d30-d97d-43ab-837c-d7701cef83dc DYN365_ENTERPRISE_TEAM_MEMBERS 1e1a282c-9c54-43a2-9310-98ef728faace DYN365_ENTERPRISE_SALES Easy! Let\u0026rsquo;s query with a filter to get users with either license SKU:\nInvoke-RestMethod -Method get -Headers $headerParams -Uri \u0026#39;https://graph.microsoft.com/v1.0/users?$filter=(assignedLicenses/any(x:x/skuId eq 8e7a3d30-d97d-43ab-837c-d7701cef83dc)) or (assignedLicenses/any(x:x/skuId eq 1e1a282c-9c54-43a2-9310-98ef728faace))\u0026#39; Invoke-RestMethod : The remote server returned an error: (400) Bad Request. At line:1 char:1 + Invoke-RestMethod -Method get -Headers $headerParams -Uri \u0026#39;https://gr ... + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : InvalidOperation: (System.Net.HttpWebRequest:HttpWebRequest) [Invoke-RestMethod], WebException + FullyQualifiedErrorId : WebCmdletWebResponseException,Microsoft.PowerShell.Commands.InvokeRestMethodCommand Oof! That\u0026rsquo;s not so hot. All the Microsoft documentation and examples I could find only dealt with AND filters, or OR filters that did not query for the same value. I ended up choosing to make two calls, then merge the results:\n$DYN365_ENTERPRISE_SALES = Invoke-RestMethod -Method get -Headers $headerParams -Uri \u0026#39;https://graph.microsoft.com/v1.0/users?$filter=assignedLicenses/any(x:x/skuId eq 1e1a282c-9c54-43a2-9310-98ef728faace)\u0026amp;$select=userPrincipalName,assignedLicenses\u0026#39; $DYN365_ENTERPRISE_TEAM_MEMBER = Invoke-RestMethod -Method get -Headers $headerParams -Uri \u0026#39;https://graph.microsoft.com/v1.0/users?$filter=assignedLicenses/any(x:x/skuId eq 8e7a3d30-d97d-43ab-837c-d7701cef83dc)\u0026amp;$select=userPrincipalName,assignedLicenses\u0026#39; $Users = Compare-Object -ReferenceObject $DYN365_ENTERPRISE_SALES.value.UserPrincipalName -DifferenceObject $DYN365_ENTERPRISE_TEAM_MEMBER.value.UserPrincipalName -IncludeEqual | Select-Object -ExpandProperty InputObject | Sort-Object That gave me the results I needed.\nI ended up asking on Reddit/r/PowerShell, and the end answer I got was that a few additional options are needed for Advanced query capabilities on Azure AD directory objects: -ConsistencyLevel = eventual needs to be set in the header -\u0026amp;$count=true needs to be included in the filter\nThen something like this becomes possible:\nhttps://graph.microsoft.com/v1.0/users?$filter=(assignedLicenses/any(x:x/skuId eq 06ebc4ee-1bb5-47dd-8120-11324bc54e06) or assignedLicenses/any(x:x/skuId eq 66b55226-6b4f-492c-910c-a3b7a3c9d993))\u0026amp;$count=true One of the other great changes I ended up making was to add some table formatting in my output. I set the table style to a fixed-width font (Hack is a personal favourite) because Powershell output doesn\u0026rsquo;t align well with anything else. There\u0026rsquo;s nothing worse then great output in Powershell turned to gobblygook in an email client!\n$msg = $Report | Sort-Object LastAccess, User | ConvertTo-Html -Head \u0026#34;\u0026lt;style\u0026gt;table{font-family:lucida console, courier new, monospace;font-size:12}\u0026lt;/style\u0026gt;\u0026#34; | Out-String ","date":"2021-08-30T22:37:13-04:00","permalink":"https://gbeifuss.github.io/p/powershell-powerplatform-api-...-now-using-the-graph-api/","title":"Powershell \u0026 PowerPlatform API ... now using the Graph API"},{"content":"I\u0026rsquo;ve already written about Running Powershell scripts with Task Scheduler, but I solved a Task Scheduler problem the other day with Powershell and want to write about it.\nI love to use Powershell to solve problems and questions that I have. These are questions that are, for all intents and purposes, impossible to do manually because of the time that it would take.\nI\u0026rsquo;ve undertaken a project to implement rotating the local Administrator passwords on our work servers. One of the things that I realized, as I was doing the preliminary investigation, is that there are quite a few scheduled jobs that run as .\\Administrator in the Task Scheduler. I wondered:\nexactly how many jobs are scheduled in Task Scheduler? what do the jobs do? which servers are they on? I used Powershell and get-adcomputer to identify 543 computers in our domain that should be scanned for Task Scheduler-scheduled jobs. These computers run OSes ranging from Server 2000 to Server 2019. There were some key pieces of information I wanted to pull from them:\nComputer Name Task Name Credentials it runs under Is it enabled? When did it last run? When will it next run? What command is executed? What actions, if any, are executed? I started with Get-AllScheduledTasks and modified it for the things I need:\nI removed ActiveDirectory module lines as they\u0026rsquo;re not necessary under Powershell 7 I removed the variable $OUDistinguishedName I added field to the HashTable storing information about each Task I added filtering when querying the Task - I only want information about Tasks in the root (\u0026quot;\u0026quot;) I added a switch condition to resolve well-known SIDs to their names (i.e. LOCAL SYSTEM) and improved the regex query I sent all the output directly to a CSV instead sending it to an array, then sending that to a CSV There are some problems with the script - namely there\u0026rsquo;s no handling of Tasks with multiple triggers or actions. They show up as \u0026ldquo;System.Object[]` in the log - but for the purpose of my high-level discussions on how to handle Tasks running as .\\Administrator, the script works. There were also one or two servers where the script just hung, and this script obviously doesn\u0026rsquo;t work on older OSes like Windows Server 2003.\n1604 jobs were found\u0026hellip; wow! Those are a lot of jobs that bypass our enterprise scheduling process. 543 domain-joined systems were scanned by this script. NOTE: some systems couldn’t be queried. I’d estimate this at 10-15% (50-75 systems). 899 jobs (56%) run on DB systems (Oracle, SQL servers) 159 jobs run under the local Administrator credentials 158 jobs are to reboot the server 598 jobs have not run since 2019 614 jobs are currently disabled and not set to run. Some of them are obsolete, but some of them are purposely created \u0026amp; disabled so that an admin can easily run a predefined task by manually triggering it. With these stats, I can create a plan to deal with all these Tasks that won\u0026rsquo;t run once I rotate the local Administrator password.\nI\u0026rsquo;ve included my full script at the end of this post.\nAs I was writing this script, I found that a number of systems still had Symantec jobs running regularly, even though Symantec was uninstalled from our servers about a year ago. The tasks ran, but didn\u0026rsquo;t actually do anything since the executable they were calling had been deleted. That seemed pretty harmless but pointless, so of course I had to fix it!\nNow I had a new quest - disable all the Symantec tasks across our organization!\nWhat tool can I use? Powershell!\nI wrote a quick script that uses 2 methods to query all 543 servers, find any Tasks in the \\Symantec Endpoint Protection\\ folder and disable them\u0026hellip; then I ran it. Mission accomplished!\n#Disable-ScheduledSEPTasks \u0026lt;# There are a lot of servers with enabled SEP-related tasks in Task Scheduler. The tasks don\u0026#39;t run because the executable doesn\u0026#39;t exist. Disable these tasks to clean up log noise and prevent someone from placing a malicious file in the path Greg Beifuss 2021-08-25 12:20 #\u0026gt; #Grab specific computers $Computers = Get-ADComputer -Filter { (name -notlike \u0026#34;AC*\u0026#34;) -and (name -notlike \u0026#34;WIFI*\u0026#34;) -and (name -notlike \u0026#34;DON*\u0026#34;) -and (Enabled -eq \u0026#34;True\u0026#34;) } | Select-Object -ExpandProperty DNSHostName | Sort-Object #Do this the Powershell 7 way $Computers | ForEach-Object -Parallel { Test-Connection -ComputerName $Computer -Count 1 -Quiet -ErrorAction Stop Get-ScheduledTask -CimSession $_ -TaskPath \u0026#34;\\Symantec Endpoint Protection\\\u0026#34; } #Do this the legacy way. #If pingable, try to connect and disable tasks ForEach ($Computer in $Computers) { # Test connection with computer Test-Connection -ComputerName $Computer -Count 1 -Quiet -ErrorAction Stop # Disable SEP tasks on machines $scriptblock = {Get-ScheduledTask -TaskPath \u0026#34;\\Symantec Endpoint Protection\\\u0026#34; | Disable-ScheduledTask} Invoke-Command -ComputerName $Computer -ScriptBlock $scriptblock | Out-Null } And here is the script that generated the statistics about Task Scheduler:\n#Get-ScheduledRootTasks \u0026lt;# Get all scheduled tasks at the root level \u0026#34;\\\u0026#34; (this is where staff typically create them and avoids OS/Application-created tasks) Export the results to a CSV Modified from https://github.com/exevolution/Get-AllScheduledTasks/blob/master/Get-AllScheduledTasks.ps1 Greg Beifuss 2021-08-25 12:25 #\u0026gt; Function Connect-TaskScheduler { [CmdletBinding()] Param( [Parameter(Mandatory = $True, ValueFromPipeline = $True)] [String]$ComputerName ) Begin { $objScheduledTask = New-Object -ComObject(\u0026#34;Schedule.Service\u0026#34;) } Process { Try { Write-Verbose \u0026#34;Connecting to $ComputerName\u0026#34; $objScheduledTask.Connect(\u0026#34;$ComputerName\u0026#34;) Write-Verbose \u0026#34;Connected: $($objScheduledTask.Connected)\u0026#34; } Catch { Write-Error \u0026#34;Failed to connect to $ComputerName\u0026#34; } } End { Return $objScheduledTask } } Function Get-AllScheduledTasks { [CmdletBinding()] Param( [Parameter(Mandatory = $True)] [ValidateScript( { If ($_.GetType().BaseType.Name -eq \u0026#34;MarshalByRefObject\u0026#34; -and $_.Connected -eq $True) { $True }Else { Throw \u0026#34;Not a valid Task Scheduler connection object\u0026#34; } })] $Session, [Parameter(ValueFromPipelineByPropertyName = $True)] [ValidateNotNullOrEmpty()] [String[]]$Path = \u0026#34;\\\u0026#34;, [Parameter()] [Switch]$Recurse = $False ) Begin { $Tasks = @() $Paths = @() } Process { $Paths += Get-TaskSchedulerPaths -Session $Session -Path $Path -Recurse:$Recurse $Tasks += $Paths | ForEach-Object { $_.Path | Get-TaskSchedulerTasks -Session $Session } } End { Return $Tasks } } Function Get-TaskSchedulerPaths { [CmdletBinding()] Param( [Parameter(ValueFromPipeline = $True)] [ValidateNotNullOrEmpty()] [String[]]$Path = \u0026#34;\\\u0026#34;, [Parameter(Mandatory = $True)] [ValidateScript( { If ($_.GetType().BaseType.Name -eq \u0026#34;MarshalByRefObject\u0026#34; -and $_.Connected -eq $True) { $True }Else { Throw \u0026#34;Not a valid Task Scheduler connection object\u0026#34; } })] $Session, [Parameter()] [Switch]$Recurse = $False ) Begin { $Paths = @() } Process { $BasePath = $Session.GetFolder(\u0026#34;$Path\u0026#34;) $Paths += $BasePath $Paths += $BasePath.GetFolders(1) If ($Recurse) { ForEach ($P in $Paths) { If ($P -eq $BasePath) { Continue } Else { $Paths += Get-TaskSchedulerPaths -Session $Session -Path $P.Path -Recurse } } } } End { Return $Paths } } Function Get-TaskSchedulerTasks { [CmdletBinding()] Param( [Parameter(ValueFromPipeline = $True)] [ValidateNotNullOrEmpty()] [String[]]$Path = \u0026#34;\\\u0026#34;, [Parameter(Mandatory = $True)] [ValidateScript( { If ($_.GetType().BaseType.Name -eq \u0026#34;MarshalByRefObject\u0026#34; -and $_.Connected -eq $True) { $True }Else { Throw \u0026#34;Not a valid Task Scheduler connection object\u0026#34; } })] $Session ) Begin { $AllTasks = @() } Process { $Folder = $Session.GetFolder(\u0026#34;$Path\u0026#34;) $FolderTasks = $Folder.GetTasks(0) ForEach ($Task in $FolderTasks) { If ((($Task.Path.ToCharArray() | Where-Object { $_ -eq \u0026#39;\\\u0026#39; } | Measure-Object).Count -eq 1) -and ($Task | Where-Object -Property Name -NotMatch \u0026#39;^(SensorFramework|Optimize Start Menu|ShadowCopyVolumeUser_Feed|User_Feed|OneDrive|MicrosoftEdgeUpdateTask|GoogleUpdateTask|Adobe Acrobat Update Task)\u0026#39;)) { $RunAsID = $Task | Select-Object @{Name = \u0026#34;RunAs\u0026#34;; Expression = { [xml]$xml = $_.xml ; $xml.Task.Principals.Principal.UserId } } | Select-Object -ExpandProperty RunAs -ErrorAction Stop If ($RunAsID -match \u0026#34;S-\\d-(?:\\d+-){1,14}\\d+\u0026#34;) { Switch ($RunAsID) { \u0026#34;S-1-5-18\u0026#34; { $RunAs = \u0026#34;LOCAL SYSTEM\u0026#34;; break } \u0026#34;S-1-5-19\u0026#34; { $RunAs = \u0026#34;LOCAL SERVICE\u0026#34;; break } \u0026#34;S-1-5-20\u0026#34; { $RunAs = \u0026#34;NETWORK SERVICE\u0026#34;; break } Default { Try { $RunAs = Get-ADUser -Identity $RunAsID | Select-Object -ExpandProperty SamAccountName -ErrorAction Stop } Catch { Try { $RunAs = Get-ADGroup -Identity $RunAsID | Select-Object -ExpandProperty SamAccountName -ErrorAction Stop } Catch { $RunAs = $RunAsID } } } } } Else { $RunAs = $RunAsID } $HashTable = [Ordered]@{ ComputerName = $Session.TargetServer Name = $Task.Name RunAs = $RunAs Action = $Task | Select-Object @{Name = \u0026#34;Actions\u0026#34;; Expression = { [xml]$xml = $_.xml ; $xml.Task.Actions.Exec.Command } } | Select-Object -ExpandProperty Actions -ErrorAction Stop Arguments = $Task | Select-Object @{Name = \u0026#34;Arguments\u0026#34;; Expression = { [xml]$xml = $_.xml ; $xml.Task.Actions.Exec.Arguments } } | Select-Object -ExpandProperty Arguments -ErrorAction Stop LastRunTime = $Task.LastRunTime NextRunTime = $Task.NextRunTime TaskEnabled = $Task.Enabled } $AllTasks += New-Object PSObject -Property $HashTable } } } End { Return $AllTasks } } $Tasks = @() ForEach ($Computer in ( Get-ADComputer -Filter { (name -notlike \u0026#34;AC*\u0026#34;) -and (name -notlike \u0026#34;WIFI*\u0026#34;) -and (name -notlike \u0026#34;DON*\u0026#34;) -and (Enabled -eq \u0026#34;True\u0026#34;) } | Select-Object -ExpandProperty DNSHostName | Sort-Object)){ #ForEach ($Computer in $Computers[355..543]) { $Connection = Connect-TaskScheduler -ComputerName $Computer -Verbose Get-AllScheduledTasks -Session $Connection -Recurse | Export-CSV c:\\users\\gbadmin\\desktop\\tasks.csv -NoTypeInformation -Append } ","date":"2021-08-25T14:19:31-04:00","permalink":"https://gbeifuss.github.io/p/using-powershell-to-interact-with-task-scheduler/","title":"Using Powershell to interact with Task Scheduler"},{"content":"I decided to update my site to make it more \u0026lsquo;search engine-friendly\u0026rsquo; by making better use of the meta description field. My particular theme, Stack, does pull the .Description field from Front Matter, BUT it also uses the .Description field to populate a brief summary of the article on the home page. I don\u0026rsquo;t want SEO-friendly content displaying as a summary on my home page, so I decided to use another Front Matter field and update the theme to use that for the meta description.\nFirst, I edited the default markdown archetype (/archetypes/default.md) in Hugo to include the .summary field:\n--- title: {{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }} date: {{ .Date }} description: summary: draft: true tags: --- With that done, I needed to tell Hugo that when it generates the web content, it should populate the meta description field with .summary. This way the .description Front Matter can still be used by the theme to populate the subtitles on my home page, while using the .summary field to generate text for SEO:\nHere\u0026rsquo;s the original layouts\\partials\\head\\head.html contents:\n\u0026lt;meta charset=\u0026#39;utf-8\u0026#39;\u0026gt; \u0026lt;meta name=\u0026#39;viewport\u0026#39; content=\u0026#39;width=device-width, initial-scale=1\u0026#39;\u0026gt; {{- $description := partialCached \u0026#34;data/description\u0026#34; . .RelPermalink -}} \u0026lt;meta name=\u0026#39;description\u0026#39; content=\u0026#39;{{ .description }}\u0026#39;\u0026gt; {{- $title := partialCached \u0026#34;data/title\u0026#34; . .RelPermalink -}} \u0026lt;title\u0026gt;{{ $title }}\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#39;canonical\u0026#39; href=\u0026#39;{{ .Permalink }}\u0026#39;\u0026gt; {{- partial \u0026#34;head/style.html\u0026#34; . -}} {{- partial \u0026#34;head/script.html\u0026#34; . -}} {{- partial \u0026#34;head/opengraph/include.html\u0026#34; . -}} {{- range .AlternativeOutputFormats -}} \u0026lt;link rel=\u0026#34;{{ .Rel }}\u0026#34; type=\u0026#34;{{ .MediaType.Type }}\u0026#34; href=\u0026#34;{{ .Permalink | safeURL }}\u0026#34;\u0026gt; {{- end -}} {{ with .Site.Params.favicon }} \u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;{{ . }}\u0026#34; /\u0026gt; {{ end }} {{- template \u0026#34;_internal/google_analytics.html\u0026#34; . -}} {{- partial \u0026#34;head/custom.html\u0026#34; . -}} I replaced the line \u0026lt;meta name='description' content='{{ .description }}'\u0026gt; with \u0026lt;meta name='description' content='{{ .summary | truncate 130 }}'\u0026gt;\nThis tells Hugo to refer to the .Summary Front Matter field, and not the .Description, when generating the meta description. I also capped the meta description at 130 characters, just in case I write more than that. Search Engines typically consider somewhere between 25-160 characters, but check with your specific search engine for the exact values.\nThis is my final layouts\\partials\\head\\head.html file:\n\u0026lt;meta charset=\u0026#39;utf-8\u0026#39;\u0026gt; \u0026lt;meta name=\u0026#39;viewport\u0026#39; content=\u0026#39;width=device-width, initial-scale=1\u0026#39;\u0026gt; {{- $description := partialCached \u0026#34;data/description\u0026#34; . .RelPermalink -}} \u0026lt;meta name=\u0026#39;description\u0026#39; content=\u0026#39;{{ .summary | truncate 130 }}\u0026#39;\u0026gt; {{- $title := partialCached \u0026#34;data/title\u0026#34; . .RelPermalink -}} \u0026lt;title\u0026gt;{{ $title }}\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#39;canonical\u0026#39; href=\u0026#39;{{ .Permalink }}\u0026#39;\u0026gt; {{- partial \u0026#34;head/style.html\u0026#34; . -}} {{- partial \u0026#34;head/script.html\u0026#34; . -}} {{- partial \u0026#34;head/opengraph/include.html\u0026#34; . -}} {{- range .AlternativeOutputFormats -}} \u0026lt;link rel=\u0026#34;{{ .Rel }}\u0026#34; type=\u0026#34;{{ .MediaType.Type }}\u0026#34; href=\u0026#34;{{ .Permalink | safeURL }}\u0026#34;\u0026gt; {{- end -}} {{ with .Site.Params.favicon }} \u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;{{ . }}\u0026#34; /\u0026gt; {{ end }} {{- template \u0026#34;_internal/google_analytics.html\u0026#34; . -}} {{- partial \u0026#34;head/custom.html\u0026#34; . -}} ","date":"2021-08-25T08:15:17-04:00","permalink":"https://gbeifuss.github.io/p/use-.summary-in-hugo-with-the-stack-theme/","title":"Use .Summary in Hugo with the Stack theme"},{"content":"Today I:\n\u0026hellip; started my computer, as usual\n\u0026hellip; checked for patches, as usual\n\u0026hellip; checked for updated software (via chocolatey!), as usual\n\u0026hellip; started Hugo\u0026rsquo;s local server\u0026hellip; as not usual!\nC:\\Users\\gbeifuss\\blog\u0026gt;hugo server -D -v -p 1313 port 1313 already in use, attempting to use an available port Start building sites … hugo v0.87.0-B0C541E4+extended windows/amd64 BuildDate=2021-08-03T10:57:28Z VendorInfo=gohugoio INFO 2021/08/20 11:20:57 syncing static files to C:\\Users\\gbeifuss\\blog\\ WARN 2021/08/20 11:20:57 Search page not found. Create a page with layout: search. WARN 2021/08/20 11:20:57 Archives page not found. Create a page with layout: archives. | EN -------------------+----- Pages | 45 Paginator pages | 5 Non-page files | 0 Static files | 5 Processed images | 5 Aliases | 12 Sitemaps | 1 Cleaned | 0 Built in 135 ms Watching for changes in C:\\Users\\gbeifuss\\blog\\{archetypes,content,data,layouts,static,themes} Watching for config changes in C:\\Users\\gbeifuss\\blog\\config.yaml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:15823/ (bind address 127.0.0.1) Press Ctrl+C to stop This has never been a problem before. How can port 1313 already be in use? There\u0026rsquo;s nothing else on my system configured to use that port!\nI pulled up netstat to confirm:\nC:\\Users\\gbeifuss\u0026gt;netstat -ano | find \u0026#34;1313\u0026#34; C:\\Users\\gbeifuss\u0026gt; Well, if nothing is using the port, then why can\u0026rsquo;t Hugo use it? If there\u0026rsquo;s nothing actively blocking Hugo, then Hugo must be prevented from using that port by another mechanism.\nI checked the reservations that Windows knows about with netsh:\nC:\\Users\\gbeifuss\\blog\u0026gt;netsh int ipv4 show excludedportrange protocol=tcp Protocol tcp Port Exclusion Ranges Start Port End Port ---------- -------- 1214 1313 8004 8004 11801 11900 29480 29579 30221 30320 32822 32921 49680 49779 50000 50059 * 50160 50259 50261 50360 50378 50477 * - Administered port exclusions. Well, there we go! What\u0026rsquo;s reserved that port?\nI stopped Docker Desktop Service and made sure all the Hyper-V services weren\u0026rsquo;t running either, but that reservation was still there.\nIt turns out that the range 1214-1313 was reserved by the Windows NAT Driver (WinNat) service.\nI stopped the winnat service via. the command line (as Administrator):\nnet stop winnat\nThen I started Hugo, which happily grabbed port 1313:\nC:\\Users\\gbeifuss\\blog\u0026gt;hugo server -D Start building sites … hugo v0.87.0-B0C541E4+extended windows/amd64 BuildDate=2021-08-03T10:57:28Z VendorInfo=gohugoio WARN 2021/08/20 14:12:16 Search page not found. Create a page with layout: search. WARN 2021/08/20 14:12:16 Archives page not found. Create a page with layout: archives. | EN -------------------+----- Pages | 46 Paginator pages | 5 Non-page files | 0 Static files | 5 Processed images | 8 Aliases | 12 Sitemaps | 1 Cleaned | 0 Built in 481 ms Watching for changes in C:\\Users\\gbeifuss\\blog\\{archetypes,content,data,layouts,static,themes} Watching for config changes in C:\\Users\\gbeifuss\\blog\\config.yaml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop Then I started the winnat service again:\nnet start winnat\n","date":"2021-08-20T14:12:12-04:00","permalink":"https://gbeifuss.github.io/p/troubleshooting-hugos-port-1313-already-in-use-error/","title":"Troubleshooting Hugo's 'Port 1313 already in use' error"},{"content":"Sometimes I try to setup PSWindowsUpdate (an amazing module from the Powershell Gallery) and receive an error like this one:\nWARNING: Source Location ‘https://www.powershellgallery.com/api/v2/package/PSWindowsUpdate/2.2.0.2\u0026#39; is not valid. PackageManagement\\Install-Package : Package ‘PSWindowsUpdate\u0026#39; failed to download. At C:\\Program Files\\WindowsPowerShell\\Modules\\PowerShellGet\\1.0.0.1\\PSModule.psm1:1772 char:21 + … $null = PackageManagement\\Install-Package @PSBoundParameters + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : ResourceUnavailable: (C:\\Users\\... :String) [Install-Package], Exception + FullyQualifiedErrorId : PackageFailedInstallOrDownload,Microsoft.PowerShell.PackageManagement.Cmdlets.InstallPackage A similar issue arises with using the Invoke-WebRequest cmdlet. The root cause is that Powershell is trying to connect to a site and there\u0026rsquo;s no agreement on the encryption protocol to use. By default, Powershell uses TLS 1.0 and that\u0026rsquo;s been widely deprecated.\nThe Background\rTransport Layer Security (TLS) is the successor to SSL. Starting in 2018, there was a groundswell of (good) advice that TLS 1.0 and 1.1 should be deprecated on websites and in browsers. This was largely adopted across the internet by 2020. That leaves TLS 1.2 as the de facto standard, with TLS 1.3 adoption rising but not as widespread yet.\nThe Problem\rIn April 2020, Microsoft disabled support for TLS 1.0 on the Powershell Gallery and now requires TLS 1.2. The issue is that Powershell 5.1 doesn\u0026rsquo;t support this configuration out of the box and the PowershellGet module (1.0.0.1) didn\u0026rsquo;t support TLS 1.2 at all. Smooth move, Microsoft.\nThe Solution\rMicrosoft released a new version of PowershellGet (2.2.4) in April 2020 that supports TLS 1.2. You can install it like this:\nInstall-Module PowerShellGet -RequiredVersion 2.2.4 -SkipPublisherCheck By default, Powershell uses whatever the system default settings for crypto:\nPS \u0026gt; [Net.ServicePointManager]::SecurityProtocol SystemDefault \u0026hellip; but the problem is that the default for each system could be different. You can force your system to enable TLS 1.2 support in your Powershell session:\n[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 \u0026hellip; but the problem with this command is that you need to run it everytime you open a new Powershell session.\nLet\u0026rsquo;s update the current user\u0026rsquo;s Powershell profile (creating it if it doesn\u0026rsquo;t exist) so that TLS 1.2 support is enabled every time a session is launched:\n$ProfileFile = \u0026#34;${PsHome}\\Profile.ps1\u0026#34; if (! (Test-Path $ProfileFile)) {New-Item -Path $ProfileFile -Type file -Force} \u0026#39;[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\u0026#39; | Out-File -FilePath $ProfileFile -Encoding ascii -Append Actually, while we\u0026rsquo;re at it, let\u0026rsquo;s configure Windows and .NET too:\n#TLS1.2-Windows.ps1 \u0026lt;# Enable only TLS 1.2 on Windows. Disable TLS 1.0, 1.2 Enable .NET to use TLS 1.2 Greg Beifuss 2020-07-02 16:11 #\u0026gt; New-Item \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.0\\Server\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.0\\Server\u0026#39; -Name \u0026#39;Enabled\u0026#39; -Value \u0026#39;0\u0026#39; -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.0\\Server\u0026#39; -Name \u0026#39;DisabledByDefault\u0026#39; -Value 1 -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null New-Item \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.0\\Client\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.0\\Client\u0026#39; -Name \u0026#39;Enabled\u0026#39; -Value \u0026#39;0\u0026#39; -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.0\\Client\u0026#39; -Name \u0026#39;DisabledByDefault\u0026#39; -Value 1 -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null Write-Host \u0026#39;TLS 1.0 has been Disabled.\u0026#39; New-Item \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.1\\Server\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.1\\Server\u0026#39; -Name \u0026#39;Enabled\u0026#39; -Value \u0026#39;0\u0026#39; -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.1\\Server\u0026#39; -Name \u0026#39;DisabledByDefault\u0026#39; -Value 1 -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null New-Item \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.1\\Client\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.1\\Client\u0026#39; -Name \u0026#39;Enabled\u0026#39; -Value \u0026#39;0\u0026#39; -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.1\\Client\u0026#39; -Name \u0026#39;DisabledByDefault\u0026#39; -Value 1 -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null Write-Host \u0026#39;TLS 1.1 has been Disabled.\u0026#39; New-Item \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.2\\Server\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.2\\Server\u0026#39; -Name \u0026#39;Enabled\u0026#39; -Value \u0026#39;1\u0026#39; -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.2\\Server\u0026#39; -Name \u0026#39;DisabledByDefault\u0026#39; -Value 0 -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null New-Item \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.2\\Client\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.2\\Client\u0026#39; -Name \u0026#39;Enabled\u0026#39; -Value \u0026#39;1\u0026#39; -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null New-ItemProperty -Path \u0026#39;HKLM:\\SYSTEM\\CurrentControlSet\\Control\\SecurityProviders\\SCHANNEL\\Protocols\\TLS 1.2\\Client\u0026#39; -Name \u0026#39;DisabledByDefault\u0026#39; -Value 0 -PropertyType \u0026#39;DWord\u0026#39; -Force | Out-Null Write-Host \u0026#39;TLS 1.2 has been Enabled.\u0026#39; Set-ItemProperty -Path \u0026#39;HKLM:\\SOFTWARE\\Wow6432Node\\Microsoft\\.NetFramework\\v4.0.30319\u0026#39; -Name \u0026#39;SchUseStrongCrypto\u0026#39; -Value \u0026#39;1\u0026#39; -Type DWord Set-ItemProperty -Path \u0026#39;HKLM:\\SOFTWARE\\Microsoft\\.NetFramework\\v4.0.30319\u0026#39; -Name \u0026#39;SchUseStrongCrypto\u0026#39; -Value \u0026#39;1\u0026#39; -Type DWord ","date":"2021-08-20T11:02:12-04:00","permalink":"https://gbeifuss.github.io/p/adding-tls-1.2-support-for-powershell/","title":"Adding TLS 1.2 support for Powershell"},{"content":"I was trying to use PS-Remoting to remotely run some powershell commands on a few servers, but one of them returned an error instead of connecting. I found this odd because:\nThe other 2 servers didn\u0026rsquo;t have this error, and they all used the same GPOs to enforce configuration, and Enter-PSSession worked on this server a few days ago. \u0026gt; enter-pssession DomainController01 Enter-PSSession: Connecting to remote server DomainController01 failed with the following error message : WinRM cannot complete the operation. Verify that the specified computer name is valid, that the computer is accessible over the network, and that a firewall exception for the WinRM service is enabled and allows access from this computer. By default, the WinRM firewall exception for public profiles limits access to remote computers within the same local subnet. For more information, see the about_Remote_Troubleshooting Help topic. I opened an RDP session to the server to see exactly what was going on:\nWinRM was correctly configured Windows Firewall was correctly configured. The Domain profile permitted Windows Remote Management. PS-Remoting was correctly configured in Powershell \u0026hellip; Then I saw it. The network connection icon in the systray said \u0026ldquo;unidentified network\u0026rdquo;.\nOf course! Since Windows isn\u0026rsquo;t detecting the network connection as part of the Domain Network, Windows Firewall can\u0026rsquo;t permit my WinRM traffic under the Domain profile rule. The job of reviewing the network configuration (IP Address) and assigning a location to the network is the job of the Network Location Awareness (NLA) service. NLA first attempts to identify a logical network by its domain name (DNS). If a logical network does not have a domain name, NLA identifies the network from custom static information stored in the registry, and finally from its subnet address.\nFixing this misdetection is quite easy. Off the top of my head, there are at least 3 ways to do this:\nFrom the command prompt: net stop nlasvc \u0026amp;\u0026amp; net start nlasvc From the services.msc GUI interface: Restart the Network Location Awareness service From powershell: get-service nlasvc | restart-service After restarting the service, the OS assigned the proper location (\u0026ldquo;domain network\u0026rdquo;) to the connection. A quick test with powershell confirmed that enter-pssession now worked like it should have 10 minutes ago.\n","date":"2021-08-18T07:50:51-04:00","permalink":"https://gbeifuss.github.io/p/enter-pssession-winrm-nla/","title":"Enter-PSSession, WinRM \u0026 NLA"},{"content":"Our FOI/Privacy Officer was informed of a an unauthorized disclosure of PII via email by one of our staff members. I was looped in to\na) quantify how many other people in our organization had that particular email in their mailbox, and\nb) administratively remove it.\nThis is fairly straightforward with Powershell. The hardest part is finding the right message(s) among hundreds of mailboxes containing thousands of messages each.\nTo start, I got the message details. When I\u0026rsquo;m not familiar with the information I\u0026rsquo;ll get back, I like to store the output in a variable before manipulating it. It saves time because I don\u0026rsquo;t need to requery the information (all Exchange mailboxes in this case). I then used the Search-Mailbox cmdlet to track it down:\n\u0026gt; $results = get-mailbox | Search-Mailbox -SearchQuery \u0026#34;subject:Attention: John Doe AND from:customer@gmail.com AND received:07/28/2021\u0026#34; \u0026gt; $results \u0026gt; Well that\u0026rsquo;s not good. There were no results. This time, I\u0026rsquo;ll look for the specific attachment that contained the PII, then display any results that have a resultcount \u0026gt; 0 (that is, a message with the attachment was found):\n\u0026gt; $results = get-mailbox | Search-Mailbox -SearchQuery \u0026#34;attachment:tax_07-10-2021-080049.pdf\u0026#34; -EstimateResultOnly \u0026gt; $results | where {$_.resultitemscount -ne 0} | select identity,resultitemscount | ft -a Identity ResultItemsCount -------- ---------------- company.com/Corporate Users/JDoe 3 company.com/Corporate Users/GWashington 2 Much better! 5 messages found across Exchange. Now, let\u0026rsquo;s purge them from JDoe\u0026rsquo;s mailbox, but leave them in GWashington\u0026rsquo;s mailbox. We need the -DeleteContent switch for that:\n\u0026gt; \u0026#34;JDoe\u0026#34; | Search-Mailbox -SearchQuery \u0026#34;attachment:tax_07-10-2021-080049.pdf\u0026#34; -DeleteContent ","date":"2021-08-11T15:01:20-04:00","permalink":"https://gbeifuss.github.io/p/exchange-deletecontent/","title":"Exchange \u0026 -DeleteContent"},{"content":"These days I do minimal networking, but my college diploma is in an IP Engineering discipline, and prior to that, I had the opportunity to complete the Cisco Networking Academy curriculum in high school. All of that to say that while I\u0026rsquo;ve not pursued that field, I have a decent foundation and understanding on how those things operate. At work, I have my hand in firewalls, switches, DNS, DHCP, routing, subnetting and other wonderful things, but it\u0026rsquo;s not super advanced work.\nFor about 8 months now, we\u0026rsquo;ve had a dedicated link setup between our main office and a remote site. This will replace an internet-based VPN tunnel that currently carries traffic. However, it\u0026rsquo;s still not in use because of problems. More specifically, whenever I switch traffic to route over the dedicated link, service becomes extremely problematic. Ping to any system works, but protocols like RDP, SMB and replication (AD \u0026amp; Exchange) work intermittantly at best (RDP), or not at all (SMB, replication). I could RDP to ServerA, but not to ServerB, although they\u0026rsquo;re both VMs hosted on the same Hyper-V server at the remote site. And while RDP worked to the DC at the remote site and I could ping all the other DCs, repadmin /replsummary showed that replication was no longer working.\nHere\u0026rsquo;s a simple topology diagram in ASCII art:\nHead Office core switch - Head Office firewall - VPN Tunnel over internet - SiteA firewall - SiteA core switch vs. Head Office core switch - dedicated L2 link - SiteA core switch I knew this was going to be a pain to troubleshoot, especially since I couldn\u0026rsquo;t take the whole DR site offline whenever I wanted during the day to conduct tests. I quintuple checked the configuration and didn\u0026rsquo;t find any errors. I compared it line-by-line to another site we have this same setup (dedicated link between core switches) and everything was configured correctly.\nI finally decided to bite the bullet and spent a couple of late nights trying to gather information so that I could quantify the precise problem. Because the physical layer was out of my control, the data link layer seemed fine (also out of my control\u0026hellip;mostly) and ping worked (layer 3), I was pretty confident the issue originated at the Network layer, or higher.\nI started by looking at the packet sizes. I knew that ping was a tiny packet, while things like replication and SMB would probably exceed the MTU. I ran some tests with the ping -f -l to determine the maximum packet size that could successfully transit the link.\nNo. Time Source Destination Protocol Length ICMP Length Info 1 0.000000 10.1.0.51 192.168.4.3 ICMP 1507 1465 Echo (ping) request id=0x006b, seq=37568/49298, ttl=128 (reply in 2) 2 0.005320 192.168.4.3 10.1.0.51 ICMP 1507 1465 Echo (ping) reply id=0x006b, seq=37568/49298, ttl=126 (request in 1) 3 1.005759 10.1.0.51 192.168.4.3 ICMP 1507 1465 Echo (ping) request id=0x006b, seq=37569/49554, ttl=128 (reply in 4) 4 1.011053 192.168.4.3 10.1.0.51 ICMP 1507 1465 Echo (ping) reply id=0x006b, seq=37569/49554, ttl=126 (request in 3) 5 2.011301 10.1.0.51 192.168.4.3 ICMP 1507 1465 Echo (ping) request id=0x006b, seq=37570/49810, ttl=128 (reply in 6) 6 2.016662 192.168.4.3 10.1.0.51 ICMP 1507 1465 Echo (ping) reply id=0x006b, seq=37570/49810, ttl=126 (request in 5) 7 3.017966 10.1.0.51 192.168.4.3 ICMP 1507 1465 Echo (ping) request id=0x006b, seq=37571/50066, ttl=128 (reply in 8) 8 3.023353 192.168.4.3 10.1.0.51 ICMP 1507 1465 Echo (ping) reply id=0x006b, seq=37571/50066, ttl=126 (request in 7) 9 16.070931 10.1.0.51 192.168.4.3 ICMP 1510 1468 Echo (ping) request id=0x006b, seq=37576/51346, ttl=128 (reply in 10) 10 16.076166 192.168.4.3 10.1.0.51 ICMP 1510 1468 Echo (ping) reply id=0x006b, seq=37576/51346, ttl=126 (request in 9) 11 17.075444 10.1.0.51 192.168.4.3 ICMP 1510 1468 Echo (ping) request id=0x006b, seq=37577/51602, ttl=128 (reply in 12) 12 17.080656 192.168.4.3 10.1.0.51 ICMP 1510 1468 Echo (ping) reply id=0x006b, seq=37577/51602, ttl=126 (request in 11) 13 18.082161 10.1.0.51 192.168.4.3 ICMP 1510 1468 Echo (ping) request id=0x006b, seq=37578/51858, ttl=128 (reply in 14) 14 18.087498 192.168.4.3 10.1.0.51 ICMP 1510 1468 Echo (ping) reply id=0x006b, seq=37578/51858, ttl=126 (request in 13) 15 19.087735 10.1.0.51 192.168.4.3 ICMP 1510 1468 Echo (ping) request id=0x006b, seq=37579/52114, ttl=128 (reply in 16) 16 19.093044 192.168.4.3 10.1.0.51 ICMP 1510 1468 Echo (ping) reply id=0x006b, seq=37579/52114, ttl=126 (request in 15) 17 26.901887 10.1.0.51 192.168.4.3 ICMP 1511 1469 Echo (ping) request id=0x006b, seq=37580/52370, ttl=128 (no response found!) 18 31.753228 10.1.0.51 192.168.4.3 ICMP 1511 1469 Echo (ping) request id=0x006b, seq=37581/52626, ttl=128 (no response found!) 19 36.753276 10.1.0.51 192.168.4.3 ICMP 1511 1469 Echo (ping) request id=0x006b, seq=37582/52882, ttl=128 (no response found!) 20 41.753263 10.1.0.51 192.168.4.3 ICMP 1511 1469 Echo (ping) request id=0x006b, seq=37583/53138, ttl=128 (no response found!) I found that where the size was \u0026lt;= 1468, the ping succeeded. At 1469-1472 bytes, there was no response. And at sizes \u0026gt;= 1473, ping helpfully told me that the Packet needs to be fragmented but DF set.\nOkay! We\u0026rsquo;re making progress! \u0026hellip;Maybe. Still working on the MTU theory, I took a look at all the network devices that these packets would transit:\nServerA Hyper-V host server Remote site core switch \u0026hellip; L2 Dedicated Link beyond my control\u0026hellip; Head Office core switch Both switches were set with an MTU of 1500 bytes, which is pretty standard. The Hyper-V host server used the Windows default of 1500, as did ServerA. So it looked like MTUs haven\u0026rsquo;t been altered in a meaningful way.\nI ran a series of packet captures to see if anything looked unusual. Nothing jumped out at me, but I did notice that whenever the packet size exceeded 1500-ish bytes, there were lots of retranmissions and/or errors.\nWait! How is it that a server with an MTU of 1500 is sending packets that are 2600 bytes?\nWireshark uses winpcap (or libpcap) to grab the data before it\u0026rsquo;s handed to the NIC: Many OS and NIC drivers support TCP Segmentation Offload / Large Segment Offload / Generic Segment Offload which offloads the task of breaking up TCP data into MSS-appropriate pieces. This is handled by the NIC, and saves resource overhead, improving performance. However\u0026hellip; with offloading enabled, this task is now completed after Wireshark has grabbed the data, so it\u0026rsquo;s not seen when capturing from the OS. Using a SPAN port (port mirroring) or TAP would not have this limitation.\nBack to my regularly scheduled commentary\u0026hellip;\nRunning with the MTU theory, I decided to set the OS MTU = 1496 bytes. That\u0026rsquo;s 1468, which is the largest setting that worked with the Don\u0026rsquo;tFragment flag set, plus the 28 byte IP header. Let\u0026rsquo;s double check what the MTU is set at on my Windows 2019 DC at SiteA: netsh int ipv4 show sub (the full command is netsh interface ipv4 show subinterfaces, but I generally use the shorthand).\nC:\\Users\u0026gt;netsh int ipv4 show sub MTU MediaSenseState Bytes In Bytes Out Interface ------ --------------- --------- --------- ------------- 4294967295 1 0 52364 Loopback Pseudo-Interface 1 1500 1 550194231 647147510 Ethernet 2 Let\u0026rsquo;s update this: netsh in ipv4 set sub \u0026quot;Ethernet 2\u0026quot; mtu=1496 store=persistent (netsh interface ipv4 set subinterface \u0026ldquo;Ethernet 2\u0026rdquo; mtu=1496 store=persistent)\nC:\\Users\u0026gt;netsh int ipv4 show sub MTU MediaSenseState Bytes In Bytes Out Interface ------ --------------- --------- --------- ------------- 4294967295 1 0 52364 Loopback Pseudo-Interface 1 1496 1 55025789 647165843 Ethernet 2 On Windows 2019, this took effect immediately. If it doesn\u0026rsquo;t, a reboot should ensure it applies. AD replication - which had been failing - picked right back up as soon as I entered this command.\nNow, I\u0026rsquo;ve proved that it\u0026rsquo;s an MTU issue, so I\u0026rsquo;ve turned it over to our L2 direct link provider. Interestingly enough, the 4 bytes (1500-1496=4) is the exact same size that a QinQ implementation adds. The packet arrives as an 802.1Q frame (tagged), and QinQ will add its own Tag \u0026amp; EType field in the frame. This resulting QinQ frame is often referred to as \u0026lsquo;double tagged\u0026rsquo;. You can read more about QinQ in the Cisco article Inter-Switch Link and IEEE 802.1Q Frame Format\n","date":"2021-08-11T14:57:19-04:00","permalink":"https://gbeifuss.github.io/p/network-troubleshooting-and-mtu/","title":"Network Troubleshooting and MTU"},{"content":"When I first committed to actually publishing a blog and settled on using Hugo and Github, I ran into a problem when trying to actually synchronize my local repository with Github: I kept getting a \u0026lsquo;Permission Denied (publickey)\u0026rsquo; error when I pushed the content up to Github. After some Google-fu, I found that the error was because I didn\u0026rsquo;t have any SSH keys setup on my Windows 10 system and it couldn\u0026rsquo;t communicate securely with GitHub.\nHere\u0026rsquo;s how I fixed that:\nOpen git bash. I could have done this in Windows Terminal (by creating a new profile linked to git bash), but since this is a one-time task, I didn\u0026rsquo;t bother. Type cd ~/.ssh. This will take you to the root directory for Git (likely C:\\Users\\username\\.ssh\\) Within the .ssh folder, there should be two files: id_rsa and id_rsa.pub. Type ls to see a directory listing. These are the files that tell your computer how to communicate with GitHub. If those two files don\u0026rsquo;t show up, continue to #4.\nNOTE: Your SSH keys must be named id_rsa and id_rsa.pub in order for Git and GitHub to recognize them by default. Since the SSH keys don\u0026rsquo;t exist yet, create thethem: type ssh-keygen -t rsa -C \u0026quot;you@email.com\u0026quot;. This will create both the id_rsa and id_rsa.pub files. Open id_rsa.pub in your favorite text editor Notepad. Copy the contents (exactly as it appears, with no extra spaces or lines) of id_rsa.pub and paste it into GitHub under the Account Settings \u0026gt; SSH Keys page. Now that you\u0026rsquo;ve added your public key to Github, try to git push again and see if it works. It should!\nA step-by-step rundown of this process (and more!) is also on the GitHub site: Connecting to GitHub with SSH\n","date":"2021-08-09T14:00:25-04:00","permalink":"https://gbeifuss.github.io/p/pushing-to-github-via.-git/","title":"Pushing to GitHub via. Git"},{"content":"I\u0026rsquo;m in the midst of trying to phase out non-Kerberos authentication traffic in our domain, because it\u0026rsquo;s time. MS has offered Kerberos since Windows 2000, and the pre-Kerberos options (LANMan, NTLMv1, NTLMv2) are woefully insecure. In fact, MS stopped recommending NTLM for use in applications in 2010. It\u0026rsquo;s still widely in use for compatibility reasons.\nThe Protocols\rLANMan\rLANMan, or LAN Manager, was offered in 1987 and can be easily broken. Among other issues:\nA LANMan password is not case-sensitive. Everything is cast to uppercase before the hash is created. This dramatically reduces the characters that need to be bruteforced, as lowercase isn\u0026rsquo;t a required set (reducing the total characters by 26). When first created, the implementation presumed that passwords woud never be longer than 7 characters, so a total of eight bytes are used to store it - the seven password characters plus a parity byte. Of course, this quickly became a problem, so the password length was increased to use 16 bytes. But in order to remain backwards compatible, this is implemented as 2x 8-byte fields (7 password characters + 1 parity byte). Any password shorter than 14 charafters will have unused bytes filled with NULLs.\nIf the password is seven characters or less, the entire second 8-byte field will be full of NULLs, which means the attacker only has to break one 7-character password. Even if the password takes a full 14 characters, the attacker doesn\u0026rsquo;t have to break that, they are actually breaking 2 separate seven-character passwords, which is magnitudes of order faster. NTLM v1\rNew Technology LAN Manager (NTLM) is the default authentication protocol for NT 4.0. It\u0026rsquo;s a challenge/response-based protocol but has issues as well:\nThe challenge issued by NTLMv1 is always a 16-byte random number. It used a DES algorithm for encryption of the challenge (with the user\u0026rsquo;s hash). DES is built for speed, which makes decryption by an attacker fast as well. As of 2019, every possible 8-character NTLM password hash can be enumerated by modern hardware in about 2.5 hours. It\u0026rsquo;s even faster if you use rainbow tables, which exist for all 8 and 9-character passwords.\nNTLM v2\rA number of the NTLM v1 weaknesses are corrected in v2:\nThe challenge is a variable-length challenge. HMAC-MD5 is used instead of DES. It\u0026rsquo;s slower, so brute force isn\u0026rsquo;t a practical option\u0026hellip; at least until quantum computers become available! NTLM remains vulnerable to pass the hash attacks, in addition to not supporting modern encryption algorithms (AES, SHA-256).\nDecommissioning NTLM and LANMan\rNow that it\u0026rsquo;s clear why NTLM is a terrible modern choice for authentication, let\u0026rsquo;s sort out how to get rid of it. Many older devices may only support NTLM, so we need to identify any devices currently using it.\nAudit\rFirst, enable NTLM auditing on your Domain Controllers. This will create Event ID 4624 in the Security Event log. This event will note which authentication method was used: KERBEROS or NTLM. If it\u0026rsquo;s NTLM, a subfield will tell whether it\u0026rsquo;s LM, V1 or V2.\nI created a GPO for my DCs and set these values under Computer Configuration\\Windows Settings\\Security Settings\\Local Policies\\Security Options:\nNetwork Security: Restrict NTLM: Audit NTLM authentication in this domain to Enable all\nNetwork security: Restrict NTLM: Audit Incoming NTLM Traffic to Enable auditing for all accounts\nNetwork security: Restrict NTLM: Outgoing NTLM traffic to remote servers to Audit all\nNow I have Event ID 4624 showing up in my logs. I want to find if there\u0026rsquo;s any NTLM v1 or LM traffic. Let\u0026rsquo;s parse this with with a custom XML filter for all records matching Event ID 4624 that don\u0026rsquo;t contain \u0026lsquo;NTLM v2\u0026rsquo;, \u0026lsquo;Kerberos\u0026rsquo; or \u0026lsquo;-\u0026rsquo; authentication types.\nNOTE: This is the relevant part of my code for the filtering. The full script does some other things like emailing me a report, which is why there are other functions and variables referenced:\nforeach ($domaincontroller in $domaincontrollers) { Clear-Variable -Name eventlog $query = @\u0026#34; \u0026lt;QueryList\u0026gt; \u0026lt;Query Id=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;Select Path=\u0026#34;Security\u0026#34;\u0026gt; *[System[(EventID=\u0026#39;4624\u0026#39;)]] \u0026lt;/Select\u0026gt; \u0026lt;Suppress Path=\u0026#34;Security\u0026#34;\u0026gt; *[EventData[Data[@Name=\u0026#39;AuthenticationPackageName\u0026#39;] and (Data=\u0026#39;NTLM v2\u0026#39;)]] or *[EventData[Data[@Name=\u0026#39;AuthenticationPackageName\u0026#39;] and (Data=\u0026#39;Kerberos\u0026#39;)]] or *[EventData[Data[@Name=\u0026#39;AuthenticationPackageName\u0026#39;] and (Data=\u0026#39;-\u0026#39;)]] \u0026lt;/Suppress\u0026gt; \u0026lt;/Query\u0026gt; \u0026lt;/QueryList\u0026gt; \u0026#34;@ $eventlog = Get-WinEvent -FilterXml $query $MyReport += Get-CustomHeader \u0026#34;1\u0026#34; \u0026#34;NTLMv1 Events on domain controller $domaincontroller\u0026#34; $MyReport += Get-HtmlTable ($eventlog | Where-Object { $_.ID -eq \u0026#34;4624\u0026#34; } | Select-Object TimeCreated, Message) $MyReport += Get-CustomHeaderClose $MyReport += Get-CustomHeader0Close $MyReport += Get-CustomHTMLClose } Good!\nThe only relevant entries I found were from a server that I upgraded 3 weeks ago, specifically because it was using NTLM for an obsolete application. After tossing on a supported OS and upgrading to the current application version, NTLMv1 authentication from this server stopped.\nBlock\rMS exposes a GPO value to control the NTLM authentication methods available on the domain. Computer Configuration\\Windows Settings\\Security Settings\\Local Policies\\Security Options\\Network security: LAN Manager authentication level will enforce these behaviours:\nSetting Description Registry security level Send LM \u0026amp; NTLM responses Client devices use LM and NTLM authentication, and they never use NTLMv2 session security. Domain controllers accept LM, NTLM, and NTLMv2 authentication. 0 Send LM \u0026amp; NTLM – use NTLMv2 session security if negotiated Client devices use LM and NTLM authentication, and they use NTLMv2 session security if the server supports it. Domain controllers accept LM, NTLM, and NTLMv2 authentication. 1 Send NTLM response only Client devices use NTLMv1 authentication, and they use NTLMv2 session security if the server supports it. Domain controllers accept LM, NTLM, and NTLMv2 authentication. 2 Send NTLMv2 response only Client devices use NTLMv2 authentication, and they use NTLMv2 session security if the server supports it. Domain controllers accept LM, NTLM, and NTLMv2 authentication. 3 Send NTLMv2 response only. Refuse LM Client devices use NTLMv2 authentication, and they use NTLMv2 session security if the server supports it. Domain controllers refuse to accept LM authentication, and they will accept only NTLM and NTLMv2 authentication. 4 Send NTLMv2 response only. Refuse LM \u0026amp; NTLM Client devices use NTLMv2 authentication, and they use NTLMv2 session security if the server supports it. Domain controllers refuse to accept LM and NTLM authentication, and they will accept only NTLMv2 authentication. 5 Obviously, 0 is the most insecure setting, but the most compatible. If your domain ever had a Domain Level lower than Windows Server 2008, 0 is probably what you\u0026rsquo;ll be using. Since Windows Server 2008, the default has been 3.\nI\u0026rsquo;d previously changed our domain from 0 to 2, but now that our last NTLMv1 application is gone, I\u0026rsquo;ve set this to 3. We\u0026rsquo;re headed full speed into the future! 2008.\n","date":"2021-08-05T11:11:56-04:00","permalink":"https://gbeifuss.github.io/p/powershell-ntmlv1-use/","title":"Powershell \u0026 NTMLv1 use"},{"content":"This blog content is generated by the Hugo static site generator and integrated with GitHub (which you can clearly tell by the URL), but I\u0026rsquo;m always struggling to remember the steps in pushing the newest content publicly.\nBut I\u0026rsquo;m getting ahead of myself. Before I get there, hugo and github need to be integrated. I\u0026rsquo;m using Windows 10, so I needed to start by installing git locally. Once that was complete, I had the git bash shell that I could use to generate the SSH keys that my computer will use to connect with my github account. On Windows 10, the .ssh directory is located at C:\\Users\\Administrator.ssh\nStart by generating a new SSH key to use for this connection. I used the ed25519 algorithm, which is faster and more \u0026lsquo;secure\u0026rsquo; than the RSA-2048 key that\u0026rsquo;s commonly used today:\nAdministrator@AC08871 MINGW64 ~/.ssh $ ssh-keygen -t ed25519 -C \u0026#34;email@email.com\u0026#34; Then, copy the newly generated public key to the clipboard, because we\u0026rsquo;re going to paste it to github:\nAdministrator@AC08871 MINGW64 ~/.ssh $ clip \u0026lt; id_ed25519.pub Log into github \u0026gt; Settings \u0026gt; SSH \u0026amp; GPG Keys. Create a new SSH key, and paste in the contents of the clipboard (which is the public key I just created)\nTest the connection via. git bash shell:\nAdministrator@AC08871 MINGW64 ~/.ssh $ ssh -T git@github.com Hi gbeifuss! You\u0026#39;ve successfully authenticated, but GitHub does not provide shell access. Now that the connection between my computer and github is established, let\u0026rsquo;s configure github as a remote repository:\nC:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt; git remote add origin https://github.com/gbeifuss/gbeifuss.github.io.git C:\\Users\\Greg\\OneDrive\\myblog.dev.repo\\public\u0026gt;git remote -v origin https://github.com/gbeifuss/gbeifuss.github.io.git (fetch) origin https://github.com/gbeifuss/gbeifuss.github.io.git (push) Finally, I\u0026rsquo;m ready to push my content up:\nChange to your site\u0026rsquo;s local directory, then tell Hugo to build the content:\ncd myblog.dev.repo\nhugo\nChange to your site\u0026rsquo;s public directory (where the new content was just generated):\ncd public\nAdd all of the new files to git. This command analyzes the repository files, adding all modified and/or untracked files in the current directory and subdirectories to staged status. This prepares them for inclusion in the next git commit. The .gitignore file can be used to define any files/patterns that should be skipped:\ngit add .\nCommit the locally staged files with a comment. This command will add all the changed files, so be aware of that if you\u0026rsquo;re only intending to commit/push select files!:\ngit commit -m \u0026quot;Website update 2021-08-03\u0026quot;\nPush the local content up to GitHub. origin is the remote repository name (alias) and main is the branch name. (Since October 2020, main is the default branch that must exist on any repository):\ngit push origin main\nAnd here it all is in a glorious code block, so that in the future, I can just copy and paste!\ncd myblog.dev.repo hugo cd public git add . git commit -m \u0026#34;Website update 2021-08-03\u0026#34; git push origin main I\u0026rsquo;ve also been trying to sort out how to update the theme I use (Stack), which is periodically updated. Right now my process is to:\nlog into github and pull the latest release into my fork delete the contents of \\themes\\hugo-stack-theme on my computer tell git to repopulate the directory: \\myblog.dev.repo\\themes\u0026gt; git clone git@github.com:gbeifuss/hugo-theme-stack hugo-theme-stack Cloning into \u0026#39;hugo-theme-stack\u0026#39;... remote: Enumerating objects: 3111, done. remote: Counting objects: 100% (1026/1026), done. remote: Compressing objects: 100% (416/416), done. remote: Total 3111 (delta 796), reused 695 (delta 601), pack-reused 2085 Receiving objects: 100% (3111/3111), 841.36 KiB | 2.85 MiB/s, done. Resolving deltas: 100% (1958/1958), done. \\myblog.dev.repo\\themes\u0026gt; recopy my avatar image to \\themes\\hugo-stack-theme\\assets\\img\\ ","date":"2021-08-04T07:49:02-04:00","permalink":"https://gbeifuss.github.io/p/pushing-hugo-content-to-github/","title":"Pushing Hugo Content to GitHub"},{"content":"A popular and quick way to work with arrays in Powershell is to use a += operation:\n$array = @() 1..10000 | Foreach-Object {$array += \u0026#34;Adding $_ to the array\u0026#34;} Let\u0026rsquo;s time this:\n$array = @() $stopwatch = [System.Diagnostics.Stopwatch]::StartNew() 1..100000 | Foreach-Object {$array += \u0026#34;Adding $_ to the array\u0026#34;} $report = \u0026#39;{0} elements collected in {1:n1} seconds\u0026#39; $report -f $array.Count, $stopwatch.Elapsed.TotalSeconds Processing these 100,000 records took my system 214.7 seconds. When Powershell code uses the += operator, it actually creates a new array that\u0026rsquo;s one record larger than the current array, copies each existing entry into the new array and then adds the newest entry into the free record. It has to do this every time the += operator is used, which is 100,000 times in this sample code. The last time it runs, it needs to copy 99,999 existing entries into a new array\u0026hellip;.ouch! It\u0026rsquo;s easy to see why this is so slow!\nLet\u0026rsquo;s use a .NET method to do this and see whether it\u0026rsquo;s any faster:\n$array = [System.Collections.ArrayList]@() $stopwatch = [System.Diagnostics.Stopwatch]::StartNew() 1..100000 | Foreach-Object { $null = $array.Add(\u0026#34;Adding $_ to the array\u0026#34;)} $report = \u0026#39;{0} elements collected in {1:n1} seconds\u0026#39; $report -f $array.Count, $stopwatch.Elapsed.TotalSeconds Instead of 214.7 seconds, this takes 0.4 seconds - a 53,575% speed increase with next to no extra work.\nTechnically, I should have used [System.Collections.Generic.List] instead, as that\u0026rsquo;s what Microsoft recommends.\nYou can do this in pure Powershell too, but it only handles one item per iteration. Because you\u0026rsquo;re returning more than one element, Powershell will automatically create an array. This method also took 0.4 seconds.\n$stopwatch = [System.Diagnostics.Stopwatch]::StartNew() $array = 1..100000 | Foreach-Object {\u0026#34;Adding $_ to the array\u0026#34;} $report = \u0026#39;{0} elements collected in {1:n1} seconds\u0026#39; $report -f $array.Count, $stopwatch.Elapsed.TotalSeconds Both the .NET method and Powershell\u0026rsquo;s native array-creation can append arrays without having to recreate/repopulate them. That\u0026rsquo;s why they\u0026rsquo;re equally fast, and both far faster than +=.\nNow, += isn\u0026rsquo;t always evil. It\u0026rsquo;s useful for adding numbers. However, once you start working with array objects, you incur the performance penalty. If you use += on a string, you incur the performance penalty since a string is really an array of characters. If you\u0026rsquo;re working with small arrays and the performance hit is acceptable, then there\u0026rsquo;s no real harm in using it.\n","date":"2021-07-22T22:10:02-04:00","permalink":"https://gbeifuss.github.io/p/array-speed-in-powershell/","title":"Array Speed in Powershell"},{"content":"Often, I\u0026rsquo;ll see scripts that use Measure-Command to calculate the length of time that code runs, while other code will use the $stopwatch = [System.Diagnostics.Stopwatch]::StartNew() method. I\u0026rsquo;d always thought that these were interchangable and practically identical, however, I\u0026rsquo;ve found out that they\u0026rsquo;re not. Depending on the code, there can be substantial time differences between the two.\nMeasure-Command will discard the pipeline output, measuring only the time it takes to actually execute the command.\nThe stopwatch timer measures everything, including the time it takes to display any output to the console.\nTo have Measure-Command to measure the time needed to write to the console, add an explicit pipe to Out-Host inside the script block that you\u0026rsquo;re measuring. This will write the output to the console, and cause a corresponding increase in the measured time.\n","date":"2021-07-22T22:09:50-04:00","permalink":"https://gbeifuss.github.io/p/powershell-stopwatch-vs-measure-command/","title":"Powershell Stopwatch vs Measure-Command"},{"content":"The other day I started a remote powershell session with Invoke-Command and my script ran\u0026hellip;\nand ran\u0026hellip;\nand ran\u0026hellip;\nI finally tired of waiting, so I decided that I\u0026rsquo;d RDP in and get the information via. GUI, but the server was sluggish. This isn\u0026rsquo;t an underspec\u0026rsquo;ed server where routine use might cause a small but noticable performance hit. I took a look at taskman and saw that the wsmprovhost.exe process was consuming 50/64GB RAM.\nIt turns out that this is the process responsible for remote Powershell sessions. While I could just kill it, I wanted to see what is going on behind the scenes.\nFirst, I grabbed the URI for WSMan connections:\nPS C:\\\u0026gt; $URI = (\u0026#39;http://{0}:5985/wsman\u0026#39; -f $env:computername); $URI http://SERVER:5985/wsman Then, I listed the connections to that URI. I could have filtered on the sessions owned by my user ID to avoid impacting others who might have active sessions, but I\u0026rsquo;m the only IT staff who uses Powershell, so that\u0026rsquo;s unnecessary:\nPS C:\\\u0026gt; $connection = Get-WSManInstance -ConnectionURI $URI -ResourceURI shell -Enumerate; $connection rsp : http://schemas.microsoft.com/wbem/wsman/1/windows/shell lang : en-US ShellId : 4C1DF700-45AE-4322-8FAC-E4E688F3F52C Name : Runspace10 ResourceUri : http://schemas.microsoft.com/powershell/Microsoft.PowerShell Owner : DOMAIN\\USERNAME ClientIP : 192.168.0.1 ProcessId : 20956 IdleTimeOut : PT7200.000S InputStreams : stdin pr OutputStreams : stdout MaxIdleTimeOut : PT2147483.647S Locale : en-US DataLocale : en-US CompressionMode : XpressCompression ProfileLoaded : Yes Encoding : UTF8 BufferMode : Block State : Connected ShellRunTime : P3DT2H43M28S ShellInactivity : P0DT0H1M21S MemoryUsed : 6MB ChildProcesses : 0 rsp : http://schemas.microsoft.com/wbem/wsman/1/windows/shell lang : en-US ShellId : 3A169BBF-C590-4D36-A055-88ED9B9F741C Name : Runspace24 ResourceUri : http://schemas.microsoft.com/powershell/Microsoft.PowerShell Owner : DOMAIN\\USERNAME ClientIP : 192.168.0.1 ProcessId : 18776 IdleTimeOut : PT7200.000S InputStreams : stdin pr OutputStreams : stdout MaxIdleTimeOut : PT2147483.647S Locale : en-US DataLocale : en-US CompressionMode : XpressCompression ProfileLoaded : Yes Encoding : UTF8 BufferMode : Block State : Connected ShellRunTime : P0DT15H38M57S ShellInactivity : P0DT0H0M27S MemoryUsed : 48818MB ChildProcesses : 0 rsp : http://schemas.microsoft.com/wbem/wsman/1/windows/shell lang : en-US ShellId : 337E488F-5385-485B-A74B-DDF5FB76AEF5 Name : Runspace17 ResourceUri : http://schemas.microsoft.com/powershell/Microsoft.PowerShell Owner : DOMAIN\\USERNAME ClientIP : 192.168.0.1 ProcessId : 11288 IdleTimeOut : PT7200.000S InputStreams : stdin pr OutputStreams : stdout MaxIdleTimeOut : PT2147483.647S Locale : en-US DataLocale : en-US CompressionMode : XpressCompression ProfileLoaded : Yes Encoding : UTF8 BufferMode : Block State : Connected ShellRunTime : P3DT0H19M58S ShellInactivity : P0DT0H1M49S MemoryUsed : 12MB ChildProcesses : 0 Bingo! There\u0026rsquo;s the session consuming 48818MB of RAM. I don\u0026rsquo;t recall what my other sessions are for, so I\u0026rsquo;ll just kill them all.\n$connection | ForEach-Object { Remove-WSManInstance -ConnectionURI $URI shell @{ShellID=$_.ShellID} } After a few seconds, taskman showed that the wsmprovhost.exe process was rapidly releasing RAM:\n50GB\u0026hellip;\n37GB\u0026hellip;\n24GB\u0026hellip;\n12GB\u0026hellip;\nprocess ended.\nAnd with that, the server was back to being its usual snappy self.\n","date":"2021-07-22T22:09:32-04:00","permalink":"https://gbeifuss.github.io/p/powershell-unresponsive-remote-sessions/","title":"Powershell \u0026 Unresponsive Remote Sessions"},{"content":"I was recently curious about what sort of SSL/TLS connections were being used to access our Exchange cluster via. IIS. We have systems spanning twenty years: from Windows XP all the way to Windows Server 2019. Who knows what \u0026ldquo;solutions\u0026rdquo; developers or power users have come up with over the years to integrate with our mail system?\nPreviously, I\u0026rsquo;d followed MS guidance in Enabling TLS 1.2 and New IIS Functionality to Help Identify Weak TLS Usage. The IIS blog explains how to set 4 server variables in IIS to track security protocols and cipher suites:\nCRYPT_PROTOCOL CRYPT_CIPHER_ALG_ID CRYPT_HASH_ALG_ID CRYPT_KEYEXCHANGE_ALG_ID First, I needed to figure out what was actually being recorded by the logs.\n#Date: 2021-03-04 00:00:03 #Fields: date time s-ip cs-method cs-uri-stem cs-uri-query s-port cs-username c-ip cs(User-Agent) cs(Referer) sc-status sc-substatus sc-win32-status time-taken crypt-protocol crypt-cipher crypt-hash crypt-keyexchange OriginalClientIP 2021-03-03 23:59:55 10.10.1.103 POST /EWS/Exchange.asmx \u0026amp;CorrelationID=\u0026lt;empty\u0026gt;;\u0026amp;cafeReqId=7a4501f0-3d61-425d-8d1f-0c3d5c9e6d13; 443 DOMAIN\\webmail 10.10.1.58 ExchangeServicesClient/15.00.0913.015 - 200 0 0 38 400 660e 800c ae06 - 2021-03-03 23:59:55 10.10.1.103 POST /EWS/Exchange.asmx \u0026amp;CorrelationID=\u0026lt;empty\u0026gt;;\u0026amp;cafeReqId=42acb956-ea0a-4a32-a097-0bc73ae92860; 443 - 10.10.12.120 ExchangeServicesClient/15.00.0913.015 - 401 1 2148074254 1 400 660e 800c ae06 - 2021-03-03 23:59:55 10.10.1.103 POST /EWS/Exchange.asmx \u0026amp;CorrelationID=\u0026lt;empty\u0026gt;;\u0026amp;cafeReqId=0e312297-90fc-4b4d-8a28-24c3c60bcb7e; 443 - 10.10.12.120 ExchangeServicesClient/15.00.0913.015 - 401 1 2148074254 2 400 660e 800c ae06 - The information I needed was in the 15th field (crypt-protocol). A value of \u0026lsquo;400\u0026rsquo; correlates to TLS 1.2, \u0026lsquo;40\u0026rsquo; represents TLS 1.0, \u0026lsquo;10\u0026rsquo; represents SSLv3 and so forth. There\u0026rsquo;s Microsoft documentation that explains the various values.\nSo, let\u0026rsquo;s write some sloppy proof-of-concept code to parse the IIS log files on 1 server. We can then wrap it in a foreach statement, or something like that, to query all the servers once it\u0026rsquo;s working. Let\u0026rsquo;s do some rough calculations on how long this will take by wrapping it in a measure-command:\n$Protocol = @() Measure-Command { Import-Csv c:\\inetpub\\logs\\LogFiles\\W3SVC1\\u_ex210713_x.log -Delimiter \u0026#39; \u0026#39; -Header \u0026#39;date\u0026#39;, \u0026#39;time\u0026#39;, s-ip, cs-method, cs-uri-stem, cs-uri-query, s-port, cs-username, c-ip, \u0026#39;cs(user-Agent)\u0026#39;, \u0026#39;cs(Referer)\u0026#39;, sc-status, sc-substatus, sc-win32-status, time-taken, crypt-protocol, crypt-cipher, crypt-hash, crypt-keyexchange, OriginalClientIP | ForEach-Object { $Protocol += $_.\u0026#39;crypt-protocol\u0026#39; } } \u0026#34;$((Get-Content C:\\inetpub\\logs\\LogFiles\\W3SVC1\\u_ex210713_x.log).count) lines processed.\u0026#34; Days : 0 Hours : 0 Minutes : 17 Seconds : 1 Milliseconds : 253 Ticks : 10212530494 TotalDays : 0.0118200584421296 TotalHours : 0.283681402611111 TotalMinutes : 17.0208841566667 TotalSeconds : 1021.2530494 TotalMilliseconds : 1021253.0494 This command took 17 minutes to run against one file on one server! I have 10 log files on each server (I wrote a separate script to automatically purge files older than that in order to save disk space). Aside from this script seeming ridiculously slow, it\u0026rsquo;ll be a whole day before my curiosity is satisfied - this simply won\u0026rsquo;t do!\nInstead of the slow process of reading the whole file into memory, enumerating the columns and adding data to an array that we\u0026rsquo;re constantly resizing, let\u0026rsquo;s use a faster method. I\u0026rsquo;ll read in 1000 rows at a time, split each row to find the 15th column, exclude irrelevant information, and add those results to an array.\nMeasure-Command { $Protocol = @() $nlines = 0 Get-Content C:\\inetpub\\logs\\LogFiles\\W3SVC1\\u_ex210713_x.log -ReadCount 1000 | ForEach-Object { ForEach ($line in $_) { If ($line.Split(\u0026#39; \u0026#39;)[15] -ne \u0026#34;400\u0026#34; -and $line.Split(\u0026#39; \u0026#39;)[15] -ne \u0026#34;-\u0026#34; -and $line.Split(\u0026#39; \u0026#39;)[15] -ne \u0026#34;time-taken\u0026#34; -and $line.split(\u0026#39; \u0026#39;)[8] -ne \u0026#34;::1\u0026#34; -and $line.split(\u0026#39; \u0026#39;)[8] -ne \u0026#34;127.0.0.1\u0026#34;) { $Protocol += $line.Split(\u0026#39; \u0026#39;)[8] } } $nlines += $_.Length } } $Protocol | Group-Object [string]::Format(\u0026#34;{1} lines\u0026#34;, $nlines) } Days : 0 Hours : 0 Minutes : 12 Seconds : 14 Milliseconds : 149 Ticks : 7341497722 TotalDays : 0.00849710384490741 TotalHours : 0.203930492277778 TotalMinutes : 12.2358295366667 TotalSeconds : 734.1497722 TotalMilliseconds : 734149.7722 Well, 12 minutes is somewhat better - a 30% improvement - but still not good enough. Let\u0026rsquo;s get rid of the two tremendously slow array additions, which even activate in conjunction, slowing things down further. Arrays are fixed sizes. When you use the += operator, PowerShell actually creates a new array with the values of the original array plus the added value. Each time I add a value, the entire existing array is replicated into a new array with one more value. It\u0026rsquo;s terribly inefficient when you have large amounts of data like the 129,102 entries in this file.\nLet\u0026rsquo;s use a .NET method to fix this speed issue. An ArrayList with a .Add() method is far more efficient for what I\u0026rsquo;m doing.\nMeasure-Command { $Protocol = [System.Collections.ArrayList]@() Get-Content C:\\inetpub\\logs\\LogFiles\\W3SVC1\\u_ex210713_x.log -ReadCount 1000 | ForEach-Object { If ($_.Split(\u0026#39; \u0026#39;)[15] -ne \u0026#34;400\u0026#34; -and $_.Split(\u0026#39; \u0026#39;)[15] -ne \u0026#34;-\u0026#34; -and $_.Split(\u0026#39; \u0026#39;)[15] -ne \u0026#34;time-taken\u0026#34; -and $_.split(\u0026#39; \u0026#39;)[8] -ne \u0026#34;::1\u0026#34; -and $_.split(\u0026#39; \u0026#39;)[8] -ne \u0026#34;127.0.0.1\u0026#34;) { $null = $Protocol.Add($_.Split(\u0026#39; \u0026#39;)[8]) } } } Days : 0 Hours : 0 Minutes : 0 Seconds : 8 Milliseconds : 314 Ticks : 83145768 TotalDays : 9.62335277777778E-05 TotalHours : 0.00230960466666667 TotalMinutes : 0.13857628 TotalSeconds : 8.3145768 TotalMilliseconds : 8314.5768 A 12180% speed increase from my first code is much better. Now let\u0026rsquo;s run this against all 10 files in the directory, then dump the statistics I\u0026rsquo;m interested in:\nMeasure-Command { $Protocol = [System.Collections.ArrayList]@() Get-Content C:\\W3SVC1\\*.log -ReadCount 1000 | ForEach-Object { If ($_.Split(\u0026#39; \u0026#39;)[15] -ne \u0026#34;400\u0026#34; -and $_.Split(\u0026#39; \u0026#39;)[15] -ne \u0026#34;-\u0026#34; -and $_.Split(\u0026#39; \u0026#39;)[15] -ne \u0026#34;time-taken\u0026#34; -and $_.split(\u0026#39; \u0026#39;)[8] -ne \u0026#34;::1\u0026#34; -and $_.split(\u0026#39; \u0026#39;)[8] -ne \u0026#34;127.0.0.1\u0026#34;) { $null = $Protocol.Add($_.Split(\u0026#39; \u0026#39;)[8]) } } } $Protocol | Group-Object | Select-Object Count, Name \u0026#34;$($Protocol.count) matching entries in $((Get-Content C:\\W3SVC1\\*.log).count) processed lines.\u0026#34; Days : 0 Hours : 0 Minutes : 1 Seconds : 8 Milliseconds : 7 Ticks : 680073775 TotalDays : 0.000787122424768519 TotalHours : 0.0188909381944444 TotalMinutes : 1.13345629166667 TotalSeconds : 68.0073775 TotalMilliseconds : 68007.3775 PS C:\\\u0026gt; $Protocol | Group-Object | Select-Object Count, Name Count Name ----- ---- 4 10.1.0.32 1340 10.1.1.250 8 169.254.3.178 85 172.16.0.12 PS C:\\\u0026gt; \u0026#34;$($Protocol.count) matching entries in $((Get-Content C:\\W3SVC1\\*.log).count) processed lines.\u0026#34; 1445 matching entries in 1589761 processed lines. This code for all 10 files is 1400% faster than the code I started with to read a single file. There are 4 IPs connecting to Exchange using something other than TLS 1.2, but they\u0026rsquo;re all private IPs for the Exchange cluster itself, or the replication network.\n","date":"2021-07-19T16:53:03-04:00","permalink":"https://gbeifuss.github.io/p/powershell-exchange-tls/","title":"Powershell, Exchange \u0026 TLS"},{"content":"Since CVE-2021 is the news item of the week, I decided that I should scan our infrastructure using byt3bl33d3r\u0026rsquo;s CVE-2021-34527 Python Scanner. I learned Docker basics on the fly with this.\nDownload Docker Desktop for Windows. Reboot if necessary.\nInstall WSL2, as prompted. Reboot if necessary.\nCreate a new directory from the CLI and clone the repo: git clone https://github.com/byt3bl33d3r/ItWasAllADream\nThis gave me a small problem:\nD:\\Docker\u0026gt;git clone https://github.com/byt3bl33d3r/ItWasAllADream Cloning into \u0026#39;ItWasAllADream\u0026#39;... fatal: unable to access \u0026#39;https://github.com/byt3bl33d3r/ItWasAllADream/\u0026#39;: SSL certificate problem: unable to get local issuer certificate Since I\u0026rsquo;m pulling from github.com, which has a valid certificate, and not some private repo with a self-signed certificate, I realised pretty quickly that this was likely due to our corporate web-filtering software. I told git to bypass certificate checking, and everything processed properly:\nD:\\Docker\u0026gt;git config --global http.sslVerify false D:\\docker\u0026gt;git clone https://github.com/byt3bl33d3r/ItWasAllADream Cloning into \u0026#39;ItWasAllADream\u0026#39;... remote: Enumerating objects: 57, done. remote: Counting objects: 100% (57/57), done. remote: Compressing objects: 100% (38/38), done. remote: Total 57 (delta 29), reused 45 (delta 17), pack-reused 0 eceiving objects: 56% (32/57) Receiving objects: 100% (57/57), 51.22 KiB | 8.54 MiB/s, done. Resolving deltas: 100% (29/29), done. D:\\docker\u0026gt;git config --global http.sslVerify true Build the docker container: cd ItWasAllADream \u0026amp;\u0026amp; docker build -t itwasalladream .\nThis gave me bigger problems:\nD:\\docker\u0026gt;cd ItWasAllADream \u0026amp;\u0026amp; docker build -t itwasalladream . [+] Building 117.3s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.1s =\u0026gt; =\u0026gt; transferring dockerfile: 329B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 53B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/python:3.8-slim 28.8s =\u0026gt; [1/4] FROM docker.io/library/python:3.8-slim@sha256:9b0d7419e2811710aacee87c40a2c94693e2b6810c3e7e466b8c7fc5bde4cd66 30.5s =\u0026gt; =\u0026gt; resolve docker.io/library/python:3.8-slim@sha256:9b0d7419e2811710aacee87c40a2c94693e2b6810c3e7e466b8c7fc5bde4cd66 0.0s =\u0026gt; =\u0026gt; sha256:3bc519ca3214d463ac521f838275e1070ab4b8dbb12e568b4739794db837dadb 1.37kB / 1.37kB 0.0s =\u0026gt; =\u0026gt; sha256:0e0d73ddd34d599cf70c5855f18135e51a9cba957b4cd41a7c677d0d3cb4edc2 7.67kB / 7.67kB 0.0s =\u0026gt; =\u0026gt; sha256:b4d181a07f8025e00e0cb28f1cc14613da2ce26450b80c54aea537fa93cf3bda 27.15MB / 27.15MB 26.0s =\u0026gt; =\u0026gt; sha256:de8ecf497b753094723ccf9cea8a46076e7cb845f333df99a6f4f397c93c6ea9 2.77MB / 2.77MB 4.9s =\u0026gt; =\u0026gt; sha256:6ea9cb12457214a5eda82bc4aad8fc1b20c0cee03042d4b6b6da946a94af3274 10.73MB / 10.73MB 29.1s =\u0026gt; =\u0026gt; sha256:9b0d7419e2811710aacee87c40a2c94693e2b6810c3e7e466b8c7fc5bde4cd66 1.86kB / 1.86kB 0.0s =\u0026gt; =\u0026gt; sha256:9a8aa9d08ec5d2bb15de97e33b599bb4472ccd7d58fe4f15f0dfd7d4fd08fa6d 234B / 234B 6.2s =\u0026gt; =\u0026gt; sha256:360b2e4ced966675f720b48dee317e200ed394abc8b95609a31bfb5225b4f746 2.64MB / 2.64MB 12.9s =\u0026gt; =\u0026gt; extracting sha256:b4d181a07f8025e00e0cb28f1cc14613da2ce26450b80c54aea537fa93cf3bda 1.9s =\u0026gt; =\u0026gt; extracting sha256:de8ecf497b753094723ccf9cea8a46076e7cb845f333df99a6f4f397c93c6ea9 0.2s =\u0026gt; =\u0026gt; extracting sha256:6ea9cb12457214a5eda82bc4aad8fc1b20c0cee03042d4b6b6da946a94af3274 0.6s =\u0026gt; =\u0026gt; extracting sha256:9a8aa9d08ec5d2bb15de97e33b599bb4472ccd7d58fe4f15f0dfd7d4fd08fa6d 0.0s =\u0026gt; =\u0026gt; extracting sha256:360b2e4ced966675f720b48dee317e200ed394abc8b95609a31bfb5225b4f746 0.3s =\u0026gt; [internal] load build context 1.1s =\u0026gt; =\u0026gt; transferring context: 194.50kB 1.0s =\u0026gt; [2/4] RUN apt-get update \u0026amp;\u0026amp; apt-get install --no-install-recommends -y git curl \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* 47.0s =\u0026gt; [3/4] COPY . . 0.0s =\u0026gt; ERROR [4/4] RUN pip3 install poetry \u0026amp;\u0026amp; poetry config virtualenvs.create false \u0026amp;\u0026amp; poetry install 10.9s ------ \u0026gt; [4/4] RUN pip3 install poetry \u0026amp;\u0026amp; poetry config virtualenvs.create false \u0026amp;\u0026amp; poetry install: #8 2.071 WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;SSLError(SSLCertVerificationError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\u0026#39;))\u0026#39;: /simple/poetry/ #8 2.648 WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;SSLError(SSLCertVerificationError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\u0026#39;))\u0026#39;: /simple/poetry/ #8 3.725 WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;SSLError(SSLCertVerificationError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\u0026#39;))\u0026#39;: /simple/poetry/ #8 5.796 WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;SSLError(SSLCertVerificationError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\u0026#39;))\u0026#39;: /simple/poetry/ #8 10.30 WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by \u0026#39;SSLError(SSLCertVerificationError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\u0026#39;))\u0026#39;: /simple/poetry/ #8 10.63 Could not fetch URL https://pypi.org/simple/poetry/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host=\u0026#39;pypi.org\u0026#39;, port=443): Max retries exceeded with url: /simple/poetry/ (Caused by SSLError(SSLCertVerificationError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\u0026#39;))) - skipping #8 10.64 ERROR: Could not find a version that satisfies the requirement poetry (from versions: none) #8 10.64 ERROR: No matching distribution found for poetry #8 10.79 Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host=\u0026#39;pypi.org\u0026#39;, port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, \u0026#39;[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\u0026#39;))) - skipping ------ executor failed running [/bin/sh -c pip3 install poetry \u0026amp;\u0026amp; poetry config virtualenvs.create false \u0026amp;\u0026amp; poetry install]: exit code: 1 I realised that since this was being done outside of git, my earlier config changes would have no effect. I bypassed the web filtering software entirely (a benefit of being a sysadmin), and Docker went on its merry way:\nD:\\docker\u0026gt;cd ItWasAllADream \u0026amp;\u0026amp; docker build -t itwasalladream . [+] Building 42.3s (9/9) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 329B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 53B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/python:3.8-slim 9.4s =\u0026gt; [1/4] FROM docker.io/library/python:3.8-slim@sha256:9b0d7419e2811710aacee87c40a2c94693e2b6810c3e7e466b8c7fc5bde4cd66 0.0s =\u0026gt; [internal] load build context 0.1s =\u0026gt; =\u0026gt; transferring context: 194.50kB 0.1s =\u0026gt; CACHED [2/4] RUN apt-get update \u0026amp;\u0026amp; apt-get install --no-install-recommends -y git curl \u0026amp;\u0026amp; rm -rf /var/lib/apt/l 0.0s =\u0026gt; [3/4] COPY . . 0.0s =\u0026gt; [4/4] RUN pip3 install poetry \u0026amp;\u0026amp; poetry config virtualenvs.create false \u0026amp;\u0026amp; poetry install 31.5s =\u0026gt; exporting to image 1.2s =\u0026gt; =\u0026gt; exporting layers 1.1s =\u0026gt; =\u0026gt; writing image sha256:717ceee523a84d3c659caecce847883ca2bf123b1e3af428388c7047fee712a8 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/itwasalladream 0.0s Use \u0026#39;docker scan\u0026#39; to run Snyk tests against images to find vulnerabilities and learn how to fix them Run a scan and wait for it to finish: docker run -it itwasalladream -u user -p password -d domain 192.168.1.0/24\n[itwasalladream] INFO - report_2021_07_08_155428.csv generated successfully List the containers so that we can get the container name. docker ps -a\nD:\\docker\\ItWasAllADream\u0026gt;docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4793adbf54d7 itwasalladream \u0026#34;itwasalladream -u g…\u0026#34; 25 minutes ago Exited (0) 18 minutes ago sad_taussig f36834af30d0 itwasalladream \u0026#34;itwasalladream -u g…\u0026#34; 36 minutes ago Exited (0) 33 minutes ago stupefied_banach 7fbeb77ce498 itwasalladream \u0026#34;itwasalladream -u g…\u0026#34; 59 minutes ago Exited (0) 58 minutes ago intelligent_allen Copy the resulting CSV out of the container and to the current local directory\nD:\\docker\\ItWasAllADream\u0026gt;docker cp sad_taussig:report_2021_07_08_155428.csv . Review the CSV: notepad report_2021_07_08_155428.csv\nGreg\n","date":"2021-07-14T08:21:50-04:00","permalink":"https://gbeifuss.github.io/p/docker-printnightmare/","title":"Docker \u0026 Printnightmare"},{"content":"I spend a lot of my day in VS Code, but not too much in GitHub because I don\u0026rsquo;t work on any joint projects. I do use a local Git repository for my code. It\u0026rsquo;s easy to see when others have made changes so that I can hunt them down speak politely to them about stopping.\nIn any case, I was watching An Introduction to Visual Studio Code and GitHub when I learned about the Remote Repositories (GitHub) extension for VS Code. You can directly edit a repository on GitHub without having to clone it locally.\nDisclaimer I must confess that in the process of using Remote Repository access this week, I managed to lose/erase 6 files that I hadn\u0026rsquo;t committed, but maybe you\u0026rsquo;re more of a git savant than I am, and you\u0026rsquo;ll have no problems.\nLet\u0026rsquo;s look at how to get this installed and integrated with VS Code:\nLoad VS Code Open Extensions (CTRL+SHIFT+X) Search for Remote Repositories (GitHub). Install it. Open the Command Palette (CTRL+SHIFT+P) Type in Remote Repositories: Open Repository Select Open Repository from GitHub You\u0026rsquo;ll be prompted to allow access to GitHub. Allow it. A browser window will open. Click Continue to authorize access to GitHub. Sign in to GitHub. If you don\u0026rsquo;t already have an account, there won\u0026rsquo;t be any repositories for you to open in VS Code, will there? You should receive a \u0026lsquo;Success!\u0026rsquo; webpage, with an authorization token on it. If your browser prompts you to Open Visual Studio Code, do so. It\u0026rsquo;ll save you the trouble of manually entering the authorization code. Once VS Code opens again, you\u0026rsquo;ll be prompted to Allow an extension to open this URI. Permit it, and VS Code will complete the link. If the browser didn\u0026rsquo;t prompt, copy the authorization token. You\u0026rsquo;ll have to paste it into VS Code manually. The Command Palette field should be asking for the token, so paste it in. VS Code should now be able to open your repositories directly on GitHub. ","date":"2021-07-14T08:21:37-04:00","permalink":"https://gbeifuss.github.io/p/vs-code-remote-repositories/","title":"VS Code \u0026 Remote Repositories"},{"content":"I\u0026rsquo;ve worked with computers for a couple of decades now, and I love Notepad\u0026hellip; #sorrynotsorry, vim fans. It\u0026rsquo;s fantastic for writing - there\u0026rsquo;s no fancy GUI needing customization to fit your workflow, and offers just enough structure that it\u0026rsquo;s usable for a variety of tasks. It\u0026rsquo;s like the pre-markdown version of markdown. My favourite feature of Notepad is the ability to insert the date \u0026amp; time via. F5. This is a brilliant and elegant (and often overlooked) feature. Word\u0026rsquo;s insert date/time function pales in comparison to the elegant simplicty of F5. Even trying to use keyboard shortcuts isn\u0026rsquo;t fast, when it should be.\nNotepad Word GUI Work shortcuts F5 click Insert ALT+I click Date/Time D read the menu read the menu double-click choice UP/DOWN ARROW to choice ENTER When I write Powershell scripts in VS Code, I had been manually typing in the date until I decided to write a custom snippet that would insert a header with all the things I normally add: script name, date/time, comments, my name. It\u0026rsquo;s automatically populated whenever I type in header \u0026lt;ENTER\u0026gt;\nOpen the Command Palette (View, Command Palette\u0026hellip;. or CTRL+SHIFT+P) Select Preferences: Configure User Snippets Select Powershell from the list Add the snippet: \u0026#34;File Header\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;header\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Output a file header with the file name, comment field, author and date\u0026#34;, \u0026#34;body\u0026#34;: [ \u0026#34;#$TM_FILENAME\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026lt;#\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;Greg Beifuss\u0026#34;, \u0026#34;$CURRENT_YEAR-$CURRENT_MONTH-$CURRENT_DATE $CURRENT_HOUR:$CURRENT_MINUTE\u0026#34;, \u0026#34;#\u0026gt;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, ] } Since I\u0026rsquo;m using VS Code to write markdown now, I decided to do a similar thing to reproduce Notepad\u0026rsquo;s F5 feature:\nOpen the Command Palette (View, Command Palette\u0026hellip;. or CTRL+SHIFT+P) Select Preferences: Configure User Snippets Select Markdown from the list Add the snippet: \u0026#34;Current Date and Time\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;datetime\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Add Current date \u0026amp; time\u0026#34;, \u0026#34;body\u0026#34;: [ \u0026#34;$CURRENT_YEAR-$CURRENT_MONTH-$CURRENT_DATE $CURRENT_HOUR:$CURRENT_MINUTE\u0026#34; ] } This snippet seems to be more tempermental - when I type in datetime \u0026lt;ENTER\u0026gt;, usually it\u0026rsquo;s not replaced with the date/timestamp. I resort to:\n1. CTRL+I 2. datetime \u0026lt;ENTER\u0026gt; ","date":"2021-07-14T08:20:02-04:00","permalink":"https://gbeifuss.github.io/p/vs-code-snippets/","title":"VS Code Snippets"},{"content":"I\u0026rsquo;m a big fan of Michael Grafnetter\u0026rsquo;s excellent DSInternals module for Powershell. I periodically use it to audit our AD instance, and pass the results along to our Security Admin for any followup that needs to happen. I usually use Have I Been Pwned\u0026rsquo;s password dump, but you can provide any file containing passwords. The scariest part is that it takes \u0026lt;5 minutes to compare your AD records against 613 million passwords\u0026hellip; and most of that time is enumerating the AD accounts.\nHere\u0026rsquo;s how to run it against your AD instance:\nDownload the latest NTML (ordered by Hash) dump from Have I Been Pwned? Load Powershell. Install the module: Install-Module DSInternals -Force Run the cmdlets: $DictFile = \u0026#34;d:\\pwned-passwords-ntlm-ordered-by-hash-v5.txt\u0026#34; $DC = \u0026#34;dc01\u0026#34; $Domain = \u0026#34;DC=company,DC=com\u0026#34; Get-ADReplAccount -All -Server $DC -NamingContext $Domain | Test-PasswordQuality -WeakPasswordHashesSortedFile $DictFile -IncludeDisabledAccounts Older versions of the tool used to output the passwords that were found in the dictionary you provided (ie. the HIBP dump), but no longer.\n","date":"2021-07-12T15:38:54-04:00","permalink":"https://gbeifuss.github.io/p/powershell-dsinternals-hibp/","title":"Powershell, DSInternals \u0026 HIBP"},{"content":"I do a fair amount of script writing to make my life easier. If it\u0026rsquo;s a repeated job, I try and automate it so that my time is freed up for more important (interesting) things. This is especially true if the task is tedious (with many manual steps) or involves lots of waiting around (like an Exchange CU). I have a fair number of Powershell scripts which run from Task Scheduler under Windows Server 2016. There are a lot of options to configure, so this can be complex. I\u0026rsquo;ll share the cheatcode way of doing this at the end.\nLoad Task Scheduler. Click Create Task Under the General tab, fill in a Name and Description. We\u0026rsquo;ll circle back to the other options at the end. Click Triggers. Most of my jobs run on a time-based schedule, not events (like computer startup). Setup the schedule that you need. Under the Actions tab, click New. Another window will appear. Leave the Action as Start a program.\nIf the script runs under Powershell 5.1, add Powershell to the Program/Script field.\nIf the script runs under Powershell 7, add \u0026quot;C:\\Program Files\\PowerShell\\7\\pwsh.exe\u0026quot; to the Program/Script field. It\u0026rsquo;s important to have the quotes so that the parser doesn\u0026rsquo;t freak out over the space in \u0026lsquo;Program Files\u0026rsquo;.\nAdd the following Arguments: -executionpolicy bypass -command \u0026quot;\u0026amp; '\\\\SERVER\\SCRIPT REPOSITORY \\script.ps1'\u0026quot;. Again, I need double quotes around my UNC because of the space. I usually clear the Power options under the Conditions tab. If the OS thinks that our server is now running on battery power, we have bigger issues. Under Settings, I usually adjust Stop the task if it runs longer than\u0026hellip; field to something appropriate for the job schedule. Back to the General tab. Under the Security Options, I add the service account that my scripts run under. I set the task to Run whether the user is logged in or not, with highest priviledges to bypass any UAC problems and Configure for Windows Server 2016. (I have no idea what the real-world implications of setting the OS field are, since my scripts seem to work with this is also set to \u0026lsquo;Windows Server 2008\u0026rsquo;). Click OK. You\u0026rsquo;ll be prompted for the password of the credentials you used in #8. At the end of all this, I like to manually run the task by right-clicking it. This is a final sanity check that everything works. It\u0026rsquo;s easier for me to check at the time of setup (instead of the first time it runs) because my brain is already in that space. Since I promised the cheatcode way of doing this, here\u0026rsquo;s how I like to add new tasks that are similar to existing ones. Since most of mine are scripts, the Task Scheduler settings are pretty much identical. This method helps avoid problems when editing the Arguments field - the string is so long and the field window is so small that I always have to scroll and I hate trying to figure out if I\u0026rsquo;ve keyed things in properly.\nIn Task Scheduler, Find a task that closely matches the new one you\u0026rsquo;re implementing. Select it, right-click and choose Export. Save the XML file someone easily accessed, like the Desktop. Right-click the resulting file and open it in Notepad (or the editor of your choice). Update the \u0026lt;Description\u0026gt;, \u0026lt;URI\u0026gt; and \u0026lt;Arguments\u0026gt; fields with the correct values. Save the file. Back in Task Scheduler, select Import Task. Browse to the XML file you just edited, and Open it. Task scheduler will add a new task. You\u0026rsquo;ll have to re-enter the password and possibly adjust the schedule, but everything else should be correct. ","date":"2021-07-09T16:08:41-04:00","permalink":"https://gbeifuss.github.io/p/running-powershell-scripts-with-task-scheduler/","title":"Running Powershell scripts with Task Scheduler"},{"content":"I found out that our netadmins are regularly checking the amount of free space we have in our Power Platform (Dynamics) subscription with the following process:\nLaunch a browser Authenticate to portal.office.com Click Admin. A new window opens. Click All Admin Centers. Click Power Apps. A new window opens. Expand Resources -\u0026gt; Capacity. Look at the Storage Capacity Usage field and make sure we\u0026rsquo;re below our capacity. Whew! I\u0026rsquo;m tired typing that up once! I don\u0026rsquo;t think I could stand doing it daily (or weekly, or whenever they remember), so let\u0026rsquo;s automate it!\nWe need to know the total capacity and what we\u0026rsquo;re using. If we cross some threshold, trigger an action. That\u0026rsquo;s pretty straightforward.\nUsed Storage\nWe can grab this from the Business App Platform API, but there are some hoops to jump through. We need to setup an account that\u0026rsquo;ll have permissions for this, Register an Admin Management Application in PowerPlatform, then query the API.\nSetup the Account\nHow to: Use the portal to create an Azure AD application and service principal that can access resources\nRegister an Admin Management Application\n#Region RegisterAdminManagementApplication #https://docs.microsoft.com/en-us/power-platform/admin/powershell-create-service-principal #Requires Powershell 5.1 Import-Module Microsoft.PowerApps.Administration.PowerShell Import-Module Microsoft.PowerApps.PowerShell $appId = \u0026#34;APPID_FROM_AZURE\u0026#34; $tenantId = \u0026#34;TENANTID_FROM_AZURE\u0026#34; # Login interactively with a tenant administrator for Power Platform Add-PowerAppsAccount -Endpoint prod -TenantID $tenantId # Register a new application, this gives the SPN / client application same permissions as a tenant admin New-PowerAppManagementApp -ApplicationId $appId #EndRegion RegisterAdminManagementApplication Query the API\n#Region AuthenticateAsServicePrincipalDynamic $clientID = \u0026#34;CLIENTID_FROM_AZURE\u0026#34; $clientSecret = \u0026#34;PASSWORD_FROM_AZURE\u0026#34; $tenantdomain = \u0026#34;company.onmicrosoft.com\u0026#34; $loginURL = \u0026#34;https://login.microsoftonline.com/\u0026#34; $resource = \u0026#34;https://api.bap.microsoft.com/\u0026#34; $body = @{grant_type = \u0026#34;client_credentials\u0026#34;; resource = $resource; client_id = $clientID; client_secret = $clientSecret } $oauth = Invoke-RestMethod -Method Post -Uri $loginURL/$tenantdomain/oauth2/token?api-version=1.0 -Body $body $headerParams = @{\u0026#39;Authorization\u0026#39; = \u0026#34;$($oauth.token_type) $($oauth.access_token)\u0026#34; } #EndRegion AuthenticateAsServicePrincipalDynamics $request = Invoke-RestMethod -Method get -Headers $headerParams -Uri \u0026#39;https://api.bap.microsoft.com/providers/Microsoft.BusinessAppPlatform/scopes/admin/environments?api-version=2020-10-01\u0026amp;$expand=properties.capacity\u0026#39; $UsedStorage = \u0026#34;{0:n2}\u0026#34; -f (($request.value.properties.capacity | Measure-Object -Property RatedConsumption -Sum).sum / 1024) Storage Capacity\nThe available Storage Capacity is more complicated to determine, since it\u0026rsquo;s not stored anywhere that I could find. It seems to be dynamically generated, which means this script will have to do the legwork. Each tenant gets a base 10GB of Storage Capacity, and additional storage is added per licensed user, or by purchasing more Storage Capacity. We can pull the licesing information from the MS Graph API:\n#Region AuthenticateAsServicePrincipalGraph $resource = \u0026#34;https://graph.microsoft.com/\u0026#34; $body = @{grant_type = \u0026#34;client_credentials\u0026#34;; resource = $resource; client_id = $clientID; client_secret = $clientSecret } $oauth = Invoke-RestMethod -Method Post -Uri $loginURL/$tenantdomain/oauth2/token -Body $body $headerParams = @{\u0026#39;Authorization\u0026#39; = \u0026#34;$($oauth.token_type) $($oauth.access_token)\u0026#34; } #EndRegion AuthenticateAsServicePrincipalGraph $request = Invoke-RestMethod -Method get -Headers $headerParams -Uri \u0026#39;https://graph.microsoft.com/v1.0/subscribedskus\u0026#39; #calculate total space in GB $BaseStorage = 10 #fixed value per tenant $DYN_365_SALES = .25 * ($request.value | Where-Object { $_.skupartnumber -eq \u0026#34;dyn365_enterprise_sales\u0026#34; } | Select-Object -ExpandProperty prepaidunits).enabled $CRMStorage = ($request.value | Where-Object { $_.skupartnumber -eq \u0026#34;CRMSTORAGE\u0026#34; } | Select-Object -ExpandProperty prepaidunits).enabled $TotalStorage = \u0026#34;{0:n2}\u0026#34; -f ($BaseStorage + $DYN_365_SALES + $CRMStorage) Now let\u0026rsquo;s see the whole thing together, including the actions we\u0026rsquo;ll take after crossing certain space thresholds, as well as a function to let us know when the Application Password is near expiry:\n#O365-Dynamics-Capacity.ps1 \u0026lt;# Script to query the used vs total storage space we have in Power Platform (Dynamics) https://docs.microsoft.com/en-us/power-platform/admin/list-environments https://jussiroine.com/2019/12/building-a-monitoring-solution-for-power-platform-events-using-powershell-c-azure-log-analytics-and-azure-sentinel/ Greg Beifuss 2021-06-14 21:27 #\u0026gt; \u0026lt;# #Region RegisterAdminManagementApplication #https://docs.microsoft.com/en-us/power-platform/admin/powershell-create-service-principal #Requires Powershell 5.1 Import-Module Microsoft.PowerApps.Administration.PowerShell Import-Module Microsoft.PowerApps.PowerShell $appId = \u0026#34;APPID_FROM_AZURE\u0026#34; $tenantId = \u0026#34;TENANTID_FROM_AZURE\u0026#34; # Login interactively with a tenant administrator for Power Platform Add-PowerAppsAccount -Endpoint prod -TenantID $tenantId # Register a new application, this gives the SPN / client application same permissions as a tenant admin New-PowerAppManagementApp -ApplicationId $appId #EndRegion RegisterAdminManagementApplication #\u0026gt; #Region AuthenticateAsServicePrincipalDynamics $clientID = \u0026#34;CLIENTID_FROM_AZURE\u0026#34; $clientSecret = \u0026#34;PASSWORD_FROM_AZURE\u0026#34; $tenantdomain = \u0026#34;company.onmicrosoft.com\u0026#34; $loginURL = \u0026#34;https://login.microsoftonline.com/\u0026#34; $resource = \u0026#34;https://api.bap.microsoft.com/\u0026#34; $body = @{grant_type = \u0026#34;client_credentials\u0026#34;; resource = $resource; client_id = $clientID; client_secret = $clientSecret } $oauth = Invoke-RestMethod -Method Post -Uri $loginURL/$tenantdomain/oauth2/token?api-version=1.0 -Body $body $headerParams = @{\u0026#39;Authorization\u0026#39; = \u0026#34;$($oauth.token_type) $($oauth.access_token)\u0026#34; } #EndRegion AuthenticateAsServicePrincipalDynamics $request = Invoke-RestMethod -Method get -Headers $headerParams -Uri \u0026#39;https://api.bap.microsoft.com/providers/Microsoft.BusinessAppPlatform/scopes/admin/environments?api-version=2020-10-01\u0026amp;$expand=properties.capacity\u0026#39; $UsedStorage = \u0026#34;{0:n2}\u0026#34; -f (($request.value.properties.capacity | Measure-Object -Property RatedConsumption -Sum).sum / 1024) #Region AuthenticateAsServicePrincipalGraph $resource = \u0026#34;https://graph.microsoft.com/\u0026#34; $body = @{grant_type = \u0026#34;client_credentials\u0026#34;; resource = $resource; client_id = $clientID; client_secret = $clientSecret } $oauth = Invoke-RestMethod -Method Post -Uri $loginURL/$tenantdomain/oauth2/token -Body $body $headerParams = @{\u0026#39;Authorization\u0026#39; = \u0026#34;$($oauth.token_type) $($oauth.access_token)\u0026#34; } #EndRegion AuthenticateAsServicePrincipalGraph $request = Invoke-RestMethod -Method get -Headers $headerParams -Uri \u0026#39;https://graph.microsoft.com/v1.0/subscribedskus\u0026#39; #calculate total space in GB $BaseStorage = 10 #fixed value per tenant $DYN_365_SALES = .25 * ($request.value | Where-Object { $_.skupartnumber -eq \u0026#34;dyn365_enterprise_sales\u0026#34; } | Select-Object -ExpandProperty prepaidunits).enabled $CRMStorage = ($request.value | Where-Object { $_.skupartnumber -eq \u0026#34;CRMSTORAGE\u0026#34; } | Select-Object -ExpandProperty prepaidunits).enabled $TotalStorage = \u0026#34;{0:n2}\u0026#34; -f ($BaseStorage + $DYN_365_SALES + $CRMStorage) #Send email based on how much storage is left. Switch ($UsedStorage / $TotalStorage) { { $_ -ge 0.70 } { Send-MailMessage -To helpdesk@company.com -From helpdesk@company.com -Subject \u0026#34;Dynamics Capacity - $(($_*100).ToString(\u0026#34;#.##\u0026#34;))% in use!\u0026#34; -Body \u0026#34;We\u0026#39;re using $UsedStorage / $TotalStorage GB. Please immediately escalate to a Netadmin.\u0026#34; -SmtpServer 192.168.1.18 ; break} { $_ -gt 0.65 } { Send-MailMessage -To helpdesk@company.com -From helpdesk@company.com -Subject \u0026#34;Dynamics Capacity - 65% in use\u0026#34; -Body \u0026#34;We\u0026#39;re using $UsedStorage / $TotalStorage GB, or $((($UsedStorage / $TotalStorage)*100).ToString(\u0026#34;#.##\u0026#34;))%. Please escalate to a Netadmin.\u0026#34; -SmtpServer 192.168.1.18 } } #Send email if Application password is near expiry if ((Get-Date).adddays(14) -ge \u0026#34;6/10/2022\u0026#34;) { Send-MailMessage -To helpdesk@company.com -From helpdesk@company.com -Subject \u0026#34;O365-Dynamics-Capacity.ps1\u0026#34; -Body \u0026#34;The AzureAD Application Powershell-PowerPlatform-CapacityReport has a Client Secret near expiry. Please escalate to Greg for renewal.\u0026#34; -SmtpServer 192.168.1.18 } Greg\n","date":"2021-07-09T16:04:57-04:00","permalink":"https://gbeifuss.github.io/p/powershell-powerplatform-api/","title":"Powershell \u0026 PowerPlatform API"},{"content":"I\u0026rsquo;ve been learning how to use Powershell to work with APIs, so I decided to write a quick script that will query our two Netscaler devices for the running configuration, and dump it. This is a quick way to backup the configuration regularly, because I often forget to grab a copy for backup purposes (unless I\u0026rsquo;m performing extensive work, or upgrading the firmware).\n#These credentials are encrypted by ServiceAccount on Server $credFilePath = \u0026#39;\\\\SERVER\\PATH\\creds.xml\u0026#39; $netscalerId = (Import-Clixml $credFilePath).UserName $netscalerPwd = [System.Runtime.InteropServices.Marshal]::PtrToStringAuto([System.Runtime.InteropServices.Marshal]::SecureStringToBSTR((Import-Clixml $credFilePath).password)) $formatedDate = Get-Date -Format \u0026#39;yyyyMMdd\u0026#39; $FilePath = \u0026#34;\\\\SERVER\\PATH\u0026#34; $headers = @{\u0026#39;X-NITRO-USER\u0026#39;=\u0026#39;nsroot\u0026#39;; \u0026#39;X-NITRO-PASS\u0026#39;=\u0026#39;DFt$LG6%6L3F\u0026#39;} #grab Netscaler01 $uri = \u0026#34;https://netscaler01.company.com/nitro/v1/config/nsrunningconfig\u0026#34; $result = (Invoke-RestMethod -Method Get -Uri $uri -Headers $headers).nsrunningconfig.response ($result | Convertto-Json).split(\u0026#39;\\n\u0026#39;) | Out-File \u0026#34;$FilePath\\Netscaler01_${formatedDate}.txt\u0026#34; #grab Netscaler03 $uri = \u0026#34;https://netscaler03.company.com/nitro/v1/config/nsrunningconfig\u0026#34; $result = (Invoke-RestMethod -Method Get -Uri $uri -ContentType application/json -Headers $headers).nsrunningconfig.response ($result | ConvertTo-Json).split(\u0026#39;\\n\u0026#39;) | Out-File \u0026#34;$FilePath\\Netscaler03_${formatedDate}.txt\u0026#34; Greg\n","date":"2021-07-09T16:04:02-04:00","permalink":"https://gbeifuss.github.io/p/powershell-netscaler-api/","title":"Powershell \u0026 Netscaler API"}]